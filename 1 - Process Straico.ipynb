{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9b10f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta de entrada: Straico/\n",
      "Carpeta de salida: Metricas LLM/Data_for_Metrics/\n",
      "Modelos: ['amazon/nova-micro-v1', 'microsoft/phi-4', 'amazon/nova-lite-v1', 'cohere/command-r-08-2024', 'google/gemini-2.0-flash-001', 'qwen/qwen-2-vl-72b-instruct', 'qwen/qwen-2.5-72b-instruct', 'google/gemma-2-27b-it', 'google/gemini-2.5-flash-lite', 'meta-llama/llama-3.3-70b-instruct']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Configuración\n",
    "STRAICO_DIR = 'Straico'\n",
    "OUTPUT_DIR = 'Metricas LLM/Data_for_Metrics'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Cargar modelos desde el archivo text_model_ids.txt\n",
    "MODELS_FILE = 'Straico/text_model_usefull.txt'\n",
    "\n",
    "def load_models_from_file(filepath):\n",
    "    \"\"\"Carga los nombres de modelos desde un archivo de texto.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontró el archivo {filepath}\")\n",
    "        return []\n",
    "\n",
    "DEFAULT_MODELS = load_models_from_file(MODELS_FILE)[:10]\n",
    "\n",
    "print(f\"Carpeta de entrada: {STRAICO_DIR}/\")\n",
    "print(f\"Carpeta de salida: {OUTPUT_DIR}/\")\n",
    "print(f\"Modelos: {DEFAULT_MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a9a02",
   "metadata": {},
   "source": [
    "## Cargar Dataset Original (Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23532e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado: 6531 modismos únicos\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def load_ground_truth(csv_path: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"Carga el dataset original y elimina duplicados.\n",
    "    \n",
    "    Returns:\n",
    "        Dict con modismo como clave y dict con {significado, ejemplo} como valor\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            modismo = row.get('modismo', '').strip()\n",
    "            if modismo and modismo not in ground_truth:\n",
    "                ground_truth[modismo] = {\n",
    "                    'significado': row.get('significado', '').strip(),\n",
    "                    'ejemplo': row.get('ejemplo', '').strip()\n",
    "                }\n",
    "    \n",
    "    print(f\"Dataset cargado: {len(ground_truth)} modismos únicos\")\n",
    "    return ground_truth\n",
    "\n",
    "# Cargar dataset\n",
    "ground_truth = load_ground_truth('modismos_Dataset_Final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06334cc8",
   "metadata": {},
   "source": [
    "## Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f535d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_responses(prompt_dir: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Carga las respuestas de todos los modelos para un prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt_dir: Directorio del prompt (ej: 'Straico/Prompt 1')\n",
    "        \n",
    "    Returns:\n",
    "        Dict con modelo como clave y lista de respuestas como valor\n",
    "    \"\"\"\n",
    "\n",
    "    all_models_path = os.path.join(prompt_dir, 'all_models.json')\n",
    "    \n",
    "    if os.path.exists(all_models_path):\n",
    "        with open(all_models_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    return {}\n",
    "\n",
    "\n",
    "def extract_output_field(response: Dict, field: str) -> str:\n",
    "    \"\"\"Extrae un campo del output de la respuesta del modelo.\n",
    "    \n",
    "    Maneja múltiples formatos:\n",
    "    1. Formato directo: {\"output\": {\"campo\": \"valor\"}}\n",
    "    2. Formato raw_response: {\"raw_response\": \"```json\\\\n{...}\\\\n```\"}\n",
    "    3. Errores: {\"error\": \"...\"}\n",
    "    \n",
    "    Args:\n",
    "        response: Dict con la respuesta completa\n",
    "        field: Campo a extraer del output\n",
    "        \n",
    "    Returns:\n",
    "        Valor del campo como string, vacío si hay error o no se encuentra\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(response, dict):\n",
    "            return ''\n",
    "        \n",
    "        # Verificar si hay error\n",
    "        if 'error' in response:\n",
    "            return ''\n",
    "        \n",
    "        # Caso 1: Formato directo con output\n",
    "        output = response.get('output', {})\n",
    "        if isinstance(output, dict) and field in output:\n",
    "            return str(output.get(field, '')).strip()\n",
    "        \n",
    "        # Caso 2: raw_response con JSON anidado\n",
    "        raw_response = response.get('raw_response', '')\n",
    "        if raw_response:\n",
    "            # Limpiar markdown code blocks\n",
    "            raw_response = raw_response.strip()\n",
    "            if raw_response.startswith('```json'):\n",
    "                raw_response = raw_response[7:]  # Remover ```json\n",
    "            if raw_response.startswith('```'):\n",
    "                raw_response = raw_response[3:]  # Remover ```\n",
    "            if raw_response.endswith('```'):\n",
    "                raw_response = raw_response[:-3]  # Remover ```\n",
    "            \n",
    "            raw_response = raw_response.strip()\n",
    "            \n",
    "            # Parsear el JSON anidado\n",
    "            try:\n",
    "                parsed = json.loads(raw_response)\n",
    "                if isinstance(parsed, dict):\n",
    "                    output = parsed.get('output', {})\n",
    "                    if isinstance(output, dict) and field in output:\n",
    "                        return str(output.get(field, '')).strip()\n",
    "            except json.JSONDecodeError:\n",
    "                return ''\n",
    "        \n",
    "        return ''\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error extrayendo campo '{field}': {e}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "def save_json(filepath: str, data: Any):\n",
    "    \"\"\"Guarda datos en formato JSON.\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Guardado: {filepath} ({len(data)} registros)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d9bb9",
   "metadata": {},
   "source": [
    "## PROMPT 1: Modismo → Definición\n",
    "\n",
    "Procesa las respuestas del Prompt 1 y genera datos para métricas comparando las definiciones generadas con el ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96b695b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROCESANDO PROMPT 1: Modismo → Definición\n",
      "============================================================\n",
      "⚠ Advertencia: Modelo meta-llama/llama-3.3-70b-instruct no encontrado en Prompt 1\n",
      "✓ Guardado: Metricas LLM/Data_for_Metrics/prompt_1_metrics_data.json (9 registros)\n",
      "\n",
      "✓ Procesados 9 registros válidos\n",
      "✗ Omitidos 0 registros con errores\n",
      "⚠ Modelos no encontrados: meta-llama/llama-3.3-70b-instruct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PROCESANDO PROMPT 1: Modismo → Definición\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cargar respuestas\n",
    "prompt_1_responses = load_prompt_responses(os.path.join(STRAICO_DIR, 'Prompt 1'))\n",
    "\n",
    "# Procesar datos\n",
    "prompt_1_data = []\n",
    "errors_count = 0\n",
    "skipped_models = []\n",
    "\n",
    "for model in DEFAULT_MODELS:\n",
    "    if model not in prompt_1_responses:\n",
    "        print(f\"⚠ Advertencia: Modelo {model} no encontrado en Prompt 1\")\n",
    "        skipped_models.append(model)\n",
    "        continue\n",
    "    \n",
    "    model_errors = 0\n",
    "    for entry in prompt_1_responses[model]:\n",
    "        modismo = entry.get('modismo', '')\n",
    "        response = entry.get('response', {})\n",
    "        \n",
    "        # Verificar si hay error en la respuesta\n",
    "        if 'error' in response:\n",
    "            model_errors += 1\n",
    "            errors_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Extraer definición generada del output\n",
    "        definicion_generada = extract_output_field(response, 'definicion')\n",
    "        \n",
    "        # Buscar en ground truth\n",
    "        gt = ground_truth.get(modismo, {})\n",
    "        definicion_real = gt.get('significado', '')\n",
    "        \n",
    "        # Solo agregar si se extrajo la definición correctamente\n",
    "        if definicion_generada:\n",
    "            prompt_1_data.append({\n",
    "                'modismo': modismo,\n",
    "                'definicion_real': definicion_real,\n",
    "                'modelo': model,\n",
    "                'definicion_generada': definicion_generada\n",
    "            })\n",
    "        else:\n",
    "            model_errors += 1\n",
    "            errors_count += 1\n",
    "    \n",
    "    if model_errors > 0:\n",
    "        print(f\"  {model}: {model_errors} errores omitidos\")\n",
    "\n",
    "# Guardar\n",
    "output_path = os.path.join(OUTPUT_DIR, 'prompt_1_metrics_data.json')\n",
    "save_json(output_path, prompt_1_data)\n",
    "\n",
    "print(f\"\\n✓ Procesados {len(prompt_1_data)} registros válidos\")\n",
    "print(f\"✗ Omitidos {errors_count} registros con errores\")\n",
    "if skipped_models:\n",
    "    print(f\"⚠ Modelos no encontrados: {', '.join(skipped_models)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012bb59",
   "metadata": {},
   "source": [
    "## PROMPT 2: Modismo → Es Modismo (Sí/No)\n",
    "\n",
    "Procesa las respuestas del Prompt 2 para evaluar si los modelos identifican correctamente los modismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95cd949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROCESANDO PROMPT 2: Modismo → Es Modismo (Sí/No)\n",
      "============================================================\n",
      "⚠ Advertencia: Modelo meta-llama/llama-3.3-70b-instruct no encontrado en Prompt 2\n",
      "✓ Guardado: Metricas LLM/Data_for_Metrics/prompt_2_metrics_data.json (9 registros)\n",
      "\n",
      "✓ Procesados 9 registros válidos\n",
      "✗ Omitidos 0 registros con errores\n",
      "⚠ Modelos no encontrados: meta-llama/llama-3.3-70b-instruct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PROCESANDO PROMPT 2: Modismo → Es Modismo (Sí/No)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cargar respuestas\n",
    "prompt_2_responses = load_prompt_responses(os.path.join(STRAICO_DIR, 'Prompt 2'))\n",
    "\n",
    "# Procesar datos\n",
    "prompt_2_data = []\n",
    "errors_count = 0\n",
    "skipped_models = []\n",
    "\n",
    "for model in DEFAULT_MODELS:\n",
    "    if model not in prompt_2_responses:\n",
    "        print(f\"⚠ Advertencia: Modelo {model} no encontrado en Prompt 2\")\n",
    "        skipped_models.append(model)\n",
    "        continue\n",
    "    \n",
    "    model_errors = 0\n",
    "    for entry in prompt_2_responses[model]:\n",
    "        modismo = entry.get('modismo', '')\n",
    "        response = entry.get('response', {})\n",
    "        \n",
    "        # Verificar si hay error en la respuesta\n",
    "        if 'error' in response:\n",
    "            model_errors += 1\n",
    "            errors_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Extraer respuesta (Sí/No) del output\n",
    "        es_modismo_generado = extract_output_field(response, 'es_modismo')\n",
    "        \n",
    "        # Solo agregar si se extrajo la respuesta correctamente\n",
    "        if es_modismo_generado:\n",
    "            prompt_2_data.append({\n",
    "                'modismo': modismo,\n",
    "                'es_modismo_real': 'Sí',  # Todos los del dataset son modismos\n",
    "                'modelo': model,\n",
    "                'es_modismo_generado': es_modismo_generado\n",
    "            })\n",
    "        else:\n",
    "            model_errors += 1\n",
    "            errors_count += 1\n",
    "    \n",
    "    if model_errors > 0:\n",
    "        print(f\"  {model}: {model_errors} errores omitidos\")\n",
    "\n",
    "# Guardar\n",
    "output_path = os.path.join(OUTPUT_DIR, 'prompt_2_metrics_data.json')\n",
    "save_json(output_path, prompt_2_data)\n",
    "\n",
    "print(f\"\\n✓ Procesados {len(prompt_2_data)} registros válidos\")\n",
    "print(f\"✗ Omitidos {errors_count} registros con errores\")\n",
    "if skipped_models:\n",
    "    print(f\"⚠ Modelos no encontrados: {', '.join(skipped_models)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb9b916",
   "metadata": {},
   "source": [
    "## PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
    "\n",
    "Procesa las respuestas del Prompt 3 para evaluar la capacidad de los modelos de generar interpretaciones literales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f79ee8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROCESANDO PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
      "============================================================\n",
      "  qwen/qwen-2.5-72b-instruct: 1 errores omitidos\n",
      "⚠ Advertencia: Modelo meta-llama/llama-3.3-70b-instruct no encontrado en Prompt 3\n",
      "✓ Guardado: Metricas LLM/Data_for_Metrics/prompt_3_metrics_data.json (8 registros)\n",
      "\n",
      "✓ Procesados 8 registros válidos\n",
      "✗ Omitidos 1 registros con errores\n",
      "⚠ Modelos no encontrados: meta-llama/llama-3.3-70b-instruct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PROCESANDO PROMPT 3: Modismo + Ejemplo → Literal + Definición\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cargar respuestas\n",
    "prompt_3_responses = load_prompt_responses(os.path.join(STRAICO_DIR, 'Prompt 3'))\n",
    "\n",
    "# Procesar datos\n",
    "prompt_3_data = []\n",
    "errors_count = 0\n",
    "skipped_models = []\n",
    "\n",
    "for model in DEFAULT_MODELS:\n",
    "    if model not in prompt_3_responses:\n",
    "        print(f\"⚠ Advertencia: Modelo {model} no encontrado en Prompt 3\")\n",
    "        skipped_models.append(model)\n",
    "        continue\n",
    "    \n",
    "    model_errors = 0\n",
    "    for entry in prompt_3_responses[model]:\n",
    "        modismo = entry.get('modismo', '')\n",
    "        ejemplo = entry.get('ejemplo', '')\n",
    "        response = entry.get('response', {})\n",
    "        \n",
    "        # Verificar si hay error en la respuesta\n",
    "        if 'error' in response:\n",
    "            model_errors += 1\n",
    "            errors_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Extraer literal y definición del output\n",
    "        literal_generado = extract_output_field(response, 'literal')\n",
    "        definicion_generada = extract_output_field(response, 'definicion')\n",
    "        \n",
    "        # Buscar en ground truth\n",
    "        gt = ground_truth.get(modismo, {})\n",
    "        \n",
    "        # Solo agregar si se extrajo al menos un campo correctamente\n",
    "        if literal_generado or definicion_generada:\n",
    "            prompt_3_data.append({\n",
    "                'modismo': modismo,\n",
    "                'ejemplo': ejemplo,\n",
    "                'significado_real': gt.get('significado', ''),\n",
    "                'modelo': model,\n",
    "                'literal_generado': literal_generado,\n",
    "                'definicion_generada': definicion_generada\n",
    "            })\n",
    "        else:\n",
    "            model_errors += 1\n",
    "            errors_count += 1\n",
    "    \n",
    "    if model_errors > 0:\n",
    "        print(f\"  {model}: {model_errors} errores omitidos\")\n",
    "\n",
    "# Guardar\n",
    "output_path = os.path.join(OUTPUT_DIR, 'prompt_3_metrics_data.json')\n",
    "save_json(output_path, prompt_3_data)\n",
    "\n",
    "print(f\"\\n✓ Procesados {len(prompt_3_data)} registros válidos\")\n",
    "print(f\"✗ Omitidos {errors_count} registros con errores\")\n",
    "if skipped_models:\n",
    "    print(f\"⚠ Modelos no encontrados: {', '.join(skipped_models)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e6a05",
   "metadata": {},
   "source": [
    "## Resumen de Archivos Generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73065d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RESUMEN DE ARCHIVOS GENERADOS\n",
      "============================================================\n",
      "  prompt_1_metrics_data.json: 9 registros\n",
      "  prompt_2_metrics_data.json: 9 registros\n",
      "  prompt_3_metrics_data.json: 8 registros\n",
      "============================================================\n",
      "Total: 3 archivos, 26 registros\n",
      "Ubicación: Metricas LLM/Data_for_Metrics/\n",
      "\n",
      "✓ Procesamiento completado\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RESUMEN DE ARCHIVOS GENERADOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import glob\n",
    "\n",
    "# Listar todos los archivos JSON generados\n",
    "json_files = glob.glob(os.path.join(OUTPUT_DIR, '*.json'))\n",
    "json_files.sort()\n",
    "\n",
    "total_registros = 0\n",
    "for filepath in json_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    num_registros = len(data)\n",
    "    total_registros += num_registros\n",
    "    \n",
    "    print(f\"  {filename}: {num_registros} registros\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total: {len(json_files)} archivos, {total_registros} registros\")\n",
    "print(f\"Ubicación: {OUTPUT_DIR}/\")\n",
    "print(\"\\n✓ Procesamiento completado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
