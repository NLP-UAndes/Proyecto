{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30cd0f2",
   "metadata": {},
   "source": [
    "# Evaluación de Modelos LLM en Modismos Colombianos\n",
    "\n",
    "Este notebook evalúa qué tan bien los modelos LLM entienden los modismos colombianos usando:\n",
    "- **BERTScore**: Para comparar similitud semántica entre textos (definiciones)\n",
    "- **Accuracy/Exact Match**: Para clasificación y coincidencias exactas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e03151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos en: Metricas LLM/Data_for_Metrics\n",
      "Resultados en: Metricas LLM/Metricas_Resultados\n",
      "Modelos: ['amazon/nova-micro-v1', 'microsoft/phi-4', 'amazon/nova-lite-v1', 'cohere/command-r-08-2024', 'google/gemini-2.0-flash-001', 'qwen/qwen-2-vl-72b-instruct', 'qwen/qwen-2.5-72b-instruct', 'google/gemma-2-27b-it', 'google/gemini-2.5-flash-lite', 'meta-llama/llama-3.3-70b-instruct']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Estilos para gráficos\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Modelos\n",
    "MODELS_FILE = 'Straico/text_model_usefull.txt'\n",
    "\n",
    "def load_models_from_file(filepath):\n",
    "    \"\"\"Carga los nombres de modelos desde un archivo de texto.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontró el archivo {filepath}\")\n",
    "        return []\n",
    "\n",
    "MODEL_NAMES = load_models_from_file(MODELS_FILE)[:10]\n",
    "\n",
    "# Directorios\n",
    "DATA_DIR = 'Metricas LLM/Data_for_Metrics'\n",
    "OUTPUT_DIR = 'Metricas LLM/Metricas_Resultados'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Datos en: {DATA_DIR}\")\n",
    "print(f\"Resultados en: {OUTPUT_DIR}\")\n",
    "print(f\"Modelos: {MODEL_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28bca76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Métricas cargadas correctamente:\n",
      "  - BERTScore (BETO y SciBETO)\n",
      "  - Sentence-BERT (paraphrase-multilingual-mpnet-base-v2)\n",
      "  - chrF (Character n-gram F-score)\n"
     ]
    }
   ],
   "source": [
    "# Importar funciones de métricas\n",
    "import sys\n",
    "sys.path.append('Metricas LLM')\n",
    "\n",
    "from BertScore import compute_bertscore_beto, compute_bertscore_sci_beto\n",
    "from SentenceBert import compute_sbert_similarity\n",
    "from chrF import compute_chrf_batch\n",
    "\n",
    "print(\"✓ Métricas cargadas correctamente:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO)\")\n",
    "print(\"  - Sentence-BERT (paraphrase-multilingual-mpnet-base-v2)\")\n",
    "print(\"  - chrF (Character n-gram F-score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14c7be",
   "metadata": {},
   "source": [
    "## 1. PROMPT 1: Modismo → Definición\n",
    "\n",
    "**Métrica**: BERTScore (F1)  \n",
    "**Justificación**: Evalúa similitud semántica entre la definición generada y la real. Ideal para entender si el modelo captura el significado del modismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2dd6f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 1: Modismo → Definición\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo puede generar una definición correcta del modismo\n",
      "Métricas:\n",
      "  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\n",
      "  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\n",
      "  - chrF: Character n-gram F-score\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 9 registros válidos de 9 totales\n",
      "\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con BETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.3708\n",
      "      • Recall:    0.3922\n",
      "      • F1 Score:  0.3812 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.3340\n",
      "      • Recall:    0.3516\n",
      "      • F1 Score:  0.3425 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.3992\n",
      "      • Recall:    0.4385\n",
      "      • F1 Score:  0.4179 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.3666\n",
      "      • Recall:    0.3769\n",
      "      • F1 Score:  0.3717 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.5522\n",
      "      • Recall:    0.5957\n",
      "      • F1 Score:  0.5731 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.7208\n",
      "      • Recall:    0.7139\n",
      "      • F1 Score:  0.7174 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.5823\n",
      "      • Recall:    0.6056\n",
      "      • F1 Score:  0.5937 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.3350\n",
      "      • Recall:    0.3755\n",
      "      • F1 Score:  0.3541 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.4698\n",
      "      • Recall:    0.5240\n",
      "      • F1 Score:  0.4954 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.3708\n",
      "      • Recall:    0.3922\n",
      "      • F1 Score:  0.3812 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.3340\n",
      "      • Recall:    0.3516\n",
      "      • F1 Score:  0.3425 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.3992\n",
      "      • Recall:    0.4385\n",
      "      • F1 Score:  0.4179 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.3666\n",
      "      • Recall:    0.3769\n",
      "      • F1 Score:  0.3717 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.5522\n",
      "      • Recall:    0.5957\n",
      "      • F1 Score:  0.5731 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.7208\n",
      "      • Recall:    0.7139\n",
      "      • F1 Score:  0.7174 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.5823\n",
      "      • Recall:    0.6056\n",
      "      • F1 Score:  0.5937 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.3350\n",
      "      • Recall:    0.3755\n",
      "      • F1 Score:  0.3541 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.4698\n",
      "      • Recall:    0.5240\n",
      "      • F1 Score:  0.4954 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.8039\n",
      "      • Recall:    0.8218\n",
      "      • F1 Score:  0.8128 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.7659\n",
      "      • Recall:    0.7945\n",
      "      • F1 Score:  0.7799 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.8047\n",
      "      • Recall:    0.8181\n",
      "      • F1 Score:  0.8113 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.7896\n",
      "      • Recall:    0.8103\n",
      "      • F1 Score:  0.7998 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.8195\n",
      "      • Recall:    0.8655\n",
      "      • F1 Score:  0.8419 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.8408\n",
      "      • Recall:    0.8339\n",
      "      • F1 Score:  0.8373 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.8480\n",
      "      • Recall:    0.8732\n",
      "      • F1 Score:  0.8605 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.7621\n",
      "      • Recall:    0.8057\n",
      "      • F1 Score:  0.7833 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.7880\n",
      "      • Recall:    0.8396\n",
      "      • F1 Score:  0.8130 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.8039\n",
      "      • Recall:    0.8218\n",
      "      • F1 Score:  0.8128 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.7659\n",
      "      • Recall:    0.7945\n",
      "      • F1 Score:  0.7799 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.8047\n",
      "      • Recall:    0.8181\n",
      "      • F1 Score:  0.8113 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.7896\n",
      "      • Recall:    0.8103\n",
      "      • F1 Score:  0.7998 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.8195\n",
      "      • Recall:    0.8655\n",
      "      • F1 Score:  0.8419 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.8408\n",
      "      • Recall:    0.8339\n",
      "      • F1 Score:  0.8373 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.8480\n",
      "      • Recall:    0.8732\n",
      "      • F1 Score:  0.8605 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.7621\n",
      "      • Recall:    0.8057\n",
      "      • F1 Score:  0.7833 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.7880\n",
      "      • Recall:    0.8396\n",
      "      • F1 Score:  0.8130 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.5400 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Similitud: 0.4959 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Similitud: 0.4688 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.7793 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Similitud: 0.6141 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Similitud: 0.8479 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Similitud: 0.8764 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Similitud: 0.3888 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Similitud: 0.5562 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2502 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • chrF: 0.1850 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • chrF: 0.2159 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.2751 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • chrF: 0.3584 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • chrF: 0.3107 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • chrF: 0.3686 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • chrF: 0.2312 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • chrF: 0.3231 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 1 COMPLETADO\n",
      "================================================================================\n",
      "\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.5400 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Similitud: 0.4959 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Similitud: 0.4688 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.7793 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Similitud: 0.6141 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Similitud: 0.8479 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Similitud: 0.8764 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Similitud: 0.3888 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Similitud: 0.5562 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2502 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • chrF: 0.1850 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • chrF: 0.2159 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.2751 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • chrF: 0.3584 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • chrF: 0.3107 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • chrF: 0.3686 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • chrF: 0.2312 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • chrF: 0.3231 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 1 COMPLETADO\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 1: Modismo → Definición\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo puede generar una definición correcta del modismo\")\n",
    "print(\"Métricas:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\")\n",
    "print(\"  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\")\n",
    "print(\"  - chrF: Character n-gram F-score\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_1_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p1 = json.load(f)\n",
    "\n",
    "# Filtrar datos vacíos\n",
    "data_p1_valid = [d for d in data_p1 if d.get('definicion_real') and d.get('definicion_generada')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p1_valid)} registros válidos de {len(data_p1)} totales\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BERTScore con BETO y SciBETO\n",
    "# ============================================================================\n",
    "bert_models = [\n",
    "    ('BETO', compute_bertscore_beto),\n",
    "    ('SciBETO', compute_bertscore_sci_beto)\n",
    "]\n",
    "\n",
    "for bert_name, bert_func in bert_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Calculando BERTScore con {bert_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    resultados_p1 = []\n",
    "    \n",
    "    for model in MODEL_NAMES:\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        \n",
    "        # Filtrar datos de este modelo\n",
    "        model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "        \n",
    "        if not model_data:\n",
    "            print(f\"  ⚠ No hay datos para {model} - asignando métricas en 0\")\n",
    "            # Crear entrada con métricas en 0\n",
    "            resultados_p1.append({\n",
    "                'modismo': 'N/A',\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'definicion_real': '',\n",
    "                'definicion_generada': ''\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        referencias = [d['definicion_real'] for d in model_data]\n",
    "        candidatos = [d['definicion_generada'] for d in model_data]\n",
    "        \n",
    "        # BERTScore: compara candidatos (generados) vs referencias (reales)\n",
    "        P, R, F1 = bert_func(candidatos, referencias)\n",
    "        \n",
    "        for idx, (p, r, f1) in enumerate(zip(P.tolist(), R.tolist(), F1.tolist())):\n",
    "            resultados_p1.append({\n",
    "                'modismo': model_data[idx]['modismo'],\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': p,\n",
    "                'recall': r,\n",
    "                'f1_score': f1,\n",
    "                'definicion_real': referencias[idx],\n",
    "                'definicion_generada': candidatos[idx]\n",
    "            })\n",
    "    \n",
    "    # Guardar resultados\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'prompt_1_{bert_name.lower()}_bertscore_resultados.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados_p1, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"RESULTADOS con {bert_name}:\")\n",
    "    for model in MODEL_NAMES:\n",
    "        model_results = [r for r in resultados_p1 if r['modelo'] == model]\n",
    "        if model_results:\n",
    "            f1_scores = [r['f1_score'] for r in model_results]\n",
    "            precisions = [r['precision'] for r in model_results]\n",
    "            recalls = [r['recall'] for r in model_results]\n",
    "            \n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            p_mean = np.mean(precisions)\n",
    "            r_mean = np.mean(recalls)\n",
    "            \n",
    "            print(f\"   {model}:\")\n",
    "            print(f\"      • Precision: {p_mean:.4f}\")\n",
    "            print(f\"      • Recall:    {r_mean:.4f}\")\n",
    "            print(f\"      • F1 Score:  {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Sentence-BERT Similarity\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando Sentence-BERT Similarity\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_sbert = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_sbert.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'similarity': 0.0,\n",
    "            'definicion_real': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['definicion_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular similitud con Sentence-BERT\n",
    "    similarities = compute_sbert_similarity(candidatos, referencias)\n",
    "    \n",
    "    for idx, sim in enumerate(similarities):\n",
    "        resultados_sbert.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'modelo': model,\n",
    "            'similarity': float(sim),\n",
    "            'definicion_real': referencias[idx],\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_1_sbert_similarity_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_sbert, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con Sentence-BERT:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_sbert if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        similarities = [r['similarity'] for r in model_results]\n",
    "        sim_mean = np.mean(similarities)\n",
    "        sim_std = np.std(similarities)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Similitud: {sim_mean:.4f} (±{sim_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. chrF Score\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando chrF Score\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_chrf = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_chrf.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'chrf_score': 0.0,\n",
    "            'definicion_real': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['definicion_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular chrF\n",
    "    chrf_scores = compute_chrf_batch(candidatos, referencias)\n",
    "    \n",
    "    for idx, score in enumerate(chrf_scores):\n",
    "        resultados_chrf.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'modelo': model,\n",
    "            'chrf_score': float(score),\n",
    "            'definicion_real': referencias[idx],\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_1_chrf_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_chrf, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con chrF:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_chrf if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        chrf_scores = [r['chrf_score'] for r in model_results]\n",
    "        chrf_mean = np.mean(chrf_scores)\n",
    "        chrf_std = np.std(chrf_scores)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • chrF: {chrf_mean:.4f} (±{chrf_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT 1 COMPLETADO\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545337c",
   "metadata": {},
   "source": [
    "## 2. PROMPT 2: Modismo → Es Modismo (Sí/No)\n",
    "\n",
    "**Métrica**: Accuracy  \n",
    "**Justificación**: Es una tarea de clasificación binaria. Mide si el modelo identifica correctamente que una expresión es un modismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a5292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 2: Modismo → ¿Es Modismo? (Sí/No)\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo identifica correctamente que una expresión es un modismo\n",
      "Métrica: Accuracy (Exactitud)\n",
      "Justificación: Problema de clasificación binaria (Sí/No).\n",
      "   Mide el porcentaje de respuestas correctas.\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 9 registros válidos de 9 totales\n",
      "\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "  • Accuracy: 0.0000 (0/1 correctos)\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "  • Accuracy: 0.0000 (0/1 correctos)\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "  • Accuracy: 0.0000 (0/1 correctos)\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "  • Accuracy: 0.0000 (0/1 correctos)\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "  • Accuracy: 1.0000 (1/1 correctos)\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "  • Accuracy: 1.0000 (1/1 correctos)\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  • Accuracy: 1.0000 (1/1 correctos)\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "  • Accuracy: 1.0000 (1/1 correctos)\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "  • Accuracy: 1.0000 (1/1 correctos)\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando accuracy en 0\n",
      "\n",
      "RESULTADOS:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   microsoft/phi-4:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Accuracy:  1.0000 (1/1 correctos)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Accuracy:  1.0000 (1/1 correctos)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Accuracy:  1.0000 (1/1 correctos)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Accuracy:  1.0000 (1/1 correctos)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Accuracy:  1.0000 (1/1 correctos)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_2_accuracy_resultados.json\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 2: Modismo → ¿Es Modismo? (Sí/No)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo identifica correctamente que una expresión es un modismo\")\n",
    "print(\"Métrica: Accuracy (Exactitud)\")\n",
    "print(\"Justificación: Problema de clasificación binaria (Sí/No).\")\n",
    "print(\"   Mide el porcentaje de respuestas correctas.\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_2_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p2 = json.load(f)\n",
    "\n",
    "# Filtrar datos vacíos\n",
    "data_p2_valid = [d for d in data_p2 if d.get('es_modismo_real') and d.get('es_modismo_generado')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p2_valid)} registros válidos de {len(data_p2)} totales\")\n",
    "print()\n",
    "\n",
    "def normalizar_respuesta(respuesta):\n",
    "    \"\"\"Normaliza respuestas a 'Sí' o 'No'\"\"\"\n",
    "    if not respuesta:\n",
    "        return None\n",
    "    respuesta = str(respuesta).strip().lower()\n",
    "    if respuesta in ['sí', 'si', 'yes', 's', 'true', '1']:\n",
    "        return 'Sí'\n",
    "    elif respuesta in ['no', 'n', 'false', '0']:\n",
    "        return 'No'\n",
    "    return respuesta\n",
    "\n",
    "resultados_p2 = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    # Filtrar datos de este modelo\n",
    "    model_data = [d for d in data_p2_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando accuracy en 0\")\n",
    "        resultados_p2.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'respuesta_real': 'Sí',\n",
    "            'respuesta_generada': 'No',\n",
    "            'correcto': False\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Calcular accuracy\n",
    "    correctos = 0\n",
    "    for d in model_data:\n",
    "        real = normalizar_respuesta(d['es_modismo_real'])\n",
    "        generado = normalizar_respuesta(d['es_modismo_generado'])\n",
    "        correcto = (real == generado)\n",
    "        \n",
    "        if correcto:\n",
    "            correctos += 1\n",
    "        \n",
    "        resultados_p2.append({\n",
    "            'modismo': d['modismo'],\n",
    "            'modelo': model,\n",
    "            'respuesta_real': real,\n",
    "            'respuesta_generada': generado,\n",
    "            'correcto': correcto\n",
    "        })\n",
    "    \n",
    "    accuracy = correctos / len(model_data) if model_data else 0\n",
    "    print(f\"  • Accuracy: {accuracy:.4f} ({correctos}/{len(model_data)} correctos)\")\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_2_accuracy_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_p2, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_p2 if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        correctos = sum(1 for r in model_results if r['correcto'])\n",
    "        total = len(model_results)\n",
    "        accuracy = correctos / total if total > 0 else 0\n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Accuracy:  {accuracy:.4f} ({correctos}/{total} correctos)\")\n",
    "\n",
    "print()\n",
    "print(f\"✓ Guardado en: {output_file}\")\n",
    "print(\"=\"*80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03bb355",
   "metadata": {},
   "source": [
    "## 3. PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
    "\n",
    "**Métrica**: BERTScore (F1)  \n",
    "**Justificación**: Evalúa si el modelo puede generar interpretaciones literales del modismo en contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d89e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo genera interpretaciones literales correctas\n",
      "Métricas:\n",
      "  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\n",
      "  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\n",
      "  - chrF: Character n-gram F-score\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 8 registros válidos de 8 totales\n",
      "\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con BETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.3511\n",
      "      • Recall:    0.3178\n",
      "      • F1 Score:  0.3336 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.4298\n",
      "      • Recall:    0.4422\n",
      "      • F1 Score:  0.4359 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.4507\n",
      "      • Recall:    0.4535\n",
      "      • F1 Score:  0.4521 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.2876\n",
      "      • Recall:    0.2797\n",
      "      • F1 Score:  0.2836 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.3395\n",
      "      • Recall:    0.3202\n",
      "      • F1 Score:  0.3296 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.6178\n",
      "      • Recall:    0.6025\n",
      "      • F1 Score:  0.6100 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.3673\n",
      "      • Recall:    0.3584\n",
      "      • F1 Score:  0.3628 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.5093\n",
      "      • Recall:    0.5187\n",
      "      • F1 Score:  0.5140 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.3511\n",
      "      • Recall:    0.3178\n",
      "      • F1 Score:  0.3336 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.4298\n",
      "      • Recall:    0.4422\n",
      "      • F1 Score:  0.4359 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.4507\n",
      "      • Recall:    0.4535\n",
      "      • F1 Score:  0.4521 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.2876\n",
      "      • Recall:    0.2797\n",
      "      • F1 Score:  0.2836 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.3395\n",
      "      • Recall:    0.3202\n",
      "      • F1 Score:  0.3296 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.6178\n",
      "      • Recall:    0.6025\n",
      "      • F1 Score:  0.6100 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.3673\n",
      "      • Recall:    0.3584\n",
      "      • F1 Score:  0.3628 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.5093\n",
      "      • Recall:    0.5187\n",
      "      • F1 Score:  0.5140 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7632\n",
      "      • Recall:    0.7770\n",
      "      • F1 Score:  0.7700 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.7388\n",
      "      • Recall:    0.7738\n",
      "      • F1 Score:  0.7559 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.7626\n",
      "      • Recall:    0.7675\n",
      "      • F1 Score:  0.7651 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.6997\n",
      "      • Recall:    0.7342\n",
      "      • F1 Score:  0.7165 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.7494\n",
      "      • Recall:    0.7362\n",
      "      • F1 Score:  0.7428 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.7756\n",
      "      • Recall:    0.7802\n",
      "      • F1 Score:  0.7779 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.8065\n",
      "      • Recall:    0.7917\n",
      "      • F1 Score:  0.7990 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.8295\n",
      "      • Recall:    0.8407\n",
      "      • F1 Score:  0.8351 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.5543 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Similitud: 0.5983 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Similitud: 0.6880 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.6044 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Similitud: 0.4469 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Similitud: 0.6945 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Similitud: 0.4970 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Similitud: 0.8397 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2045 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • chrF: 0.2457 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • chrF: 0.2283 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.1523 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • chrF: 0.1504 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • chrF: 0.1609 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • chrF: 0.1879 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • chrF: 0.2528 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 3 COMPLETADO\n",
      "================================================================================\n",
      "\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7632\n",
      "      • Recall:    0.7770\n",
      "      • F1 Score:  0.7700 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.7388\n",
      "      • Recall:    0.7738\n",
      "      • F1 Score:  0.7559 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.7626\n",
      "      • Recall:    0.7675\n",
      "      • F1 Score:  0.7651 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.6997\n",
      "      • Recall:    0.7342\n",
      "      • F1 Score:  0.7165 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.7494\n",
      "      • Recall:    0.7362\n",
      "      • F1 Score:  0.7428 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.7756\n",
      "      • Recall:    0.7802\n",
      "      • F1 Score:  0.7779 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.8065\n",
      "      • Recall:    0.7917\n",
      "      • F1 Score:  0.7990 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.8295\n",
      "      • Recall:    0.8407\n",
      "      • F1 Score:  0.8351 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.5543 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • Similitud: 0.5983 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Similitud: 0.6880 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.6044 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Similitud: 0.4469 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Similitud: 0.6945 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Similitud: 0.4970 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Similitud: 0.8397 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.3-70b-instruct - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2045 (±0.0000)\n",
      "   microsoft/phi-4:\n",
      "      • chrF: 0.2457 (±0.0000)\n",
      "   amazon/nova-lite-v1:\n",
      "      • chrF: 0.2283 (±0.0000)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.1523 (±0.0000)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • chrF: 0.1504 (±0.0000)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • chrF: 0.1609 (±0.0000)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • chrF: 0.1879 (±0.0000)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • chrF: 0.2528 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 3 COMPLETADO\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 3: Modismo + Ejemplo → Literal + Definición\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo genera interpretaciones literales correctas\")\n",
    "print(\"Métricas:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\")\n",
    "print(\"  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\")\n",
    "print(\"  - chrF: Character n-gram F-score\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_3_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p3 = json.load(f)\n",
    "\n",
    "# Filtrar datos con literal y definición generados\n",
    "data_p3_valid = [d for d in data_p3 if d.get('significado_real') and d.get('definicion_generada')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p3_valid)} registros válidos de {len(data_p3)} totales\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BERTScore con BETO y SciBETO\n",
    "# ============================================================================\n",
    "bert_models = [\n",
    "    ('BETO', compute_bertscore_beto),\n",
    "    ('SciBETO', compute_bertscore_sci_beto)\n",
    "]\n",
    "\n",
    "for bert_name, bert_func in bert_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Calculando BERTScore con {bert_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    resultados_p3 = []\n",
    "    \n",
    "    for model in MODEL_NAMES:\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        \n",
    "        # Filtrar datos de este modelo\n",
    "        model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "        \n",
    "        if not model_data:\n",
    "            print(f\"  ⚠ No hay datos para {model} - asignando métricas en 0\")\n",
    "            resultados_p3.append({\n",
    "                'modismo': 'N/A',\n",
    "                'ejemplo': '',\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'significado_real': '',\n",
    "                'literal_generado': '',\n",
    "                'definicion_generada': ''\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        referencias = [d['significado_real'] for d in model_data]\n",
    "        candidatos = [d['definicion_generada'] for d in model_data]\n",
    "        \n",
    "        # BERTScore: compara definición generada vs significado real\n",
    "        P, R, F1 = bert_func(candidatos, referencias)\n",
    "        \n",
    "        for idx, (p, r, f1) in enumerate(zip(P.tolist(), R.tolist(), F1.tolist())):\n",
    "            resultados_p3.append({\n",
    "                'modismo': model_data[idx]['modismo'],\n",
    "                'ejemplo': model_data[idx]['ejemplo'],\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': p,\n",
    "                'recall': r,\n",
    "                'f1_score': f1,\n",
    "                'significado_real': referencias[idx],\n",
    "                'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "                'definicion_generada': candidatos[idx]\n",
    "            })\n",
    "    \n",
    "    # Guardar resultados\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'prompt_3_{bert_name.lower()}_bertscore_resultados.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados_p3, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"RESULTADOS con {bert_name}:\")\n",
    "    for model in MODEL_NAMES:\n",
    "        model_results = [r for r in resultados_p3 if r['modelo'] == model]\n",
    "        if model_results:\n",
    "            f1_scores = [r['f1_score'] for r in model_results]\n",
    "            precisions = [r['precision'] for r in model_results]\n",
    "            recalls = [r['recall'] for r in model_results]\n",
    "            \n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            p_mean = np.mean(precisions)\n",
    "            r_mean = np.mean(recalls)\n",
    "            \n",
    "            print(f\"   {model}:\")\n",
    "            print(f\"      • Precision: {p_mean:.4f}\")\n",
    "            print(f\"      • Recall:    {r_mean:.4f}\")\n",
    "            print(f\"      • F1 Score:  {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Sentence-BERT Similarity\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando Sentence-BERT Similarity\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_sbert = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_sbert.append({\n",
    "            'modismo': 'N/A',\n",
    "            'ejemplo': '',\n",
    "            'modelo': model,\n",
    "            'similarity': 0.0,\n",
    "            'significado_real': '',\n",
    "            'literal_generado': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['significado_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular similitud con Sentence-BERT\n",
    "    similarities = compute_sbert_similarity(candidatos, referencias)\n",
    "    \n",
    "    for idx, sim in enumerate(similarities):\n",
    "        resultados_sbert.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'ejemplo': model_data[idx]['ejemplo'],\n",
    "            'modelo': model,\n",
    "            'similarity': float(sim),\n",
    "            'significado_real': referencias[idx],\n",
    "            'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_3_sbert_similarity_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_sbert, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con Sentence-BERT:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_sbert if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        similarities = [r['similarity'] for r in model_results]\n",
    "        sim_mean = np.mean(similarities)\n",
    "        sim_std = np.std(similarities)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Similitud: {sim_mean:.4f} (±{sim_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. chrF Score\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando chrF Score\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_chrf = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_chrf.append({\n",
    "            'modismo': 'N/A',\n",
    "            'ejemplo': '',\n",
    "            'modelo': model,\n",
    "            'chrf_score': 0.0,\n",
    "            'significado_real': '',\n",
    "            'literal_generado': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['significado_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular chrF\n",
    "    chrf_scores = compute_chrf_batch(candidatos, referencias)\n",
    "    \n",
    "    for idx, score in enumerate(chrf_scores):\n",
    "        resultados_chrf.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'ejemplo': model_data[idx]['ejemplo'],\n",
    "            'modelo': model,\n",
    "            'chrf_score': float(score),\n",
    "            'significado_real': referencias[idx],\n",
    "            'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_3_chrf_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_chrf, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con chrF:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_chrf if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        chrf_scores = [r['chrf_score'] for r in model_results]\n",
    "        chrf_mean = np.mean(chrf_scores)\n",
    "        chrf_std = np.std(chrf_scores)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • chrF: {chrf_mean:.4f} (±{chrf_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT 3 COMPLETADO\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
