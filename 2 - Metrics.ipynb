{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30cd0f2",
   "metadata": {},
   "source": [
    "# Evaluación de Modelos LLM en Modismos Colombianos\n",
    "\n",
    "Este notebook evalúa qué tan bien los modelos LLM entienden los modismos colombianos usando:\n",
    "- **BERTScore**: Para comparar similitud semántica entre textos (definiciones)\n",
    "- **Accuracy/Exact Match**: Para clasificación y coincidencias exactas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e03151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos en: Metricas LLM/Data_for_Metrics\n",
      "Resultados en: Metricas LLM/Metricas_Resultados\n",
      "Modelos: ['amazon/nova-micro-v1', 'microsoft/phi-4', 'amazon/nova-lite-v1', 'cohere/command-r-08-2024', 'qwen/qwen-2.5-72b-instruct', 'google/gemma-2-27b-it', 'meta-llama/llama-3.3-70b-instruct', 'microsoft/wizardlm-2-8x22b', 'meta-llama/llama-4-maverick', 'qwen/qwen2.5-vl-32b-instruct:free', 'x-ai/grok-3-mini-beta', 'perplexity/sonar', 'mistralai/mistral-medium-3', 'mistralai/mixtral-8x7b-instruct', 'google/gemini-2.5-flash', 'meta-llama/llama-3.1-405b-instruct', 'deepseek/deepseek-chat-v3.1', 'moonshotai/kimi-k2-0905', 'openai/o4-mini-high', 'openai/gpt-4.1']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Estilos para gráficos\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Modelos\n",
    "MODELS_FILE = 'Straico/text_model_usefull.txt'\n",
    "\n",
    "def load_models_from_file(filepath):\n",
    "    \"\"\"Carga los nombres de modelos desde un archivo de texto.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontró el archivo {filepath}\")\n",
    "        return []\n",
    "\n",
    "MODEL_NAMES = load_models_from_file(MODELS_FILE)[:20]\n",
    "\n",
    "# Directorios\n",
    "DATA_DIR = 'Metricas LLM/Data_for_Metrics'\n",
    "OUTPUT_DIR = 'Metricas LLM/Metricas_Resultados'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Datos en: {DATA_DIR}\")\n",
    "print(f\"Resultados en: {OUTPUT_DIR}\")\n",
    "print(f\"Modelos: {MODEL_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28bca76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Métricas cargadas correctamente:\n",
      "  - BERTScore (BETO y SciBETO)\n",
      "  - Sentence-BERT (paraphrase-multilingual-mpnet-base-v2)\n",
      "  - chrF (Character n-gram F-score)\n"
     ]
    }
   ],
   "source": [
    "# Importar funciones de métricas\n",
    "import sys\n",
    "sys.path.append('Metricas LLM')\n",
    "\n",
    "from BertScore import compute_bertscore_beto, compute_bertscore_sci_beto\n",
    "from SentenceBert import compute_sbert_similarity\n",
    "from chrF import compute_chrf_batch\n",
    "\n",
    "print(\"✓ Métricas cargadas correctamente:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO)\")\n",
    "print(\"  - Sentence-BERT (paraphrase-multilingual-mpnet-base-v2)\")\n",
    "print(\"  - chrF (Character n-gram F-score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14c7be",
   "metadata": {},
   "source": [
    "## 1. PROMPT 1: Modismo → Definición\n",
    "\n",
    "**Métrica**: BERTScore (F1)  \n",
    "**Justificación**: Evalúa similitud semántica entre la definición generada y la real. Ideal para entender si el modelo captura el significado del modismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd6f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 1: Modismo → Definición\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo puede generar una definición correcta del modismo\n",
      "Métricas:\n",
      "  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\n",
      "  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\n",
      "  - chrF: Character n-gram F-score\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 35728 registros válidos de 35728 totales\n",
      "\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con BETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0a269b6ccc4146ba79eb8a856cd9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "  ⚠ No hay datos para meta-llama/llama-4-maverick - asignando métricas en 0\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "  ⚠ No hay datos para qwen/qwen2.5-vl-32b-instruct:free - asignando métricas en 0\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "  ⚠ No hay datos para x-ai/grok-3-mini-beta - asignando métricas en 0\n",
      "Evaluando modelo: perplexity/sonar\n",
      "  ⚠ No hay datos para perplexity/sonar - asignando métricas en 0\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "  ⚠ No hay datos para mistralai/mistral-medium-3 - asignando métricas en 0\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "  ⚠ No hay datos para mistralai/mixtral-8x7b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "  ⚠ No hay datos para google/gemini-2.5-flash - asignando métricas en 0\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.1-405b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "  ⚠ No hay datos para deepseek/deepseek-chat-v3.1 - asignando métricas en 0\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "  ⚠ No hay datos para moonshotai/kimi-k2-0905 - asignando métricas en 0\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "  ⚠ No hay datos para openai/o4-mini-high - asignando métricas en 0\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "  ⚠ No hay datos para openai/gpt-4.1 - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.4709\n",
      "      • Recall:    0.4679\n",
      "      • F1 Score:  0.4684 (±0.0940)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.4430\n",
      "      • Recall:    0.4533\n",
      "      • F1 Score:  0.4471 (±0.0805)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.4737\n",
      "      • Recall:    0.4543\n",
      "      • F1 Score:  0.4627 (±0.0977)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.4745\n",
      "      • Recall:    0.4719\n",
      "      • F1 Score:  0.4722 (±0.0958)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.4783\n",
      "      • Recall:    0.4794\n",
      "      • F1 Score:  0.4778 (±0.1020)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.4617\n",
      "      • Recall:    0.4489\n",
      "      • F1 Score:  0.4543 (±0.0941)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.4553\n",
      "      • Recall:    0.4312\n",
      "      • F1 Score:  0.4419 (±0.0997)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.4696\n",
      "      • Recall:    0.4802\n",
      "      • F1 Score:  0.4738 (±0.0951)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   perplexity/sonar:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/o4-mini-high:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/gpt-4.1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "  ⚠ No hay datos para meta-llama/llama-4-maverick - asignando métricas en 0\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "  ⚠ No hay datos para qwen/qwen2.5-vl-32b-instruct:free - asignando métricas en 0\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "  ⚠ No hay datos para x-ai/grok-3-mini-beta - asignando métricas en 0\n",
      "Evaluando modelo: perplexity/sonar\n",
      "  ⚠ No hay datos para perplexity/sonar - asignando métricas en 0\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "  ⚠ No hay datos para mistralai/mistral-medium-3 - asignando métricas en 0\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "  ⚠ No hay datos para mistralai/mixtral-8x7b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "  ⚠ No hay datos para google/gemini-2.5-flash - asignando métricas en 0\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.1-405b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "  ⚠ No hay datos para deepseek/deepseek-chat-v3.1 - asignando métricas en 0\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "  ⚠ No hay datos para moonshotai/kimi-k2-0905 - asignando métricas en 0\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "  ⚠ No hay datos para openai/o4-mini-high - asignando métricas en 0\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "  ⚠ No hay datos para openai/gpt-4.1 - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7614\n",
      "      • Recall:    0.7535\n",
      "      • F1 Score:  0.7571 (±0.0352)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.7421\n",
      "      • Recall:    0.7486\n",
      "      • F1 Score:  0.7449 (±0.0283)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.7698\n",
      "      • Recall:    0.7490\n",
      "      • F1 Score:  0.7589 (±0.0384)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.7599\n",
      "      • Recall:    0.7548\n",
      "      • F1 Score:  0.7569 (±0.0342)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.7607\n",
      "      • Recall:    0.7583\n",
      "      • F1 Score:  0.7590 (±0.0359)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.7607\n",
      "      • Recall:    0.7474\n",
      "      • F1 Score:  0.7536 (±0.0356)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.7454\n",
      "      • Recall:    0.7330\n",
      "      • F1 Score:  0.7387 (±0.0437)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.7521\n",
      "      • Recall:    0.7568\n",
      "      • F1 Score:  0.7540 (±0.0342)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   perplexity/sonar:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/o4-mini-high:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/gpt-4.1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "  ⚠ No hay datos para meta-llama/llama-4-maverick - asignando métrica en 0\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "  ⚠ No hay datos para qwen/qwen2.5-vl-32b-instruct:free - asignando métrica en 0\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "  ⚠ No hay datos para x-ai/grok-3-mini-beta - asignando métrica en 0\n",
      "Evaluando modelo: perplexity/sonar\n",
      "  ⚠ No hay datos para perplexity/sonar - asignando métrica en 0\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "  ⚠ No hay datos para mistralai/mistral-medium-3 - asignando métrica en 0\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "  ⚠ No hay datos para mistralai/mixtral-8x7b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "  ⚠ No hay datos para google/gemini-2.5-flash - asignando métrica en 0\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.1-405b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "  ⚠ No hay datos para deepseek/deepseek-chat-v3.1 - asignando métrica en 0\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "  ⚠ No hay datos para moonshotai/kimi-k2-0905 - asignando métrica en 0\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "  ⚠ No hay datos para openai/o4-mini-high - asignando métrica en 0\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "  ⚠ No hay datos para openai/gpt-4.1 - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.3675 (±0.2116)\n",
      "   microsoft/phi-4:\n",
      "      • Similitud: 0.2980 (±0.1889)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Similitud: 0.3656 (±0.2129)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.3892 (±0.2234)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Similitud: 0.3917 (±0.2314)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Similitud: 0.3349 (±0.2003)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Similitud: 0.3724 (±0.2140)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Similitud: 0.3618 (±0.2179)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   perplexity/sonar:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   openai/o4-mini-high:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   openai/gpt-4.1:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "  ⚠ No hay datos para meta-llama/llama-4-maverick - asignando métrica en 0\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "  ⚠ No hay datos para qwen/qwen2.5-vl-32b-instruct:free - asignando métrica en 0\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "  ⚠ No hay datos para x-ai/grok-3-mini-beta - asignando métrica en 0\n",
      "Evaluando modelo: perplexity/sonar\n",
      "  ⚠ No hay datos para perplexity/sonar - asignando métrica en 0\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "  ⚠ No hay datos para mistralai/mistral-medium-3 - asignando métrica en 0\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "  ⚠ No hay datos para mistralai/mixtral-8x7b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "  ⚠ No hay datos para google/gemini-2.5-flash - asignando métrica en 0\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.1-405b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "  ⚠ No hay datos para deepseek/deepseek-chat-v3.1 - asignando métrica en 0\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "  ⚠ No hay datos para moonshotai/kimi-k2-0905 - asignando métrica en 0\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "  ⚠ No hay datos para openai/o4-mini-high - asignando métrica en 0\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "  ⚠ No hay datos para openai/gpt-4.1 - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2272 (±0.0776)\n",
      "   microsoft/phi-4:\n",
      "      • chrF: 0.2278 (±0.0617)\n",
      "   amazon/nova-lite-v1:\n",
      "      • chrF: 0.2042 (±0.0813)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.2260 (±0.0790)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • chrF: 0.2395 (±0.0893)\n",
      "   google/gemma-2-27b-it:\n",
      "      • chrF: 0.2014 (±0.0753)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • chrF: 0.1811 (±0.0831)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • chrF: 0.2454 (±0.0836)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   perplexity/sonar:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   google/gemini-2.5-flash:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   openai/o4-mini-high:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   openai/gpt-4.1:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 1 COMPLETADO\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 1: Modismo → Definición\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo puede generar una definición correcta del modismo\")\n",
    "print(\"Métricas:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\")\n",
    "print(\"  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\")\n",
    "print(\"  - chrF: Character n-gram F-score\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_1_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p1 = json.load(f)\n",
    "\n",
    "# Filtrar datos vacíos\n",
    "data_p1_valid = [d for d in data_p1 if d.get('definicion_real') and d.get('definicion_generada')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p1_valid)} registros válidos de {len(data_p1)} totales\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BERTScore con BETO y SciBETO\n",
    "# ============================================================================\n",
    "bert_models = [\n",
    "    ('BETO', compute_bertscore_beto),\n",
    "    ('SciBETO', compute_bertscore_sci_beto)\n",
    "]\n",
    "\n",
    "for bert_name, bert_func in bert_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Calculando BERTScore con {bert_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    resultados_p1 = []\n",
    "    \n",
    "    for model in MODEL_NAMES:\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        \n",
    "        # Filtrar datos de este modelo\n",
    "        model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "        \n",
    "        if not model_data:\n",
    "            print(f\"  ⚠ No hay datos para {model} - asignando métricas en 0\")\n",
    "            # Crear entrada con métricas en 0\n",
    "            resultados_p1.append({\n",
    "                'modismo': 'N/A',\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'definicion_real': '',\n",
    "                'definicion_generada': ''\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        referencias = [d['definicion_real'] for d in model_data]\n",
    "        candidatos = [d['definicion_generada'] for d in model_data]\n",
    "        \n",
    "        # BERTScore: compara candidatos (generados) vs referencias (reales)\n",
    "        P, R, F1 = bert_func(candidatos, referencias)\n",
    "        \n",
    "        for idx, (p, r, f1) in enumerate(zip(P.tolist(), R.tolist(), F1.tolist())):\n",
    "            resultados_p1.append({\n",
    "                'modismo': model_data[idx]['modismo'],\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': p,\n",
    "                'recall': r,\n",
    "                'f1_score': f1,\n",
    "                'definicion_real': referencias[idx],\n",
    "                'definicion_generada': candidatos[idx]\n",
    "            })\n",
    "    \n",
    "    # Guardar resultados\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'prompt_1_{bert_name.lower()}_bertscore_resultados.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados_p1, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"RESULTADOS con {bert_name}:\")\n",
    "    for model in MODEL_NAMES:\n",
    "        model_results = [r for r in resultados_p1 if r['modelo'] == model]\n",
    "        if model_results:\n",
    "            f1_scores = [r['f1_score'] for r in model_results]\n",
    "            precisions = [r['precision'] for r in model_results]\n",
    "            recalls = [r['recall'] for r in model_results]\n",
    "            \n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            p_mean = np.mean(precisions)\n",
    "            r_mean = np.mean(recalls)\n",
    "            \n",
    "            print(f\"   {model}:\")\n",
    "            print(f\"      • Precision: {p_mean:.4f}\")\n",
    "            print(f\"      • Recall:    {r_mean:.4f}\")\n",
    "            print(f\"      • F1 Score:  {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Sentence-BERT Similarity\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando Sentence-BERT Similarity\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_sbert = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_sbert.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'similarity': 0.0,\n",
    "            'definicion_real': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['definicion_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular similitud con Sentence-BERT\n",
    "    similarities = compute_sbert_similarity(candidatos, referencias)\n",
    "    \n",
    "    for idx, sim in enumerate(similarities):\n",
    "        resultados_sbert.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'modelo': model,\n",
    "            'similarity': float(sim),\n",
    "            'definicion_real': referencias[idx],\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_1_sbert_similarity_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_sbert, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con Sentence-BERT:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_sbert if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        similarities = [r['similarity'] for r in model_results]\n",
    "        sim_mean = np.mean(similarities)\n",
    "        sim_std = np.std(similarities)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Similitud: {sim_mean:.4f} (±{sim_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. chrF Score\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando chrF Score\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_chrf = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_chrf.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'chrf_score': 0.0,\n",
    "            'definicion_real': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['definicion_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular chrF\n",
    "    chrf_scores = compute_chrf_batch(candidatos, referencias)\n",
    "    \n",
    "    for idx, score in enumerate(chrf_scores):\n",
    "        resultados_chrf.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'modelo': model,\n",
    "            'chrf_score': float(score),\n",
    "            'definicion_real': referencias[idx],\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_1_chrf_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_chrf, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con chrF:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_chrf if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        chrf_scores = [r['chrf_score'] for r in model_results]\n",
    "        chrf_mean = np.mean(chrf_scores)\n",
    "        chrf_std = np.std(chrf_scores)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • chrF: {chrf_mean:.4f} (±{chrf_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT 1 COMPLETADO\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545337c",
   "metadata": {},
   "source": [
    "## 2. PROMPT 2: Modismo → Es Modismo (Sí/No)\n",
    "\n",
    "**Métrica**: Accuracy  \n",
    "**Justificación**: Es una tarea de clasificación binaria. Mide si el modelo identifica correctamente que una expresión es un modismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a5292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 2: Modismo → ¿Es Modismo? (Sí/No)\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo identifica correctamente que una expresión es un modismo\n",
      "Métrica: Accuracy (Exactitud)\n",
      "Justificación: Problema de clasificación binaria (Sí/No).\n",
      "   Mide el porcentaje de respuestas correctas.\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 35881 registros válidos de 35881 totales\n",
      "\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "  • Accuracy: 0.2106 (968/4597 correctos)\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "  • Accuracy: 0.6904 (2883/4176 correctos)\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "  • Accuracy: 0.3001 (1378/4592 correctos)\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "  • Accuracy: 0.6923 (3181/4595 correctos)\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  • Accuracy: 0.5729 (2380/4154 correctos)\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "  • Accuracy: 0.9403 (4303/4576 correctos)\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  • Accuracy: 0.7801 (3590/4602 correctos)\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "  • Accuracy: 0.4790 (2198/4589 correctos)\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "  ⚠ No hay datos para meta-llama/llama-4-maverick - asignando accuracy en 0\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "  ⚠ No hay datos para qwen/qwen2.5-vl-32b-instruct:free - asignando accuracy en 0\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "  ⚠ No hay datos para x-ai/grok-3-mini-beta - asignando accuracy en 0\n",
      "Evaluando modelo: perplexity/sonar\n",
      "  ⚠ No hay datos para perplexity/sonar - asignando accuracy en 0\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "  ⚠ No hay datos para mistralai/mistral-medium-3 - asignando accuracy en 0\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "  ⚠ No hay datos para mistralai/mixtral-8x7b-instruct - asignando accuracy en 0\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "  ⚠ No hay datos para google/gemini-2.5-flash - asignando accuracy en 0\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.1-405b-instruct - asignando accuracy en 0\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "  ⚠ No hay datos para deepseek/deepseek-chat-v3.1 - asignando accuracy en 0\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "  ⚠ No hay datos para moonshotai/kimi-k2-0905 - asignando accuracy en 0\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "  ⚠ No hay datos para openai/o4-mini-high - asignando accuracy en 0\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "  ⚠ No hay datos para openai/gpt-4.1 - asignando accuracy en 0\n",
      "\n",
      "RESULTADOS:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Accuracy:  0.2106 (968/4597 correctos)\n",
      "   microsoft/phi-4:\n",
      "      • Accuracy:  0.6904 (2883/4176 correctos)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Accuracy:  0.3001 (1378/4592 correctos)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Accuracy:  0.6923 (3181/4595 correctos)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Accuracy:  0.5729 (2380/4154 correctos)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Accuracy:  0.9403 (4303/4576 correctos)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Accuracy:  0.7801 (3590/4602 correctos)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Accuracy:  0.4790 (2198/4589 correctos)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   perplexity/sonar:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   openai/o4-mini-high:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   openai/gpt-4.1:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_2_accuracy_resultados.json\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 2: Modismo → ¿Es Modismo? (Sí/No)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo identifica correctamente que una expresión es un modismo\")\n",
    "print(\"Métrica: Accuracy (Exactitud)\")\n",
    "print(\"Justificación: Problema de clasificación binaria (Sí/No).\")\n",
    "print(\"   Mide el porcentaje de respuestas correctas.\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_2_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p2 = json.load(f)\n",
    "\n",
    "# Filtrar datos vacíos\n",
    "data_p2_valid = [d for d in data_p2 if d.get('es_modismo_real') and d.get('es_modismo_generado')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p2_valid)} registros válidos de {len(data_p2)} totales\")\n",
    "print()\n",
    "\n",
    "def normalizar_respuesta(respuesta):\n",
    "    \"\"\"Normaliza respuestas a 'Sí' o 'No'\"\"\"\n",
    "    if not respuesta:\n",
    "        return None\n",
    "    respuesta = str(respuesta).strip().lower()\n",
    "    if respuesta in ['sí', 'si', 'yes', 's', 'true', '1']:\n",
    "        return 'Sí'\n",
    "    elif respuesta in ['no', 'n', 'false', '0']:\n",
    "        return 'No'\n",
    "    return respuesta\n",
    "\n",
    "resultados_p2 = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    # Filtrar datos de este modelo\n",
    "    model_data = [d for d in data_p2_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando accuracy en 0\")\n",
    "        resultados_p2.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'respuesta_real': 'Sí',\n",
    "            'respuesta_generada': 'No',\n",
    "            'correcto': False\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Calcular accuracy\n",
    "    correctos = 0\n",
    "    for d in model_data:\n",
    "        real = normalizar_respuesta(d['es_modismo_real'])\n",
    "        generado = normalizar_respuesta(d['es_modismo_generado'])\n",
    "        correcto = (real == generado)\n",
    "        \n",
    "        if correcto:\n",
    "            correctos += 1\n",
    "        \n",
    "        resultados_p2.append({\n",
    "            'modismo': d['modismo'],\n",
    "            'modelo': model,\n",
    "            'respuesta_real': real,\n",
    "            'respuesta_generada': generado,\n",
    "            'correcto': correcto\n",
    "        })\n",
    "    \n",
    "    accuracy = correctos / len(model_data) if model_data else 0\n",
    "    print(f\"  • Accuracy: {accuracy:.4f} ({correctos}/{len(model_data)} correctos)\")\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_2_accuracy_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_p2, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_p2 if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        correctos = sum(1 for r in model_results if r['correcto'])\n",
    "        total = len(model_results)\n",
    "        accuracy = correctos / total if total > 0 else 0\n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Accuracy:  {accuracy:.4f} ({correctos}/{total} correctos)\")\n",
    "\n",
    "print()\n",
    "print(f\"✓ Guardado en: {output_file}\")\n",
    "print(\"=\"*80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03bb355",
   "metadata": {},
   "source": [
    "## 3. PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
    "\n",
    "**Métrica**: BERTScore (F1)  \n",
    "**Justificación**: Evalúa si el modelo puede generar interpretaciones literales del modismo en contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d89e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo genera interpretaciones literales correctas\n",
      "Métricas:\n",
      "  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\n",
      "  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\n",
      "  - chrF: Character n-gram F-score\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 26782 registros válidos de 27027 totales\n",
      "\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con BETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "  ⚠ No hay datos para google/gemma-2-27b-it - asignando métricas en 0\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "  ⚠ No hay datos para meta-llama/llama-4-maverick - asignando métricas en 0\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "  ⚠ No hay datos para qwen/qwen2.5-vl-32b-instruct:free - asignando métricas en 0\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "  ⚠ No hay datos para x-ai/grok-3-mini-beta - asignando métricas en 0\n",
      "Evaluando modelo: perplexity/sonar\n",
      "  ⚠ No hay datos para perplexity/sonar - asignando métricas en 0\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "  ⚠ No hay datos para mistralai/mistral-medium-3 - asignando métricas en 0\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "  ⚠ No hay datos para mistralai/mixtral-8x7b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "  ⚠ No hay datos para google/gemini-2.5-flash - asignando métricas en 0\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.1-405b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "  ⚠ No hay datos para deepseek/deepseek-chat-v3.1 - asignando métricas en 0\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "  ⚠ No hay datos para moonshotai/kimi-k2-0905 - asignando métricas en 0\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "  ⚠ No hay datos para openai/o4-mini-high - asignando métricas en 0\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "  ⚠ No hay datos para openai/gpt-4.1 - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.4854\n",
      "      • Recall:    0.4682\n",
      "      • F1 Score:  0.4756 (±0.0973)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.4741\n",
      "      • Recall:    0.4742\n",
      "      • F1 Score:  0.4732 (±0.0925)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.4982\n",
      "      • Recall:    0.4801\n",
      "      • F1 Score:  0.4879 (±0.0973)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.5057\n",
      "      • Recall:    0.4911\n",
      "      • F1 Score:  0.4971 (±0.0958)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.4705\n",
      "      • Recall:    0.4292\n",
      "      • F1 Score:  0.4478 (±0.0965)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.5189\n",
      "      • Recall:    0.5139\n",
      "      • F1 Score:  0.5152 (±0.1003)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   perplexity/sonar:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/o4-mini-high:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/gpt-4.1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "  ⚠ No hay datos para google/gemma-2-27b-it - asignando métricas en 0\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "  ⚠ No hay datos para meta-llama/llama-4-maverick - asignando métricas en 0\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "  ⚠ No hay datos para qwen/qwen2.5-vl-32b-instruct:free - asignando métricas en 0\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "  ⚠ No hay datos para x-ai/grok-3-mini-beta - asignando métricas en 0\n",
      "Evaluando modelo: perplexity/sonar\n",
      "  ⚠ No hay datos para perplexity/sonar - asignando métricas en 0\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "  ⚠ No hay datos para mistralai/mistral-medium-3 - asignando métricas en 0\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "  ⚠ No hay datos para mistralai/mixtral-8x7b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "  ⚠ No hay datos para google/gemini-2.5-flash - asignando métricas en 0\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.1-405b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "  ⚠ No hay datos para deepseek/deepseek-chat-v3.1 - asignando métricas en 0\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "  ⚠ No hay datos para moonshotai/kimi-k2-0905 - asignando métricas en 0\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "  ⚠ No hay datos para openai/o4-mini-high - asignando métricas en 0\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "  ⚠ No hay datos para openai/gpt-4.1 - asignando métricas en 0\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7426\n",
      "      • Recall:    0.7366\n",
      "      • F1 Score:  0.7392 (±0.0358)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.7334\n",
      "      • Recall:    0.7381\n",
      "      • F1 Score:  0.7354 (±0.0323)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.7457\n",
      "      • Recall:    0.7378\n",
      "      • F1 Score:  0.7413 (±0.0371)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.7731\n",
      "      • Recall:    0.7560\n",
      "      • F1 Score:  0.7640 (±0.0366)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.7369\n",
      "      • Recall:    0.7192\n",
      "      • F1 Score:  0.7276 (±0.0391)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.7784\n",
      "      • Recall:    0.7678\n",
      "      • F1 Score:  0.7726 (±0.0374)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   perplexity/sonar:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/o4-mini-high:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/gpt-4.1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "  ⚠ No hay datos para google/gemma-2-27b-it - asignando métrica en 0\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "  ⚠ No hay datos para meta-llama/llama-4-maverick - asignando métrica en 0\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "  ⚠ No hay datos para qwen/qwen2.5-vl-32b-instruct:free - asignando métrica en 0\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "  ⚠ No hay datos para x-ai/grok-3-mini-beta - asignando métrica en 0\n",
      "Evaluando modelo: perplexity/sonar\n",
      "  ⚠ No hay datos para perplexity/sonar - asignando métrica en 0\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "  ⚠ No hay datos para mistralai/mistral-medium-3 - asignando métrica en 0\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "  ⚠ No hay datos para mistralai/mixtral-8x7b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "  ⚠ No hay datos para google/gemini-2.5-flash - asignando métrica en 0\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.1-405b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "  ⚠ No hay datos para deepseek/deepseek-chat-v3.1 - asignando métrica en 0\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "  ⚠ No hay datos para moonshotai/kimi-k2-0905 - asignando métrica en 0\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "  ⚠ No hay datos para openai/o4-mini-high - asignando métrica en 0\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "  ⚠ No hay datos para openai/gpt-4.1 - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.5363 (±0.1980)\n",
      "   microsoft/phi-4:\n",
      "      • Similitud: 0.4931 (±0.2036)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Similitud: 0.5654 (±0.1950)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.5386 (±0.1955)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Similitud: 0.5388 (±0.1905)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Similitud: 0.5693 (±0.1964)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   perplexity/sonar:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   openai/o4-mini-high:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   openai/gpt-4.1:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2.5-72b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "  ⚠ No hay datos para google/gemma-2-27b-it - asignando métrica en 0\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "  ⚠ No hay datos para meta-llama/llama-4-maverick - asignando métrica en 0\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "  ⚠ No hay datos para qwen/qwen2.5-vl-32b-instruct:free - asignando métrica en 0\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "  ⚠ No hay datos para x-ai/grok-3-mini-beta - asignando métrica en 0\n",
      "Evaluando modelo: perplexity/sonar\n",
      "  ⚠ No hay datos para perplexity/sonar - asignando métrica en 0\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "  ⚠ No hay datos para mistralai/mistral-medium-3 - asignando métrica en 0\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "  ⚠ No hay datos para mistralai/mixtral-8x7b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "  ⚠ No hay datos para google/gemini-2.5-flash - asignando métrica en 0\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "  ⚠ No hay datos para meta-llama/llama-3.1-405b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "  ⚠ No hay datos para deepseek/deepseek-chat-v3.1 - asignando métrica en 0\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "  ⚠ No hay datos para moonshotai/kimi-k2-0905 - asignando métrica en 0\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "  ⚠ No hay datos para openai/o4-mini-high - asignando métrica en 0\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "  ⚠ No hay datos para openai/gpt-4.1 - asignando métrica en 0\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2217 (±0.0878)\n",
      "   microsoft/phi-4:\n",
      "      • chrF: 0.2343 (±0.0785)\n",
      "   amazon/nova-lite-v1:\n",
      "      • chrF: 0.2293 (±0.0950)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.2294 (±0.0888)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   google/gemma-2-27b-it:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • chrF: 0.1732 (±0.0837)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • chrF: 0.2558 (±0.0953)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   perplexity/sonar:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   google/gemini-2.5-flash:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   openai/o4-mini-high:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   openai/gpt-4.1:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 3 COMPLETADO\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 3: Modismo + Ejemplo → Literal + Definición\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo genera interpretaciones literales correctas\")\n",
    "print(\"Métricas:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\")\n",
    "print(\"  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\")\n",
    "print(\"  - chrF: Character n-gram F-score\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_3_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p3 = json.load(f)\n",
    "\n",
    "# Filtrar datos con literal y definición generados\n",
    "data_p3_valid = [d for d in data_p3 if d.get('significado_real') and d.get('definicion_generada')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p3_valid)} registros válidos de {len(data_p3)} totales\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BERTScore con BETO y SciBETO\n",
    "# ============================================================================\n",
    "bert_models = [\n",
    "    ('BETO', compute_bertscore_beto),\n",
    "    ('SciBETO', compute_bertscore_sci_beto)\n",
    "]\n",
    "\n",
    "for bert_name, bert_func in bert_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Calculando BERTScore con {bert_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    resultados_p3 = []\n",
    "    \n",
    "    for model in MODEL_NAMES:\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        \n",
    "        # Filtrar datos de este modelo\n",
    "        model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "        \n",
    "        if not model_data:\n",
    "            print(f\"  ⚠ No hay datos para {model} - asignando métricas en 0\")\n",
    "            resultados_p3.append({\n",
    "                'modismo': 'N/A',\n",
    "                'ejemplo': '',\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'significado_real': '',\n",
    "                'literal_generado': '',\n",
    "                'definicion_generada': ''\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        referencias = [d['significado_real'] for d in model_data]\n",
    "        candidatos = [d['definicion_generada'] for d in model_data]\n",
    "        \n",
    "        # BERTScore: compara definición generada vs significado real\n",
    "        P, R, F1 = bert_func(candidatos, referencias)\n",
    "        \n",
    "        for idx, (p, r, f1) in enumerate(zip(P.tolist(), R.tolist(), F1.tolist())):\n",
    "            resultados_p3.append({\n",
    "                'modismo': model_data[idx]['modismo'],\n",
    "                'ejemplo': model_data[idx]['ejemplo'],\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': p,\n",
    "                'recall': r,\n",
    "                'f1_score': f1,\n",
    "                'significado_real': referencias[idx],\n",
    "                'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "                'definicion_generada': candidatos[idx]\n",
    "            })\n",
    "    \n",
    "    # Guardar resultados\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'prompt_3_{bert_name.lower()}_bertscore_resultados.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados_p3, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"RESULTADOS con {bert_name}:\")\n",
    "    for model in MODEL_NAMES:\n",
    "        model_results = [r for r in resultados_p3 if r['modelo'] == model]\n",
    "        if model_results:\n",
    "            f1_scores = [r['f1_score'] for r in model_results]\n",
    "            precisions = [r['precision'] for r in model_results]\n",
    "            recalls = [r['recall'] for r in model_results]\n",
    "            \n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            p_mean = np.mean(precisions)\n",
    "            r_mean = np.mean(recalls)\n",
    "            \n",
    "            print(f\"   {model}:\")\n",
    "            print(f\"      • Precision: {p_mean:.4f}\")\n",
    "            print(f\"      • Recall:    {r_mean:.4f}\")\n",
    "            print(f\"      • F1 Score:  {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Sentence-BERT Similarity\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando Sentence-BERT Similarity\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_sbert = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_sbert.append({\n",
    "            'modismo': 'N/A',\n",
    "            'ejemplo': '',\n",
    "            'modelo': model,\n",
    "            'similarity': 0.0,\n",
    "            'significado_real': '',\n",
    "            'literal_generado': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['significado_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular similitud con Sentence-BERT\n",
    "    similarities = compute_sbert_similarity(candidatos, referencias)\n",
    "    \n",
    "    for idx, sim in enumerate(similarities):\n",
    "        resultados_sbert.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'ejemplo': model_data[idx]['ejemplo'],\n",
    "            'modelo': model,\n",
    "            'similarity': float(sim),\n",
    "            'significado_real': referencias[idx],\n",
    "            'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_3_sbert_similarity_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_sbert, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con Sentence-BERT:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_sbert if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        similarities = [r['similarity'] for r in model_results]\n",
    "        sim_mean = np.mean(similarities)\n",
    "        sim_std = np.std(similarities)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Similitud: {sim_mean:.4f} (±{sim_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. chrF Score\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando chrF Score\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_chrf = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_chrf.append({\n",
    "            'modismo': 'N/A',\n",
    "            'ejemplo': '',\n",
    "            'modelo': model,\n",
    "            'chrf_score': 0.0,\n",
    "            'significado_real': '',\n",
    "            'literal_generado': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['significado_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular chrF\n",
    "    chrf_scores = compute_chrf_batch(candidatos, referencias)\n",
    "    \n",
    "    for idx, score in enumerate(chrf_scores):\n",
    "        resultados_chrf.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'ejemplo': model_data[idx]['ejemplo'],\n",
    "            'modelo': model,\n",
    "            'chrf_score': float(score),\n",
    "            'significado_real': referencias[idx],\n",
    "            'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_3_chrf_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_chrf, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con chrF:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_chrf if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        chrf_scores = [r['chrf_score'] for r in model_results]\n",
    "        chrf_mean = np.mean(chrf_scores)\n",
    "        chrf_std = np.std(chrf_scores)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • chrF: {chrf_mean:.4f} (±{chrf_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT 3 COMPLETADO\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
