{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30cd0f2",
   "metadata": {},
   "source": [
    "# Evaluación de Modelos LLM en Modismos Colombianos\n",
    "\n",
    "Este notebook evalúa qué tan bien los modelos LLM entienden los modismos colombianos usando:\n",
    "- **BERTScore**: Para comparar similitud semántica entre textos (definiciones)\n",
    "- **Accuracy/Exact Match**: Para clasificación y coincidencias exactas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e03151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos en: Metricas LLM/Data_for_Metrics\n",
      "Resultados en: Metricas LLM/Metricas_Resultados\n",
      "Modelos: ['amazon/nova-micro-v1', 'microsoft/phi-4', 'amazon/nova-lite-v1', 'cohere/command-r-08-2024', 'google/gemini-2.0-flash-001', 'qwen/qwen-2-vl-72b-instruct', 'qwen/qwen-2.5-72b-instruct', 'google/gemma-2-27b-it', 'google/gemini-2.5-flash-lite', 'meta-llama/llama-3.3-70b-instruct', 'minimax/minimax-m2:free', 'nvidia/llama-3.3-nemotron-super-49b-v1.5', 'nvidia/llama-3.3-nemotron-super-49b-v1', 'openai/gpt-4.1-nano', 'openai/gpt-5-nano', 'qwen/qwen-2-72b-instruct', 'qwen/qwen3-235b-a22b', 'microsoft/wizardlm-2-8x22b', 'meta-llama/llama-3.1-70b-instruct', 'meta-llama/llama-4-maverick']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Estilos para gráficos\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Modelos\n",
    "MODELS_FILE = 'Straico/text_model_usefull.txt'\n",
    "\n",
    "def load_models_from_file(filepath):\n",
    "    \"\"\"Carga los nombres de modelos desde un archivo de texto.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontró el archivo {filepath}\")\n",
    "        return []\n",
    "\n",
    "MODEL_NAMES = load_models_from_file(MODELS_FILE)[:20]\n",
    "\n",
    "# Directorios\n",
    "DATA_DIR = 'Metricas LLM/Data_for_Metrics'\n",
    "OUTPUT_DIR = 'Metricas LLM/Metricas_Resultados'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Datos en: {DATA_DIR}\")\n",
    "print(f\"Resultados en: {OUTPUT_DIR}\")\n",
    "print(f\"Modelos: {MODEL_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28bca76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Métricas cargadas correctamente:\n",
      "  - BERTScore (BETO y SciBETO)\n",
      "  - Sentence-BERT (paraphrase-multilingual-mpnet-base-v2)\n",
      "  - chrF (Character n-gram F-score)\n"
     ]
    }
   ],
   "source": [
    "# Importar funciones de métricas\n",
    "import sys\n",
    "sys.path.append('Metricas LLM')\n",
    "\n",
    "from BertScore import compute_bertscore_beto, compute_bertscore_sci_beto\n",
    "from SentenceBert import compute_sbert_similarity\n",
    "from chrF import compute_chrf_batch\n",
    "\n",
    "print(\"✓ Métricas cargadas correctamente:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO)\")\n",
    "print(\"  - Sentence-BERT (paraphrase-multilingual-mpnet-base-v2)\")\n",
    "print(\"  - chrF (Character n-gram F-score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14c7be",
   "metadata": {},
   "source": [
    "## 1. PROMPT 1: Modismo → Definición\n",
    "\n",
    "**Métrica**: BERTScore (F1)  \n",
    "**Justificación**: Evalúa similitud semántica entre la definición generada y la real. Ideal para entender si el modelo captura el significado del modismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2dd6f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 1: Modismo → Definición\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo puede generar una definición correcta del modismo\n",
      "Métricas:\n",
      "  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\n",
      "  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\n",
      "  - chrF: Character n-gram F-score\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 324 registros válidos de 324 totales\n",
      "\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con BETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: minimax/minimax-m2:free\n",
      "  ⚠ No hay datos para minimax/minimax-m2:free - asignando métricas en 0\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1\n",
      "  ⚠ No hay datos para nvidia/llama-3.3-nemotron-super-49b-v1 - asignando métricas en 0\n",
      "Evaluando modelo: openai/gpt-4.1-nano\n",
      "Evaluando modelo: openai/gpt-5-nano\n",
      "Evaluando modelo: qwen/qwen-2-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2-72b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: qwen/qwen3-235b-a22b\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-3.1-70b-instruct\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.4428\n",
      "      • Recall:    0.4315\n",
      "      • F1 Score:  0.4367 (±0.0656)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.4450\n",
      "      • Recall:    0.4412\n",
      "      • F1 Score:  0.4423 (±0.0935)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.4608\n",
      "      • Recall:    0.4246\n",
      "      • F1 Score:  0.4408 (±0.0791)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.4972\n",
      "      • Recall:    0.4756\n",
      "      • F1 Score:  0.4854 (±0.1061)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.5138\n",
      "      • Recall:    0.5115\n",
      "      • F1 Score:  0.5120 (±0.0973)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.4709\n",
      "      • Recall:    0.4669\n",
      "      • F1 Score:  0.4681 (±0.0846)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.4830\n",
      "      • Recall:    0.4729\n",
      "      • F1 Score:  0.4774 (±0.0773)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.4631\n",
      "      • Recall:    0.4281\n",
      "      • F1 Score:  0.4442 (±0.1050)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.4950\n",
      "      • Recall:    0.4879\n",
      "      • F1 Score:  0.4906 (±0.1048)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.4316\n",
      "      • Recall:    0.3913\n",
      "      • F1 Score:  0.4095 (±0.0672)\n",
      "   minimax/minimax-m2:free:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1.5:\n",
      "      • Precision: 0.4813\n",
      "      • Recall:    0.4738\n",
      "      • F1 Score:  0.4763 (±0.0882)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/gpt-4.1-nano:\n",
      "      • Precision: 0.4508\n",
      "      • Recall:    0.4453\n",
      "      • F1 Score:  0.4475 (±0.0683)\n",
      "   openai/gpt-5-nano:\n",
      "      • Precision: 0.4587\n",
      "      • Recall:    0.4582\n",
      "      • F1 Score:  0.4578 (±0.0812)\n",
      "   qwen/qwen-2-72b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   qwen/qwen3-235b-a22b:\n",
      "      • Precision: 0.4449\n",
      "      • Recall:    0.4382\n",
      "      • F1 Score:  0.4409 (±0.0513)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.4472\n",
      "      • Recall:    0.4501\n",
      "      • F1 Score:  0.4476 (±0.0893)\n",
      "   meta-llama/llama-3.1-70b-instruct:\n",
      "      • Precision: 0.4623\n",
      "      • Recall:    0.4319\n",
      "      • F1 Score:  0.4456 (±0.0689)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.4907\n",
      "      • Recall:    0.4878\n",
      "      • F1 Score:  0.4882 (±0.0950)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: minimax/minimax-m2:free\n",
      "  ⚠ No hay datos para minimax/minimax-m2:free - asignando métricas en 0\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1\n",
      "  ⚠ No hay datos para nvidia/llama-3.3-nemotron-super-49b-v1 - asignando métricas en 0\n",
      "Evaluando modelo: openai/gpt-4.1-nano\n",
      "Evaluando modelo: openai/gpt-5-nano\n",
      "Evaluando modelo: qwen/qwen-2-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2-72b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: qwen/qwen3-235b-a22b\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-3.1-70b-instruct\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7544\n",
      "      • Recall:    0.7443\n",
      "      • F1 Score:  0.7489 (±0.0281)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.7478\n",
      "      • Recall:    0.7480\n",
      "      • F1 Score:  0.7474 (±0.0285)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.7679\n",
      "      • Recall:    0.7445\n",
      "      • F1 Score:  0.7557 (±0.0397)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.7668\n",
      "      • Recall:    0.7571\n",
      "      • F1 Score:  0.7616 (±0.0391)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.7724\n",
      "      • Recall:    0.7786\n",
      "      • F1 Score:  0.7751 (±0.0454)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.7591\n",
      "      • Recall:    0.7619\n",
      "      • F1 Score:  0.7601 (±0.0290)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.7704\n",
      "      • Recall:    0.7668\n",
      "      • F1 Score:  0.7683 (±0.0337)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.7672\n",
      "      • Recall:    0.7476\n",
      "      • F1 Score:  0.7568 (±0.0385)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.7691\n",
      "      • Recall:    0.7659\n",
      "      • F1 Score:  0.7670 (±0.0328)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.7585\n",
      "      • Recall:    0.7313\n",
      "      • F1 Score:  0.7443 (±0.0315)\n",
      "   minimax/minimax-m2:free:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1.5:\n",
      "      • Precision: 0.7740\n",
      "      • Recall:    0.7672\n",
      "      • F1 Score:  0.7701 (±0.0474)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/gpt-4.1-nano:\n",
      "      • Precision: 0.7583\n",
      "      • Recall:    0.7553\n",
      "      • F1 Score:  0.7565 (±0.0353)\n",
      "   openai/gpt-5-nano:\n",
      "      • Precision: 0.7555\n",
      "      • Recall:    0.7585\n",
      "      • F1 Score:  0.7565 (±0.0239)\n",
      "   qwen/qwen-2-72b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   qwen/qwen3-235b-a22b:\n",
      "      • Precision: 0.7401\n",
      "      • Recall:    0.7354\n",
      "      • F1 Score:  0.7376 (±0.0296)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.7473\n",
      "      • Recall:    0.7541\n",
      "      • F1 Score:  0.7504 (±0.0238)\n",
      "   meta-llama/llama-3.1-70b-instruct:\n",
      "      • Precision: 0.7575\n",
      "      • Recall:    0.7414\n",
      "      • F1 Score:  0.7489 (±0.0346)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.7591\n",
      "      • Recall:    0.7643\n",
      "      • F1 Score:  0.7611 (±0.0426)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: minimax/minimax-m2:free\n",
      "  ⚠ No hay datos para minimax/minimax-m2:free - asignando métrica en 0\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1\n",
      "  ⚠ No hay datos para nvidia/llama-3.3-nemotron-super-49b-v1 - asignando métrica en 0\n",
      "Evaluando modelo: openai/gpt-4.1-nano\n",
      "Evaluando modelo: openai/gpt-5-nano\n",
      "Evaluando modelo: qwen/qwen-2-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2-72b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: qwen/qwen3-235b-a22b\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-3.1-70b-instruct\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.2634 (±0.1732)\n",
      "   microsoft/phi-4:\n",
      "      • Similitud: 0.2339 (±0.1675)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Similitud: 0.3031 (±0.1494)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.4127 (±0.2306)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Similitud: 0.3990 (±0.2485)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Similitud: 0.3744 (±0.2051)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Similitud: 0.3861 (±0.2355)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Similitud: 0.2911 (±0.1795)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Similitud: 0.4110 (±0.1935)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Similitud: 0.2730 (±0.1409)\n",
      "   minimax/minimax-m2:free:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1.5:\n",
      "      • Similitud: 0.3444 (±0.2106)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   openai/gpt-4.1-nano:\n",
      "      • Similitud: 0.3125 (±0.1605)\n",
      "   openai/gpt-5-nano:\n",
      "      • Similitud: 0.3257 (±0.1772)\n",
      "   qwen/qwen-2-72b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   qwen/qwen3-235b-a22b:\n",
      "      • Similitud: 0.3771 (±0.1772)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Similitud: 0.2932 (±0.1500)\n",
      "   meta-llama/llama-3.1-70b-instruct:\n",
      "      • Similitud: 0.3113 (±0.1422)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Similitud: 0.3898 (±0.2272)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: minimax/minimax-m2:free\n",
      "  ⚠ No hay datos para minimax/minimax-m2:free - asignando métrica en 0\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1\n",
      "  ⚠ No hay datos para nvidia/llama-3.3-nemotron-super-49b-v1 - asignando métrica en 0\n",
      "Evaluando modelo: openai/gpt-4.1-nano\n",
      "Evaluando modelo: openai/gpt-5-nano\n",
      "Evaluando modelo: qwen/qwen-2-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2-72b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: qwen/qwen3-235b-a22b\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-3.1-70b-instruct\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2057 (±0.0424)\n",
      "   microsoft/phi-4:\n",
      "      • chrF: 0.2359 (±0.0583)\n",
      "   amazon/nova-lite-v1:\n",
      "      • chrF: 0.1858 (±0.0523)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.2296 (±0.0721)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • chrF: 0.2998 (±0.1345)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • chrF: 0.2579 (±0.0893)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • chrF: 0.2362 (±0.0647)\n",
      "   google/gemma-2-27b-it:\n",
      "      • chrF: 0.2095 (±0.0938)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • chrF: 0.2522 (±0.0799)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • chrF: 0.1716 (±0.0386)\n",
      "   minimax/minimax-m2:free:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1.5:\n",
      "      • chrF: 0.2825 (±0.1478)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   openai/gpt-4.1-nano:\n",
      "      • chrF: 0.2454 (±0.0968)\n",
      "   openai/gpt-5-nano:\n",
      "      • chrF: 0.2591 (±0.0918)\n",
      "   qwen/qwen-2-72b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   qwen/qwen3-235b-a22b:\n",
      "      • chrF: 0.2330 (±0.0940)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • chrF: 0.2336 (±0.0453)\n",
      "   meta-llama/llama-3.1-70b-instruct:\n",
      "      • chrF: 0.1952 (±0.0513)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • chrF: 0.2893 (±0.1066)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 1 COMPLETADO\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 1: Modismo → Definición\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo puede generar una definición correcta del modismo\")\n",
    "print(\"Métricas:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\")\n",
    "print(\"  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\")\n",
    "print(\"  - chrF: Character n-gram F-score\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_1_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p1 = json.load(f)\n",
    "\n",
    "# Filtrar datos vacíos\n",
    "data_p1_valid = [d for d in data_p1 if d.get('definicion_real') and d.get('definicion_generada')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p1_valid)} registros válidos de {len(data_p1)} totales\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BERTScore con BETO y SciBETO\n",
    "# ============================================================================\n",
    "bert_models = [\n",
    "    ('BETO', compute_bertscore_beto),\n",
    "    ('SciBETO', compute_bertscore_sci_beto)\n",
    "]\n",
    "\n",
    "for bert_name, bert_func in bert_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Calculando BERTScore con {bert_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    resultados_p1 = []\n",
    "    \n",
    "    for model in MODEL_NAMES:\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        \n",
    "        # Filtrar datos de este modelo\n",
    "        model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "        \n",
    "        if not model_data:\n",
    "            print(f\"  ⚠ No hay datos para {model} - asignando métricas en 0\")\n",
    "            # Crear entrada con métricas en 0\n",
    "            resultados_p1.append({\n",
    "                'modismo': 'N/A',\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'definicion_real': '',\n",
    "                'definicion_generada': ''\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        referencias = [d['definicion_real'] for d in model_data]\n",
    "        candidatos = [d['definicion_generada'] for d in model_data]\n",
    "        \n",
    "        # BERTScore: compara candidatos (generados) vs referencias (reales)\n",
    "        P, R, F1 = bert_func(candidatos, referencias)\n",
    "        \n",
    "        for idx, (p, r, f1) in enumerate(zip(P.tolist(), R.tolist(), F1.tolist())):\n",
    "            resultados_p1.append({\n",
    "                'modismo': model_data[idx]['modismo'],\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': p,\n",
    "                'recall': r,\n",
    "                'f1_score': f1,\n",
    "                'definicion_real': referencias[idx],\n",
    "                'definicion_generada': candidatos[idx]\n",
    "            })\n",
    "    \n",
    "    # Guardar resultados\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'prompt_1_{bert_name.lower()}_bertscore_resultados.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados_p1, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"RESULTADOS con {bert_name}:\")\n",
    "    for model in MODEL_NAMES:\n",
    "        model_results = [r for r in resultados_p1 if r['modelo'] == model]\n",
    "        if model_results:\n",
    "            f1_scores = [r['f1_score'] for r in model_results]\n",
    "            precisions = [r['precision'] for r in model_results]\n",
    "            recalls = [r['recall'] for r in model_results]\n",
    "            \n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            p_mean = np.mean(precisions)\n",
    "            r_mean = np.mean(recalls)\n",
    "            \n",
    "            print(f\"   {model}:\")\n",
    "            print(f\"      • Precision: {p_mean:.4f}\")\n",
    "            print(f\"      • Recall:    {r_mean:.4f}\")\n",
    "            print(f\"      • F1 Score:  {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Sentence-BERT Similarity\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando Sentence-BERT Similarity\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_sbert = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_sbert.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'similarity': 0.0,\n",
    "            'definicion_real': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['definicion_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular similitud con Sentence-BERT\n",
    "    similarities = compute_sbert_similarity(candidatos, referencias)\n",
    "    \n",
    "    for idx, sim in enumerate(similarities):\n",
    "        resultados_sbert.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'modelo': model,\n",
    "            'similarity': float(sim),\n",
    "            'definicion_real': referencias[idx],\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_1_sbert_similarity_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_sbert, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con Sentence-BERT:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_sbert if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        similarities = [r['similarity'] for r in model_results]\n",
    "        sim_mean = np.mean(similarities)\n",
    "        sim_std = np.std(similarities)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Similitud: {sim_mean:.4f} (±{sim_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. chrF Score\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando chrF Score\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_chrf = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_chrf.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'chrf_score': 0.0,\n",
    "            'definicion_real': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['definicion_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular chrF\n",
    "    chrf_scores = compute_chrf_batch(candidatos, referencias)\n",
    "    \n",
    "    for idx, score in enumerate(chrf_scores):\n",
    "        resultados_chrf.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'modelo': model,\n",
    "            'chrf_score': float(score),\n",
    "            'definicion_real': referencias[idx],\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_1_chrf_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_chrf, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con chrF:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_chrf if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        chrf_scores = [r['chrf_score'] for r in model_results]\n",
    "        chrf_mean = np.mean(chrf_scores)\n",
    "        chrf_std = np.std(chrf_scores)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • chrF: {chrf_mean:.4f} (±{chrf_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT 1 COMPLETADO\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545337c",
   "metadata": {},
   "source": [
    "## 2. PROMPT 2: Modismo → Es Modismo (Sí/No)\n",
    "\n",
    "**Métrica**: Accuracy  \n",
    "**Justificación**: Es una tarea de clasificación binaria. Mide si el modelo identifica correctamente que una expresión es un modismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a5292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 2: Modismo → ¿Es Modismo? (Sí/No)\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo identifica correctamente que una expresión es un modismo\n",
      "Métrica: Accuracy (Exactitud)\n",
      "Justificación: Problema de clasificación binaria (Sí/No).\n",
      "   Mide el porcentaje de respuestas correctas.\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 325 registros válidos de 325 totales\n",
      "\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "  • Accuracy: 0.2500 (5/20 correctos)\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "  • Accuracy: 0.7778 (14/18 correctos)\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "  • Accuracy: 0.3500 (7/20 correctos)\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "  • Accuracy: 0.6500 (13/20 correctos)\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "  • Accuracy: 0.8500 (17/20 correctos)\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "  • Accuracy: 0.4500 (9/20 correctos)\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  • Accuracy: 0.5789 (11/19 correctos)\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "  • Accuracy: 0.9500 (19/20 correctos)\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "  • Accuracy: 0.4500 (9/20 correctos)\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  • Accuracy: 0.8000 (16/20 correctos)\n",
      "Evaluando modelo: minimax/minimax-m2:free\n",
      "  ⚠ No hay datos para minimax/minimax-m2:free - asignando accuracy en 0\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      "  • Accuracy: 0.4500 (9/20 correctos)\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1\n",
      "  ⚠ No hay datos para nvidia/llama-3.3-nemotron-super-49b-v1 - asignando accuracy en 0\n",
      "Evaluando modelo: openai/gpt-4.1-nano\n",
      "  • Accuracy: 0.5500 (11/20 correctos)\n",
      "Evaluando modelo: openai/gpt-5-nano\n",
      "  • Accuracy: 0.0000 (0/20 correctos)\n",
      "Evaluando modelo: qwen/qwen-2-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2-72b-instruct - asignando accuracy en 0\n",
      "Evaluando modelo: qwen/qwen3-235b-a22b\n",
      "  • Accuracy: 0.5556 (5/9 correctos)\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "  • Accuracy: 0.5000 (10/20 correctos)\n",
      "Evaluando modelo: meta-llama/llama-3.1-70b-instruct\n",
      "  • Accuracy: 0.7895 (15/19 correctos)\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "  • Accuracy: 0.6500 (13/20 correctos)\n",
      "\n",
      "RESULTADOS:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Accuracy:  0.2500 (5/20 correctos)\n",
      "   microsoft/phi-4:\n",
      "      • Accuracy:  0.7778 (14/18 correctos)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Accuracy:  0.3500 (7/20 correctos)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Accuracy:  0.6500 (13/20 correctos)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Accuracy:  0.8500 (17/20 correctos)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Accuracy:  0.4500 (9/20 correctos)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Accuracy:  0.5789 (11/19 correctos)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Accuracy:  0.9500 (19/20 correctos)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Accuracy:  0.4500 (9/20 correctos)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Accuracy:  0.8000 (16/20 correctos)\n",
      "   minimax/minimax-m2:free:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1.5:\n",
      "      • Accuracy:  0.4500 (9/20 correctos)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   openai/gpt-4.1-nano:\n",
      "      • Accuracy:  0.5500 (11/20 correctos)\n",
      "   openai/gpt-5-nano:\n",
      "      • Accuracy:  0.0000 (0/20 correctos)\n",
      "   qwen/qwen-2-72b-instruct:\n",
      "      • Accuracy:  0.0000 (0/1 correctos)\n",
      "   qwen/qwen3-235b-a22b:\n",
      "      • Accuracy:  0.5556 (5/9 correctos)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Accuracy:  0.5000 (10/20 correctos)\n",
      "   meta-llama/llama-3.1-70b-instruct:\n",
      "      • Accuracy:  0.7895 (15/19 correctos)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Accuracy:  0.6500 (13/20 correctos)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_2_accuracy_resultados.json\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 2: Modismo → ¿Es Modismo? (Sí/No)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo identifica correctamente que una expresión es un modismo\")\n",
    "print(\"Métrica: Accuracy (Exactitud)\")\n",
    "print(\"Justificación: Problema de clasificación binaria (Sí/No).\")\n",
    "print(\"   Mide el porcentaje de respuestas correctas.\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_2_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p2 = json.load(f)\n",
    "\n",
    "# Filtrar datos vacíos\n",
    "data_p2_valid = [d for d in data_p2 if d.get('es_modismo_real') and d.get('es_modismo_generado')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p2_valid)} registros válidos de {len(data_p2)} totales\")\n",
    "print()\n",
    "\n",
    "def normalizar_respuesta(respuesta):\n",
    "    \"\"\"Normaliza respuestas a 'Sí' o 'No'\"\"\"\n",
    "    if not respuesta:\n",
    "        return None\n",
    "    respuesta = str(respuesta).strip().lower()\n",
    "    if respuesta in ['sí', 'si', 'yes', 's', 'true', '1']:\n",
    "        return 'Sí'\n",
    "    elif respuesta in ['no', 'n', 'false', '0']:\n",
    "        return 'No'\n",
    "    return respuesta\n",
    "\n",
    "resultados_p2 = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    # Filtrar datos de este modelo\n",
    "    model_data = [d for d in data_p2_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando accuracy en 0\")\n",
    "        resultados_p2.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'respuesta_real': 'Sí',\n",
    "            'respuesta_generada': 'No',\n",
    "            'correcto': False\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Calcular accuracy\n",
    "    correctos = 0\n",
    "    for d in model_data:\n",
    "        real = normalizar_respuesta(d['es_modismo_real'])\n",
    "        generado = normalizar_respuesta(d['es_modismo_generado'])\n",
    "        correcto = (real == generado)\n",
    "        \n",
    "        if correcto:\n",
    "            correctos += 1\n",
    "        \n",
    "        resultados_p2.append({\n",
    "            'modismo': d['modismo'],\n",
    "            'modelo': model,\n",
    "            'respuesta_real': real,\n",
    "            'respuesta_generada': generado,\n",
    "            'correcto': correcto\n",
    "        })\n",
    "    \n",
    "    accuracy = correctos / len(model_data) if model_data else 0\n",
    "    print(f\"  • Accuracy: {accuracy:.4f} ({correctos}/{len(model_data)} correctos)\")\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_2_accuracy_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_p2, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_p2 if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        correctos = sum(1 for r in model_results if r['correcto'])\n",
    "        total = len(model_results)\n",
    "        accuracy = correctos / total if total > 0 else 0\n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Accuracy:  {accuracy:.4f} ({correctos}/{total} correctos)\")\n",
    "\n",
    "print()\n",
    "print(f\"✓ Guardado en: {output_file}\")\n",
    "print(\"=\"*80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03bb355",
   "metadata": {},
   "source": [
    "## 3. PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
    "\n",
    "**Métrica**: BERTScore (F1)  \n",
    "**Justificación**: Evalúa si el modelo puede generar interpretaciones literales del modismo en contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d89e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo genera interpretaciones literales correctas\n",
      "Métricas:\n",
      "  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\n",
      "  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\n",
      "  - chrF: Character n-gram F-score\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 323 registros válidos de 324 totales\n",
      "\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con BETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: minimax/minimax-m2:free\n",
      "  ⚠ No hay datos para minimax/minimax-m2:free - asignando métricas en 0\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1\n",
      "  ⚠ No hay datos para nvidia/llama-3.3-nemotron-super-49b-v1 - asignando métricas en 0\n",
      "Evaluando modelo: openai/gpt-4.1-nano\n",
      "Evaluando modelo: openai/gpt-5-nano\n",
      "Evaluando modelo: qwen/qwen-2-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2-72b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: qwen/qwen3-235b-a22b\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-3.1-70b-instruct\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.4965\n",
      "      • Recall:    0.4715\n",
      "      • F1 Score:  0.4831 (±0.0825)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.4983\n",
      "      • Recall:    0.4752\n",
      "      • F1 Score:  0.4861 (±0.0904)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.5052\n",
      "      • Recall:    0.4817\n",
      "      • F1 Score:  0.4925 (±0.0855)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.4749\n",
      "      • Recall:    0.4323\n",
      "      • F1 Score:  0.4521 (±0.0815)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.5596\n",
      "      • Recall:    0.5130\n",
      "      • F1 Score:  0.5347 (±0.0953)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.5489\n",
      "      • Recall:    0.5052\n",
      "      • F1 Score:  0.5254 (±0.0882)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.5217\n",
      "      • Recall:    0.4820\n",
      "      • F1 Score:  0.5004 (±0.1007)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.4931\n",
      "      • Recall:    0.4585\n",
      "      • F1 Score:  0.4744 (±0.0870)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.5768\n",
      "      • Recall:    0.5384\n",
      "      • F1 Score:  0.5563 (±0.0853)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.4501\n",
      "      • Recall:    0.4039\n",
      "      • F1 Score:  0.4253 (±0.0897)\n",
      "   minimax/minimax-m2:free:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1.5:\n",
      "      • Precision: 0.5030\n",
      "      • Recall:    0.4701\n",
      "      • F1 Score:  0.4854 (±0.1150)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/gpt-4.1-nano:\n",
      "      • Precision: 0.5342\n",
      "      • Recall:    0.5190\n",
      "      • F1 Score:  0.5259 (±0.0788)\n",
      "   openai/gpt-5-nano:\n",
      "      • Precision: 0.4918\n",
      "      • Recall:    0.4766\n",
      "      • F1 Score:  0.4835 (±0.0772)\n",
      "   qwen/qwen-2-72b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   qwen/qwen3-235b-a22b:\n",
      "      • Precision: 0.4819\n",
      "      • Recall:    0.4462\n",
      "      • F1 Score:  0.4629 (±0.1262)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.5031\n",
      "      • Recall:    0.4775\n",
      "      • F1 Score:  0.4895 (±0.1150)\n",
      "   meta-llama/llama-3.1-70b-instruct:\n",
      "      • Precision: 0.4902\n",
      "      • Recall:    0.4369\n",
      "      • F1 Score:  0.4609 (±0.0997)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.5602\n",
      "      • Recall:    0.5309\n",
      "      • F1 Score:  0.5442 (±0.1011)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: minimax/minimax-m2:free\n",
      "  ⚠ No hay datos para minimax/minimax-m2:free - asignando métricas en 0\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1\n",
      "  ⚠ No hay datos para nvidia/llama-3.3-nemotron-super-49b-v1 - asignando métricas en 0\n",
      "Evaluando modelo: openai/gpt-4.1-nano\n",
      "Evaluando modelo: openai/gpt-5-nano\n",
      "Evaluando modelo: qwen/qwen-2-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2-72b-instruct - asignando métricas en 0\n",
      "Evaluando modelo: qwen/qwen3-235b-a22b\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-3.1-70b-instruct\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7400\n",
      "      • Recall:    0.7362\n",
      "      • F1 Score:  0.7377 (±0.0271)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.7387\n",
      "      • Recall:    0.7372\n",
      "      • F1 Score:  0.7375 (±0.0277)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.7620\n",
      "      • Recall:    0.7424\n",
      "      • F1 Score:  0.7517 (±0.0400)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.7665\n",
      "      • Recall:    0.7439\n",
      "      • F1 Score:  0.7547 (±0.0258)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Precision: 0.8038\n",
      "      • Recall:    0.7671\n",
      "      • F1 Score:  0.7847 (±0.0435)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Precision: 0.7760\n",
      "      • Recall:    0.7513\n",
      "      • F1 Score:  0.7630 (±0.0336)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.7623\n",
      "      • Recall:    0.7390\n",
      "      • F1 Score:  0.7500 (±0.0317)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.7776\n",
      "      • Recall:    0.7490\n",
      "      • F1 Score:  0.7627 (±0.0309)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Precision: 0.8020\n",
      "      • Recall:    0.7710\n",
      "      • F1 Score:  0.7859 (±0.0410)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.7262\n",
      "      • Recall:    0.7108\n",
      "      • F1 Score:  0.7181 (±0.0340)\n",
      "   minimax/minimax-m2:free:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1.5:\n",
      "      • Precision: 0.7690\n",
      "      • Recall:    0.7458\n",
      "      • F1 Score:  0.7568 (±0.0407)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   openai/gpt-4.1-nano:\n",
      "      • Precision: 0.7716\n",
      "      • Recall:    0.7640\n",
      "      • F1 Score:  0.7676 (±0.0271)\n",
      "   openai/gpt-5-nano:\n",
      "      • Precision: 0.7574\n",
      "      • Recall:    0.7446\n",
      "      • F1 Score:  0.7506 (±0.0275)\n",
      "   qwen/qwen-2-72b-instruct:\n",
      "      • Precision: 0.0000\n",
      "      • Recall:    0.0000\n",
      "      • F1 Score:  0.0000 (±0.0000)\n",
      "   qwen/qwen3-235b-a22b:\n",
      "      • Precision: 0.7485\n",
      "      • Recall:    0.7351\n",
      "      • F1 Score:  0.7415 (±0.0271)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.7796\n",
      "      • Recall:    0.7625\n",
      "      • F1 Score:  0.7706 (±0.0255)\n",
      "   meta-llama/llama-3.1-70b-instruct:\n",
      "      • Precision: 0.7679\n",
      "      • Recall:    0.7335\n",
      "      • F1 Score:  0.7499 (±0.0385)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.7809\n",
      "      • Recall:    0.7643\n",
      "      • F1 Score:  0.7722 (±0.0390)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: minimax/minimax-m2:free\n",
      "  ⚠ No hay datos para minimax/minimax-m2:free - asignando métrica en 0\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1\n",
      "  ⚠ No hay datos para nvidia/llama-3.3-nemotron-super-49b-v1 - asignando métrica en 0\n",
      "Evaluando modelo: openai/gpt-4.1-nano\n",
      "Evaluando modelo: openai/gpt-5-nano\n",
      "Evaluando modelo: qwen/qwen-2-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2-72b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: qwen/qwen3-235b-a22b\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-3.1-70b-instruct\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.5148 (±0.1621)\n",
      "   microsoft/phi-4:\n",
      "      • Similitud: 0.5213 (±0.1769)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Similitud: 0.6069 (±0.1690)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.4599 (±0.1629)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • Similitud: 0.5899 (±0.1675)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • Similitud: 0.5123 (±0.1879)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Similitud: 0.5178 (±0.2096)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Similitud: 0.4923 (±0.1716)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • Similitud: 0.6436 (±0.1348)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Similitud: 0.5023 (±0.1950)\n",
      "   minimax/minimax-m2:free:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1.5:\n",
      "      • Similitud: 0.5651 (±0.2046)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   openai/gpt-4.1-nano:\n",
      "      • Similitud: 0.5406 (±0.2014)\n",
      "   openai/gpt-5-nano:\n",
      "      • Similitud: 0.5372 (±0.1763)\n",
      "   qwen/qwen-2-72b-instruct:\n",
      "      • Similitud: 0.0000 (±0.0000)\n",
      "   qwen/qwen3-235b-a22b:\n",
      "      • Similitud: 0.6277 (±0.1800)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Similitud: 0.5670 (±0.1846)\n",
      "   meta-llama/llama-3.1-70b-instruct:\n",
      "      • Similitud: 0.5296 (±0.1640)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Similitud: 0.6215 (±0.1924)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: google/gemini-2.0-flash-001\n",
      "Evaluando modelo: qwen/qwen-2-vl-72b-instruct\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: google/gemini-2.5-flash-lite\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: minimax/minimax-m2:free\n",
      "  ⚠ No hay datos para minimax/minimax-m2:free - asignando métrica en 0\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1.5\n",
      "Evaluando modelo: nvidia/llama-3.3-nemotron-super-49b-v1\n",
      "  ⚠ No hay datos para nvidia/llama-3.3-nemotron-super-49b-v1 - asignando métrica en 0\n",
      "Evaluando modelo: openai/gpt-4.1-nano\n",
      "Evaluando modelo: openai/gpt-5-nano\n",
      "Evaluando modelo: qwen/qwen-2-72b-instruct\n",
      "  ⚠ No hay datos para qwen/qwen-2-72b-instruct - asignando métrica en 0\n",
      "Evaluando modelo: qwen/qwen3-235b-a22b\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-3.1-70b-instruct\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2314 (±0.0603)\n",
      "   microsoft/phi-4:\n",
      "      • chrF: 0.2330 (±0.0546)\n",
      "   amazon/nova-lite-v1:\n",
      "      • chrF: 0.2432 (±0.0571)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.1908 (±0.0321)\n",
      "   google/gemini-2.0-flash-001:\n",
      "      • chrF: 0.2500 (±0.1036)\n",
      "   qwen/qwen-2-vl-72b-instruct:\n",
      "      • chrF: 0.2620 (±0.1090)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • chrF: 0.2273 (±0.0834)\n",
      "   google/gemma-2-27b-it:\n",
      "      • chrF: 0.2147 (±0.0611)\n",
      "   google/gemini-2.5-flash-lite:\n",
      "      • chrF: 0.2706 (±0.0714)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • chrF: 0.1719 (±0.0466)\n",
      "   minimax/minimax-m2:free:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1.5:\n",
      "      • chrF: 0.2277 (±0.0755)\n",
      "   nvidia/llama-3.3-nemotron-super-49b-v1:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   openai/gpt-4.1-nano:\n",
      "      • chrF: 0.2625 (±0.0863)\n",
      "   openai/gpt-5-nano:\n",
      "      • chrF: 0.2396 (±0.0824)\n",
      "   qwen/qwen-2-72b-instruct:\n",
      "      • chrF: 0.0000 (±0.0000)\n",
      "   qwen/qwen3-235b-a22b:\n",
      "      • chrF: 0.2522 (±0.0604)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • chrF: 0.2399 (±0.0679)\n",
      "   meta-llama/llama-3.1-70b-instruct:\n",
      "      • chrF: 0.2191 (±0.0946)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • chrF: 0.2962 (±0.0994)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 3 COMPLETADO\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 3: Modismo + Ejemplo → Literal + Definición\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo genera interpretaciones literales correctas\")\n",
    "print(\"Métricas:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\")\n",
    "print(\"  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\")\n",
    "print(\"  - chrF: Character n-gram F-score\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_3_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p3 = json.load(f)\n",
    "\n",
    "# Filtrar datos con literal y definición generados\n",
    "data_p3_valid = [d for d in data_p3 if d.get('significado_real') and d.get('definicion_generada')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p3_valid)} registros válidos de {len(data_p3)} totales\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BERTScore con BETO y SciBETO\n",
    "# ============================================================================\n",
    "bert_models = [\n",
    "    ('BETO', compute_bertscore_beto),\n",
    "    ('SciBETO', compute_bertscore_sci_beto)\n",
    "]\n",
    "\n",
    "for bert_name, bert_func in bert_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Calculando BERTScore con {bert_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    resultados_p3 = []\n",
    "    \n",
    "    for model in MODEL_NAMES:\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        \n",
    "        # Filtrar datos de este modelo\n",
    "        model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "        \n",
    "        if not model_data:\n",
    "            print(f\"  ⚠ No hay datos para {model} - asignando métricas en 0\")\n",
    "            resultados_p3.append({\n",
    "                'modismo': 'N/A',\n",
    "                'ejemplo': '',\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'significado_real': '',\n",
    "                'literal_generado': '',\n",
    "                'definicion_generada': ''\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        referencias = [d['significado_real'] for d in model_data]\n",
    "        candidatos = [d['definicion_generada'] for d in model_data]\n",
    "        \n",
    "        # BERTScore: compara definición generada vs significado real\n",
    "        P, R, F1 = bert_func(candidatos, referencias)\n",
    "        \n",
    "        for idx, (p, r, f1) in enumerate(zip(P.tolist(), R.tolist(), F1.tolist())):\n",
    "            resultados_p3.append({\n",
    "                'modismo': model_data[idx]['modismo'],\n",
    "                'ejemplo': model_data[idx]['ejemplo'],\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': p,\n",
    "                'recall': r,\n",
    "                'f1_score': f1,\n",
    "                'significado_real': referencias[idx],\n",
    "                'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "                'definicion_generada': candidatos[idx]\n",
    "            })\n",
    "    \n",
    "    # Guardar resultados\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'prompt_3_{bert_name.lower()}_bertscore_resultados.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados_p3, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"RESULTADOS con {bert_name}:\")\n",
    "    for model in MODEL_NAMES:\n",
    "        model_results = [r for r in resultados_p3 if r['modelo'] == model]\n",
    "        if model_results:\n",
    "            f1_scores = [r['f1_score'] for r in model_results]\n",
    "            precisions = [r['precision'] for r in model_results]\n",
    "            recalls = [r['recall'] for r in model_results]\n",
    "            \n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            p_mean = np.mean(precisions)\n",
    "            r_mean = np.mean(recalls)\n",
    "            \n",
    "            print(f\"   {model}:\")\n",
    "            print(f\"      • Precision: {p_mean:.4f}\")\n",
    "            print(f\"      • Recall:    {r_mean:.4f}\")\n",
    "            print(f\"      • F1 Score:  {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Sentence-BERT Similarity\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando Sentence-BERT Similarity\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_sbert = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_sbert.append({\n",
    "            'modismo': 'N/A',\n",
    "            'ejemplo': '',\n",
    "            'modelo': model,\n",
    "            'similarity': 0.0,\n",
    "            'significado_real': '',\n",
    "            'literal_generado': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['significado_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular similitud con Sentence-BERT\n",
    "    similarities = compute_sbert_similarity(candidatos, referencias)\n",
    "    \n",
    "    for idx, sim in enumerate(similarities):\n",
    "        resultados_sbert.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'ejemplo': model_data[idx]['ejemplo'],\n",
    "            'modelo': model,\n",
    "            'similarity': float(sim),\n",
    "            'significado_real': referencias[idx],\n",
    "            'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_3_sbert_similarity_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_sbert, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con Sentence-BERT:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_sbert if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        similarities = [r['similarity'] for r in model_results]\n",
    "        sim_mean = np.mean(similarities)\n",
    "        sim_std = np.std(similarities)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Similitud: {sim_mean:.4f} (±{sim_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. chrF Score\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando chrF Score\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_chrf = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_chrf.append({\n",
    "            'modismo': 'N/A',\n",
    "            'ejemplo': '',\n",
    "            'modelo': model,\n",
    "            'chrf_score': 0.0,\n",
    "            'significado_real': '',\n",
    "            'literal_generado': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['significado_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular chrF\n",
    "    chrf_scores = compute_chrf_batch(candidatos, referencias)\n",
    "    \n",
    "    for idx, score in enumerate(chrf_scores):\n",
    "        resultados_chrf.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'ejemplo': model_data[idx]['ejemplo'],\n",
    "            'modelo': model,\n",
    "            'chrf_score': float(score),\n",
    "            'significado_real': referencias[idx],\n",
    "            'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_3_chrf_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_chrf, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con chrF:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_chrf if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        chrf_scores = [r['chrf_score'] for r in model_results]\n",
    "        chrf_mean = np.mean(chrf_scores)\n",
    "        chrf_std = np.std(chrf_scores)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • chrF: {chrf_mean:.4f} (±{chrf_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT 3 COMPLETADO\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
