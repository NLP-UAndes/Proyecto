{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30cd0f2",
   "metadata": {},
   "source": [
    "# Evaluación de Modelos LLM en Modismos Colombianos\n",
    "\n",
    "Este notebook evalúa qué tan bien los modelos LLM entienden los modismos colombianos usando:\n",
    "- **BERTScore**: Para comparar similitud semántica entre textos (definiciones)\n",
    "- **Accuracy/Exact Match**: Para clasificación y coincidencias exactas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00e03151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos en: Metricas LLM/Data_for_Metrics\n",
      "Resultados en: Metricas LLM/Metricas_Resultados\n",
      "Modelos: ['amazon/nova-micro-v1', 'microsoft/phi-4', 'amazon/nova-lite-v1', 'cohere/command-r-08-2024', 'qwen/qwen-2.5-72b-instruct', 'google/gemma-2-27b-it', 'meta-llama/llama-3.3-70b-instruct', 'microsoft/wizardlm-2-8x22b', 'meta-llama/llama-4-maverick', 'qwen/qwen2.5-vl-32b-instruct:free', 'x-ai/grok-3-mini-beta', 'perplexity/sonar', 'mistralai/mistral-medium-3', 'mistralai/mixtral-8x7b-instruct', 'google/gemini-2.5-flash', 'meta-llama/llama-3.1-405b-instruct', 'deepseek/deepseek-chat-v3.1', 'moonshotai/kimi-k2-0905', 'openai/o4-mini-high', 'openai/gpt-4.1', 'openai/o1-mini', 'anthropic/claude-sonnet-4']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Estilos para gráficos\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Modelos\n",
    "MODELS_FILE = 'Straico/text_model_usefull.txt'\n",
    "\n",
    "def load_models_from_file(filepath):\n",
    "    \"\"\"Carga los nombres de modelos desde un archivo de texto.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontró el archivo {filepath}\")\n",
    "        return []\n",
    "\n",
    "MODEL_NAMES = load_models_from_file(MODELS_FILE)\n",
    "\n",
    "# Directorios\n",
    "DATA_DIR = 'Metricas LLM/Data_for_Metrics'\n",
    "OUTPUT_DIR = 'Metricas LLM/Metricas_Resultados'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Datos en: {DATA_DIR}\")\n",
    "print(f\"Resultados en: {OUTPUT_DIR}\")\n",
    "print(f\"Modelos: {MODEL_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28bca76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Métricas cargadas correctamente:\n",
      "  - BERTScore (BETO y SciBETO)\n",
      "  - Sentence-BERT (paraphrase-multilingual-mpnet-base-v2)\n",
      "  - chrF (Character n-gram F-score)\n"
     ]
    }
   ],
   "source": [
    "# Importar funciones de métricas\n",
    "import sys\n",
    "sys.path.append('Metricas LLM')\n",
    "\n",
    "from BertScore import compute_bertscore_beto, compute_bertscore_sci_beto\n",
    "from SentenceBert import compute_sbert_similarity\n",
    "from chrF import compute_chrf_batch\n",
    "\n",
    "print(\"✓ Métricas cargadas correctamente:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO)\")\n",
    "print(\"  - Sentence-BERT (paraphrase-multilingual-mpnet-base-v2)\")\n",
    "print(\"  - chrF (Character n-gram F-score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14c7be",
   "metadata": {},
   "source": [
    "## 1. PROMPT 1: Modismo → Definición\n",
    "\n",
    "**Métrica**: BERTScore (F1)  \n",
    "**Justificación**: Evalúa similitud semántica entre la definición generada y la real. Ideal para entender si el modelo captura el significado del modismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2dd6f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 1: Modismo → Definición\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo puede generar una definición correcta del modismo\n",
      "Métricas:\n",
      "  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\n",
      "  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\n",
      "  - chrF: Character n-gram F-score\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 93568 registros válidos de 93568 totales\n",
      "\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con BETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "Evaluando modelo: perplexity/sonar\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "Evaluando modelo: openai/o1-mini\n",
      "Evaluando modelo: anthropic/claude-sonnet-4\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.4709\n",
      "      • Recall:    0.4679\n",
      "      • F1 Score:  0.4684 (±0.0940)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.4430\n",
      "      • Recall:    0.4533\n",
      "      • F1 Score:  0.4471 (±0.0805)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.4737\n",
      "      • Recall:    0.4543\n",
      "      • F1 Score:  0.4627 (±0.0977)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.4745\n",
      "      • Recall:    0.4719\n",
      "      • F1 Score:  0.4722 (±0.0958)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.4783\n",
      "      • Recall:    0.4794\n",
      "      • F1 Score:  0.4778 (±0.1020)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.4617\n",
      "      • Recall:    0.4489\n",
      "      • F1 Score:  0.4543 (±0.0941)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.4553\n",
      "      • Recall:    0.4312\n",
      "      • F1 Score:  0.4419 (±0.0997)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.4696\n",
      "      • Recall:    0.4802\n",
      "      • F1 Score:  0.4738 (±0.0951)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.4800\n",
      "      • Recall:    0.4897\n",
      "      • F1 Score:  0.4838 (±0.1055)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Precision: 0.4675\n",
      "      • Recall:    0.4706\n",
      "      • F1 Score:  0.4681 (±0.0873)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Precision: 0.4803\n",
      "      • Recall:    0.4706\n",
      "      • F1 Score:  0.4744 (±0.1030)\n",
      "   perplexity/sonar:\n",
      "      • Precision: 0.5465\n",
      "      • Recall:    0.5607\n",
      "      • F1 Score:  0.5523 (±0.1248)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Precision: 0.4763\n",
      "      • Recall:    0.4809\n",
      "      • F1 Score:  0.4776 (±0.1009)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Precision: 0.4721\n",
      "      • Recall:    0.4604\n",
      "      • F1 Score:  0.4652 (±0.0983)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Precision: 0.5072\n",
      "      • Recall:    0.5112\n",
      "      • F1 Score:  0.5081 (±0.1084)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Precision: 0.4714\n",
      "      • Recall:    0.4740\n",
      "      • F1 Score:  0.4716 (±0.0999)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Precision: 0.4832\n",
      "      • Recall:    0.4936\n",
      "      • F1 Score:  0.4873 (±0.1036)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Precision: 0.4808\n",
      "      • Recall:    0.4750\n",
      "      • F1 Score:  0.4768 (±0.1021)\n",
      "   openai/o4-mini-high:\n",
      "      • Precision: 0.4639\n",
      "      • Recall:    0.4735\n",
      "      • F1 Score:  0.4676 (±0.0987)\n",
      "   openai/gpt-4.1:\n",
      "      • Precision: 0.4957\n",
      "      • Recall:    0.5091\n",
      "      • F1 Score:  0.5012 (±0.1072)\n",
      "   openai/o1-mini:\n",
      "      • Precision: 0.4627\n",
      "      • Recall:    0.4723\n",
      "      • F1 Score:  0.4664 (±0.1000)\n",
      "   anthropic/claude-sonnet-4:\n",
      "      • Precision: 0.5048\n",
      "      • Recall:    0.5091\n",
      "      • F1 Score:  0.5058 (±0.1139)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "Evaluando modelo: perplexity/sonar\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "Evaluando modelo: openai/o1-mini\n",
      "Evaluando modelo: anthropic/claude-sonnet-4\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7614\n",
      "      • Recall:    0.7535\n",
      "      • F1 Score:  0.7571 (±0.0352)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.7421\n",
      "      • Recall:    0.7486\n",
      "      • F1 Score:  0.7449 (±0.0283)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.7698\n",
      "      • Recall:    0.7490\n",
      "      • F1 Score:  0.7589 (±0.0384)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.7599\n",
      "      • Recall:    0.7548\n",
      "      • F1 Score:  0.7569 (±0.0342)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.7607\n",
      "      • Recall:    0.7583\n",
      "      • F1 Score:  0.7590 (±0.0359)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.7607\n",
      "      • Recall:    0.7474\n",
      "      • F1 Score:  0.7536 (±0.0356)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.7454\n",
      "      • Recall:    0.7330\n",
      "      • F1 Score:  0.7387 (±0.0437)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.7521\n",
      "      • Recall:    0.7568\n",
      "      • F1 Score:  0.7540 (±0.0342)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.7485\n",
      "      • Recall:    0.7573\n",
      "      • F1 Score:  0.7524 (±0.0391)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Precision: 0.7526\n",
      "      • Recall:    0.7532\n",
      "      • F1 Score:  0.7525 (±0.0289)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Precision: 0.7703\n",
      "      • Recall:    0.7568\n",
      "      • F1 Score:  0.7630 (±0.0369)\n",
      "   perplexity/sonar:\n",
      "      • Precision: 0.7758\n",
      "      • Recall:    0.7859\n",
      "      • F1 Score:  0.7803 (±0.0478)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Precision: 0.7578\n",
      "      • Recall:    0.7584\n",
      "      • F1 Score:  0.7577 (±0.0357)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Precision: 0.7564\n",
      "      • Recall:    0.7473\n",
      "      • F1 Score:  0.7514 (±0.0396)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Precision: 0.7657\n",
      "      • Recall:    0.7666\n",
      "      • F1 Score:  0.7657 (±0.0395)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Precision: 0.7538\n",
      "      • Recall:    0.7539\n",
      "      • F1 Score:  0.7534 (±0.0360)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Precision: 0.7535\n",
      "      • Recall:    0.7609\n",
      "      • F1 Score:  0.7568 (±0.0365)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Precision: 0.7557\n",
      "      • Recall:    0.7535\n",
      "      • F1 Score:  0.7541 (±0.0377)\n",
      "   openai/o4-mini-high:\n",
      "      • Precision: 0.7513\n",
      "      • Recall:    0.7573\n",
      "      • F1 Score:  0.7539 (±0.0353)\n",
      "   openai/gpt-4.1:\n",
      "      • Precision: 0.7592\n",
      "      • Recall:    0.7675\n",
      "      • F1 Score:  0.7629 (±0.0381)\n",
      "   openai/o1-mini:\n",
      "      • Precision: 0.7518\n",
      "      • Recall:    0.7576\n",
      "      • F1 Score:  0.7543 (±0.0352)\n",
      "   anthropic/claude-sonnet-4:\n",
      "      • Precision: 0.7629\n",
      "      • Recall:    0.7671\n",
      "      • F1 Score:  0.7645 (±0.0410)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "Evaluando modelo: perplexity/sonar\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "Evaluando modelo: openai/o1-mini\n",
      "Evaluando modelo: anthropic/claude-sonnet-4\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.3675 (±0.2116)\n",
      "   microsoft/phi-4:\n",
      "      • Similitud: 0.2980 (±0.1889)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Similitud: 0.3656 (±0.2129)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.3892 (±0.2234)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Similitud: 0.3917 (±0.2314)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Similitud: 0.3349 (±0.2003)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Similitud: 0.3724 (±0.2140)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Similitud: 0.3618 (±0.2179)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Similitud: 0.3998 (±0.2481)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Similitud: 0.3583 (±0.2181)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Similitud: 0.4058 (±0.2345)\n",
      "   perplexity/sonar:\n",
      "      • Similitud: 0.5528 (±0.2586)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Similitud: 0.3797 (±0.2342)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Similitud: 0.3895 (±0.2186)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Similitud: 0.4623 (±0.2482)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Similitud: 0.3675 (±0.2239)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Similitud: 0.3937 (±0.2437)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Similitud: 0.3940 (±0.2334)\n",
      "   openai/o4-mini-high:\n",
      "      • Similitud: 0.3761 (±0.2413)\n",
      "   openai/gpt-4.1:\n",
      "      • Similitud: 0.4408 (±0.2565)\n",
      "   openai/o1-mini:\n",
      "      • Similitud: 0.3733 (±0.2427)\n",
      "   anthropic/claude-sonnet-4:\n",
      "      • Similitud: 0.4437 (±0.2574)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "Evaluando modelo: perplexity/sonar\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "Evaluando modelo: openai/o1-mini\n",
      "Evaluando modelo: anthropic/claude-sonnet-4\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2272 (±0.0776)\n",
      "   microsoft/phi-4:\n",
      "      • chrF: 0.2278 (±0.0617)\n",
      "   amazon/nova-lite-v1:\n",
      "      • chrF: 0.2042 (±0.0813)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.2260 (±0.0790)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • chrF: 0.2395 (±0.0893)\n",
      "   google/gemma-2-27b-it:\n",
      "      • chrF: 0.2014 (±0.0753)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • chrF: 0.1811 (±0.0831)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • chrF: 0.2454 (±0.0836)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • chrF: 0.2574 (±0.0988)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • chrF: 0.2288 (±0.0655)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • chrF: 0.2266 (±0.0916)\n",
      "   perplexity/sonar:\n",
      "      • chrF: 0.3235 (±0.1388)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • chrF: 0.2449 (±0.0892)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • chrF: 0.2118 (±0.0873)\n",
      "   google/gemini-2.5-flash:\n",
      "      • chrF: 0.2621 (±0.1052)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • chrF: 0.2344 (±0.0865)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • chrF: 0.2566 (±0.0946)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • chrF: 0.2273 (±0.0929)\n",
      "   openai/o4-mini-high:\n",
      "      • chrF: 0.2432 (±0.0851)\n",
      "   openai/gpt-4.1:\n",
      "      • chrF: 0.2753 (±0.1005)\n",
      "   openai/o1-mini:\n",
      "      • chrF: 0.2437 (±0.0857)\n",
      "   anthropic/claude-sonnet-4:\n",
      "      • chrF: 0.2705 (±0.1106)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 1 COMPLETADO\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 1: Modismo → Definición\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo puede generar una definición correcta del modismo\")\n",
    "print(\"Métricas:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\")\n",
    "print(\"  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\")\n",
    "print(\"  - chrF: Character n-gram F-score\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_1_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p1 = json.load(f)\n",
    "\n",
    "# Filtrar datos vacíos\n",
    "data_p1_valid = [d for d in data_p1 if d.get('definicion_real') and d.get('definicion_generada')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p1_valid)} registros válidos de {len(data_p1)} totales\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BERTScore con BETO y SciBETO\n",
    "# ============================================================================\n",
    "bert_models = [\n",
    "    ('BETO', compute_bertscore_beto),\n",
    "    ('SciBETO', compute_bertscore_sci_beto)\n",
    "]\n",
    "\n",
    "for bert_name, bert_func in bert_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Calculando BERTScore con {bert_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    resultados_p1 = []\n",
    "    \n",
    "    for model in MODEL_NAMES:\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        \n",
    "        # Filtrar datos de este modelo\n",
    "        model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "        \n",
    "        if not model_data:\n",
    "            print(f\"  ⚠ No hay datos para {model} - asignando métricas en 0\")\n",
    "            # Crear entrada con métricas en 0\n",
    "            resultados_p1.append({\n",
    "                'modismo': 'N/A',\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'definicion_real': '',\n",
    "                'definicion_generada': ''\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        referencias = [d['definicion_real'] for d in model_data]\n",
    "        candidatos = [d['definicion_generada'] for d in model_data]\n",
    "        \n",
    "        # BERTScore: compara candidatos (generados) vs referencias (reales)\n",
    "        P, R, F1 = bert_func(candidatos, referencias)\n",
    "        \n",
    "        for idx, (p, r, f1) in enumerate(zip(P.tolist(), R.tolist(), F1.tolist())):\n",
    "            resultados_p1.append({\n",
    "                'modismo': model_data[idx]['modismo'],\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': p,\n",
    "                'recall': r,\n",
    "                'f1_score': f1,\n",
    "                'definicion_real': referencias[idx],\n",
    "                'definicion_generada': candidatos[idx]\n",
    "            })\n",
    "    \n",
    "    # Guardar resultados\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'prompt_1_{bert_name.lower()}_bertscore_resultados.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados_p1, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"RESULTADOS con {bert_name}:\")\n",
    "    for model in MODEL_NAMES:\n",
    "        model_results = [r for r in resultados_p1 if r['modelo'] == model]\n",
    "        if model_results:\n",
    "            f1_scores = [r['f1_score'] for r in model_results]\n",
    "            precisions = [r['precision'] for r in model_results]\n",
    "            recalls = [r['recall'] for r in model_results]\n",
    "            \n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            p_mean = np.mean(precisions)\n",
    "            r_mean = np.mean(recalls)\n",
    "            \n",
    "            print(f\"   {model}:\")\n",
    "            print(f\"      • Precision: {p_mean:.4f}\")\n",
    "            print(f\"      • Recall:    {r_mean:.4f}\")\n",
    "            print(f\"      • F1 Score:  {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Sentence-BERT Similarity\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando Sentence-BERT Similarity\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_sbert = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_sbert.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'similarity': 0.0,\n",
    "            'definicion_real': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['definicion_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular similitud con Sentence-BERT\n",
    "    similarities = compute_sbert_similarity(candidatos, referencias)\n",
    "    \n",
    "    for idx, sim in enumerate(similarities):\n",
    "        resultados_sbert.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'modelo': model,\n",
    "            'similarity': float(sim),\n",
    "            'definicion_real': referencias[idx],\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_1_sbert_similarity_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_sbert, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con Sentence-BERT:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_sbert if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        similarities = [r['similarity'] for r in model_results]\n",
    "        sim_mean = np.mean(similarities)\n",
    "        sim_std = np.std(similarities)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Similitud: {sim_mean:.4f} (±{sim_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. chrF Score\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando chrF Score\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_chrf = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_chrf.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'chrf_score': 0.0,\n",
    "            'definicion_real': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['definicion_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular chrF\n",
    "    chrf_scores = compute_chrf_batch(candidatos, referencias)\n",
    "    \n",
    "    for idx, score in enumerate(chrf_scores):\n",
    "        resultados_chrf.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'modelo': model,\n",
    "            'chrf_score': float(score),\n",
    "            'definicion_real': referencias[idx],\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_1_chrf_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_chrf, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con chrF:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_chrf if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        chrf_scores = [r['chrf_score'] for r in model_results]\n",
    "        chrf_mean = np.mean(chrf_scores)\n",
    "        chrf_std = np.std(chrf_scores)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • chrF: {chrf_mean:.4f} (±{chrf_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT 1 COMPLETADO\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545337c",
   "metadata": {},
   "source": [
    "## 2. PROMPT 2: Modismo → Es Modismo (Sí/No)\n",
    "\n",
    "**Métrica**: Accuracy  \n",
    "**Justificación**: Es una tarea de clasificación binaria. Mide si el modelo identifica correctamente que una expresión es un modismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a5292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 2: Modismo → ¿Es Modismo? (Sí/No)\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo identifica correctamente que una expresión es un modismo\n",
      "Métrica: Accuracy (Exactitud)\n",
      "Justificación: Problema de clasificación binaria (Sí/No).\n",
      "   Mide el porcentaje de respuestas correctas.\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 93633 registros válidos de 93633 totales\n",
      "\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "  • Accuracy: 0.2106 (968/4597 correctos)\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "  • Accuracy: 0.6904 (2883/4176 correctos)\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "  • Accuracy: 0.3001 (1378/4592 correctos)\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "  • Accuracy: 0.6923 (3181/4595 correctos)\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "  • Accuracy: 0.5729 (2380/4154 correctos)\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "  • Accuracy: 0.9403 (4303/4576 correctos)\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "  • Accuracy: 0.7801 (3590/4602 correctos)\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "  • Accuracy: 0.4790 (2198/4589 correctos)\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "  • Accuracy: 0.6742 (3092/4586 correctos)\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "  • Accuracy: 0.4772 (397/832 correctos)\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "  • Accuracy: 0.1942 (887/4567 correctos)\n",
      "Evaluando modelo: perplexity/sonar\n",
      "  • Accuracy: 0.7004 (3184/4546 correctos)\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "  • Accuracy: 0.6460 (2920/4520 correctos)\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "  • Accuracy: 0.4636 (1197/2582 correctos)\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "  • Accuracy: 0.7028 (3212/4570 correctos)\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "  • Accuracy: 0.7908 (3270/4135 correctos)\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "  • Accuracy: 0.5712 (2628/4601 correctos)\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "  • Accuracy: 0.5813 (2570/4421 correctos)\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "  • Accuracy: 0.2304 (1059/4596 correctos)\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "  • Accuracy: 0.7897 (3634/4602 correctos)\n",
      "Evaluando modelo: openai/o1-mini\n",
      "  • Accuracy: 0.2308 (1061/4597 correctos)\n",
      "Evaluando modelo: anthropic/claude-sonnet-4\n",
      "  • Accuracy: 0.4564 (2098/4597 correctos)\n",
      "\n",
      "RESULTADOS:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Accuracy:  0.2106 (968/4597 correctos)\n",
      "   microsoft/phi-4:\n",
      "      • Accuracy:  0.6904 (2883/4176 correctos)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Accuracy:  0.3001 (1378/4592 correctos)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Accuracy:  0.6923 (3181/4595 correctos)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Accuracy:  0.5729 (2380/4154 correctos)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Accuracy:  0.9403 (4303/4576 correctos)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Accuracy:  0.7801 (3590/4602 correctos)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Accuracy:  0.4790 (2198/4589 correctos)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Accuracy:  0.6742 (3092/4586 correctos)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Accuracy:  0.4772 (397/832 correctos)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Accuracy:  0.1942 (887/4567 correctos)\n",
      "   perplexity/sonar:\n",
      "      • Accuracy:  0.7004 (3184/4546 correctos)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Accuracy:  0.6460 (2920/4520 correctos)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Accuracy:  0.4636 (1197/2582 correctos)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Accuracy:  0.7028 (3212/4570 correctos)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Accuracy:  0.7908 (3270/4135 correctos)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Accuracy:  0.5712 (2628/4601 correctos)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Accuracy:  0.5813 (2570/4421 correctos)\n",
      "   openai/o4-mini-high:\n",
      "      • Accuracy:  0.2304 (1059/4596 correctos)\n",
      "   openai/gpt-4.1:\n",
      "      • Accuracy:  0.7897 (3634/4602 correctos)\n",
      "   openai/o1-mini:\n",
      "      • Accuracy:  0.2308 (1061/4597 correctos)\n",
      "   anthropic/claude-sonnet-4:\n",
      "      • Accuracy:  0.4564 (2098/4597 correctos)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_2_accuracy_resultados.json\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 2: Modismo → ¿Es Modismo? (Sí/No)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo identifica correctamente que una expresión es un modismo\")\n",
    "print(\"Métrica: Accuracy (Exactitud)\")\n",
    "print(\"Justificación: Problema de clasificación binaria (Sí/No).\")\n",
    "print(\"   Mide el porcentaje de respuestas correctas.\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_2_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p2 = json.load(f)\n",
    "\n",
    "# Filtrar datos vacíos\n",
    "data_p2_valid = [d for d in data_p2 if d.get('es_modismo_real') and d.get('es_modismo_generado')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p2_valid)} registros válidos de {len(data_p2)} totales\")\n",
    "print()\n",
    "\n",
    "def normalizar_respuesta(respuesta):\n",
    "    \"\"\"Normaliza respuestas a 'Sí' o 'No'\"\"\"\n",
    "    if not respuesta:\n",
    "        return None\n",
    "    respuesta = str(respuesta).strip().lower()\n",
    "    if respuesta in ['sí', 'si', 'yes', 's', 'true', '1']:\n",
    "        return 'Sí'\n",
    "    elif respuesta in ['no', 'n', 'false', '0']:\n",
    "        return 'No'\n",
    "    return respuesta\n",
    "\n",
    "resultados_p2 = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    # Filtrar datos de este modelo\n",
    "    model_data = [d for d in data_p2_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando accuracy en 0\")\n",
    "        resultados_p2.append({\n",
    "            'modismo': 'N/A',\n",
    "            'modelo': model,\n",
    "            'respuesta_real': 'Sí',\n",
    "            'respuesta_generada': 'No',\n",
    "            'correcto': False\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Calcular accuracy\n",
    "    correctos = 0\n",
    "    for d in model_data:\n",
    "        real = normalizar_respuesta(d['es_modismo_real'])\n",
    "        generado = normalizar_respuesta(d['es_modismo_generado'])\n",
    "        correcto = (real == generado)\n",
    "        \n",
    "        if correcto:\n",
    "            correctos += 1\n",
    "        \n",
    "        resultados_p2.append({\n",
    "            'modismo': d['modismo'],\n",
    "            'modelo': model,\n",
    "            'respuesta_real': real,\n",
    "            'respuesta_generada': generado,\n",
    "            'correcto': correcto\n",
    "        })\n",
    "    \n",
    "    accuracy = correctos / len(model_data) if model_data else 0\n",
    "    print(f\"  • Accuracy: {accuracy:.4f} ({correctos}/{len(model_data)} correctos)\")\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_2_accuracy_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_p2, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_p2 if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        correctos = sum(1 for r in model_results if r['correcto'])\n",
    "        total = len(model_results)\n",
    "        accuracy = correctos / total if total > 0 else 0\n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Accuracy:  {accuracy:.4f} ({correctos}/{total} correctos)\")\n",
    "\n",
    "print()\n",
    "print(f\"✓ Guardado en: {output_file}\")\n",
    "print(\"=\"*80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03bb355",
   "metadata": {},
   "source": [
    "## 3. PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
    "\n",
    "**Métrica**: BERTScore (F1)  \n",
    "**Justificación**: Evalúa si el modelo puede generar interpretaciones literales del modismo en contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d89e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo genera interpretaciones literales correctas\n",
      "Métricas:\n",
      "  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\n",
      "  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\n",
      "  - chrF: Character n-gram F-score\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 93861 registros válidos de 94534 totales\n",
      "\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con BETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "Evaluando modelo: perplexity/sonar\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "Evaluando modelo: openai/o1-mini\n",
      "Evaluando modelo: anthropic/claude-sonnet-4\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.4854\n",
      "      • Recall:    0.4682\n",
      "      • F1 Score:  0.4756 (±0.0973)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.4741\n",
      "      • Recall:    0.4742\n",
      "      • F1 Score:  0.4732 (±0.0925)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.4982\n",
      "      • Recall:    0.4801\n",
      "      • F1 Score:  0.4879 (±0.0973)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.5057\n",
      "      • Recall:    0.4911\n",
      "      • F1 Score:  0.4971 (±0.0958)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.5175\n",
      "      • Recall:    0.4910\n",
      "      • F1 Score:  0.5028 (±0.1005)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.5138\n",
      "      • Recall:    0.4922\n",
      "      • F1 Score:  0.5017 (±0.0970)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.4705\n",
      "      • Recall:    0.4292\n",
      "      • F1 Score:  0.4478 (±0.0965)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.5189\n",
      "      • Recall:    0.5139\n",
      "      • F1 Score:  0.5152 (±0.1003)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.5305\n",
      "      • Recall:    0.5157\n",
      "      • F1 Score:  0.5218 (±0.1023)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Precision: 0.5197\n",
      "      • Recall:    0.5035\n",
      "      • F1 Score:  0.5106 (±0.0923)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Precision: 0.5029\n",
      "      • Recall:    0.4824\n",
      "      • F1 Score:  0.4914 (±0.0970)\n",
      "   perplexity/sonar:\n",
      "      • Precision: 0.5478\n",
      "      • Recall:    0.5399\n",
      "      • F1 Score:  0.5426 (±0.1056)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Precision: 0.5357\n",
      "      • Recall:    0.5175\n",
      "      • F1 Score:  0.5253 (±0.0986)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Precision: 0.4920\n",
      "      • Recall:    0.4838\n",
      "      • F1 Score:  0.4868 (±0.0973)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Precision: 0.5371\n",
      "      • Recall:    0.5277\n",
      "      • F1 Score:  0.5313 (±0.0966)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Precision: 0.5134\n",
      "      • Recall:    0.5005\n",
      "      • F1 Score:  0.5057 (±0.0979)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Precision: 0.5450\n",
      "      • Recall:    0.5458\n",
      "      • F1 Score:  0.5443 (±0.0989)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Precision: 0.5183\n",
      "      • Recall:    0.5132\n",
      "      • F1 Score:  0.5148 (±0.0988)\n",
      "   openai/o4-mini-high:\n",
      "      • Precision: 0.5110\n",
      "      • Recall:    0.4853\n",
      "      • F1 Score:  0.4967 (±0.1063)\n",
      "   openai/gpt-4.1:\n",
      "      • Precision: 0.5305\n",
      "      • Recall:    0.5161\n",
      "      • F1 Score:  0.5221 (±0.0997)\n",
      "   openai/o1-mini:\n",
      "      • Precision: 0.5093\n",
      "      • Recall:    0.4841\n",
      "      • F1 Score:  0.4953 (±0.1048)\n",
      "   anthropic/claude-sonnet-4:\n",
      "      • Precision: 0.5588\n",
      "      • Recall:    0.5602\n",
      "      • F1 Score:  0.5584 (±0.1019)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "Evaluando modelo: perplexity/sonar\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "Evaluando modelo: openai/o1-mini\n",
      "Evaluando modelo: anthropic/claude-sonnet-4\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7426\n",
      "      • Recall:    0.7366\n",
      "      • F1 Score:  0.7392 (±0.0358)\n",
      "   microsoft/phi-4:\n",
      "      • Precision: 0.7334\n",
      "      • Recall:    0.7381\n",
      "      • F1 Score:  0.7354 (±0.0323)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Precision: 0.7457\n",
      "      • Recall:    0.7378\n",
      "      • F1 Score:  0.7413 (±0.0371)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.7731\n",
      "      • Recall:    0.7560\n",
      "      • F1 Score:  0.7640 (±0.0366)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Precision: 0.7601\n",
      "      • Recall:    0.7438\n",
      "      • F1 Score:  0.7515 (±0.0412)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Precision: 0.7729\n",
      "      • Recall:    0.7525\n",
      "      • F1 Score:  0.7621 (±0.0402)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Precision: 0.7369\n",
      "      • Recall:    0.7192\n",
      "      • F1 Score:  0.7276 (±0.0391)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Precision: 0.7784\n",
      "      • Recall:    0.7678\n",
      "      • F1 Score:  0.7726 (±0.0374)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Precision: 0.7620\n",
      "      • Recall:    0.7538\n",
      "      • F1 Score:  0.7574 (±0.0440)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Precision: 0.7742\n",
      "      • Recall:    0.7567\n",
      "      • F1 Score:  0.7650 (±0.0352)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Precision: 0.7571\n",
      "      • Recall:    0.7439\n",
      "      • F1 Score:  0.7501 (±0.0379)\n",
      "   perplexity/sonar:\n",
      "      • Precision: 0.7626\n",
      "      • Recall:    0.7609\n",
      "      • F1 Score:  0.7613 (±0.0440)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Precision: 0.7835\n",
      "      • Recall:    0.7624\n",
      "      • F1 Score:  0.7724 (±0.0378)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Precision: 0.7466\n",
      "      • Recall:    0.7447\n",
      "      • F1 Score:  0.7453 (±0.0373)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Precision: 0.7817\n",
      "      • Recall:    0.7681\n",
      "      • F1 Score:  0.7744 (±0.0374)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Precision: 0.7534\n",
      "      • Recall:    0.7480\n",
      "      • F1 Score:  0.7503 (±0.0408)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Precision: 0.7771\n",
      "      • Recall:    0.7751\n",
      "      • F1 Score:  0.7756 (±0.0387)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Precision: 0.7428\n",
      "      • Recall:    0.7476\n",
      "      • F1 Score:  0.7449 (±0.0362)\n",
      "   openai/o4-mini-high:\n",
      "      • Precision: 0.7499\n",
      "      • Recall:    0.7396\n",
      "      • F1 Score:  0.7443 (±0.0396)\n",
      "   openai/gpt-4.1:\n",
      "      • Precision: 0.7614\n",
      "      • Recall:    0.7534\n",
      "      • F1 Score:  0.7570 (±0.0399)\n",
      "   openai/o1-mini:\n",
      "      • Precision: 0.7496\n",
      "      • Recall:    0.7397\n",
      "      • F1 Score:  0.7442 (±0.0391)\n",
      "   anthropic/claude-sonnet-4:\n",
      "      • Precision: 0.7815\n",
      "      • Recall:    0.7803\n",
      "      • F1 Score:  0.7804 (±0.0405)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "Evaluando modelo: perplexity/sonar\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "Evaluando modelo: openai/o1-mini\n",
      "Evaluando modelo: anthropic/claude-sonnet-4\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.5363 (±0.1980)\n",
      "   microsoft/phi-4:\n",
      "      • Similitud: 0.4931 (±0.2036)\n",
      "   amazon/nova-lite-v1:\n",
      "      • Similitud: 0.5654 (±0.1950)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.5386 (±0.1955)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • Similitud: 0.5864 (±0.1892)\n",
      "   google/gemma-2-27b-it:\n",
      "      • Similitud: 0.5509 (±0.1932)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • Similitud: 0.5388 (±0.1905)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • Similitud: 0.5693 (±0.1964)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • Similitud: 0.6167 (±0.1870)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • Similitud: 0.5440 (±0.1872)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • Similitud: 0.5815 (±0.1906)\n",
      "   perplexity/sonar:\n",
      "      • Similitud: 0.6536 (±0.1792)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • Similitud: 0.6039 (±0.1844)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • Similitud: 0.5570 (±0.1922)\n",
      "   google/gemini-2.5-flash:\n",
      "      • Similitud: 0.6050 (±0.1834)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • Similitud: 0.5852 (±0.1894)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • Similitud: 0.6182 (±0.1880)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • Similitud: 0.6009 (±0.1840)\n",
      "   openai/o4-mini-high:\n",
      "      • Similitud: 0.5921 (±0.1984)\n",
      "   openai/gpt-4.1:\n",
      "      • Similitud: 0.6304 (±0.1788)\n",
      "   openai/o1-mini:\n",
      "      • Similitud: 0.5917 (±0.1965)\n",
      "   anthropic/claude-sonnet-4:\n",
      "      • Similitud: 0.6427 (±0.1824)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: microsoft/phi-4\n",
      "Evaluando modelo: amazon/nova-lite-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: qwen/qwen-2.5-72b-instruct\n",
      "Evaluando modelo: google/gemma-2-27b-it\n",
      "Evaluando modelo: meta-llama/llama-3.3-70b-instruct\n",
      "Evaluando modelo: microsoft/wizardlm-2-8x22b\n",
      "Evaluando modelo: meta-llama/llama-4-maverick\n",
      "Evaluando modelo: qwen/qwen2.5-vl-32b-instruct:free\n",
      "Evaluando modelo: x-ai/grok-3-mini-beta\n",
      "Evaluando modelo: perplexity/sonar\n",
      "Evaluando modelo: mistralai/mistral-medium-3\n",
      "Evaluando modelo: mistralai/mixtral-8x7b-instruct\n",
      "Evaluando modelo: google/gemini-2.5-flash\n",
      "Evaluando modelo: meta-llama/llama-3.1-405b-instruct\n",
      "Evaluando modelo: deepseek/deepseek-chat-v3.1\n",
      "Evaluando modelo: moonshotai/kimi-k2-0905\n",
      "Evaluando modelo: openai/o4-mini-high\n",
      "Evaluando modelo: openai/gpt-4.1\n",
      "Evaluando modelo: openai/o1-mini\n",
      "Evaluando modelo: anthropic/claude-sonnet-4\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2217 (±0.0878)\n",
      "   microsoft/phi-4:\n",
      "      • chrF: 0.2343 (±0.0785)\n",
      "   amazon/nova-lite-v1:\n",
      "      • chrF: 0.2293 (±0.0950)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.2294 (±0.0888)\n",
      "   qwen/qwen-2.5-72b-instruct:\n",
      "      • chrF: 0.2265 (±0.0970)\n",
      "   google/gemma-2-27b-it:\n",
      "      • chrF: 0.2245 (±0.0939)\n",
      "   meta-llama/llama-3.3-70b-instruct:\n",
      "      • chrF: 0.1732 (±0.0837)\n",
      "   microsoft/wizardlm-2-8x22b:\n",
      "      • chrF: 0.2558 (±0.0953)\n",
      "   meta-llama/llama-4-maverick:\n",
      "      • chrF: 0.2569 (±0.1065)\n",
      "   qwen/qwen2.5-vl-32b-instruct:free:\n",
      "      • chrF: 0.2274 (±0.0789)\n",
      "   x-ai/grok-3-mini-beta:\n",
      "      • chrF: 0.2238 (±0.0857)\n",
      "   perplexity/sonar:\n",
      "      • chrF: 0.2790 (±0.1151)\n",
      "   mistralai/mistral-medium-3:\n",
      "      • chrF: 0.2436 (±0.0959)\n",
      "   mistralai/mixtral-8x7b-instruct:\n",
      "      • chrF: 0.2370 (±0.0937)\n",
      "   google/gemini-2.5-flash:\n",
      "      • chrF: 0.2547 (±0.0953)\n",
      "   meta-llama/llama-3.1-405b-instruct:\n",
      "      • chrF: 0.2433 (±0.0989)\n",
      "   deepseek/deepseek-chat-v3.1:\n",
      "      • chrF: 0.2802 (±0.1036)\n",
      "   moonshotai/kimi-k2-0905:\n",
      "      • chrF: 0.2555 (±0.0955)\n",
      "   openai/o4-mini-high:\n",
      "      • chrF: 0.2322 (±0.1018)\n",
      "   openai/gpt-4.1:\n",
      "      • chrF: 0.2529 (±0.0968)\n",
      "   openai/o1-mini:\n",
      "      • chrF: 0.2306 (±0.0990)\n",
      "   anthropic/claude-sonnet-4:\n",
      "      • chrF: 0.2982 (±0.1119)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 3 COMPLETADO\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 3: Modismo + Ejemplo → Literal + Definición\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo genera interpretaciones literales correctas\")\n",
    "print(\"Métricas:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\")\n",
    "print(\"  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\")\n",
    "print(\"  - chrF: Character n-gram F-score\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_3_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p3 = json.load(f)\n",
    "\n",
    "# Filtrar datos con literal y definición generados\n",
    "data_p3_valid = [d for d in data_p3 if d.get('significado_real') and d.get('definicion_generada')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p3_valid)} registros válidos de {len(data_p3)} totales\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BERTScore con BETO y SciBETO\n",
    "# ============================================================================\n",
    "bert_models = [\n",
    "    ('BETO', compute_bertscore_beto),\n",
    "    ('SciBETO', compute_bertscore_sci_beto)\n",
    "]\n",
    "\n",
    "for bert_name, bert_func in bert_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Calculando BERTScore con {bert_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    resultados_p3 = []\n",
    "    \n",
    "    for model in MODEL_NAMES:\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        \n",
    "        # Filtrar datos de este modelo\n",
    "        model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "        \n",
    "        if not model_data:\n",
    "            print(f\"  ⚠ No hay datos para {model} - asignando métricas en 0\")\n",
    "            resultados_p3.append({\n",
    "                'modismo': 'N/A',\n",
    "                'ejemplo': '',\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': 0.0,\n",
    "                'recall': 0.0,\n",
    "                'f1_score': 0.0,\n",
    "                'significado_real': '',\n",
    "                'literal_generado': '',\n",
    "                'definicion_generada': ''\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        referencias = [d['significado_real'] for d in model_data]\n",
    "        candidatos = [d['definicion_generada'] for d in model_data]\n",
    "        \n",
    "        # BERTScore: compara definición generada vs significado real\n",
    "        P, R, F1 = bert_func(candidatos, referencias)\n",
    "        \n",
    "        for idx, (p, r, f1) in enumerate(zip(P.tolist(), R.tolist(), F1.tolist())):\n",
    "            resultados_p3.append({\n",
    "                'modismo': model_data[idx]['modismo'],\n",
    "                'ejemplo': model_data[idx]['ejemplo'],\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': p,\n",
    "                'recall': r,\n",
    "                'f1_score': f1,\n",
    "                'significado_real': referencias[idx],\n",
    "                'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "                'definicion_generada': candidatos[idx]\n",
    "            })\n",
    "    \n",
    "    # Guardar resultados\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'prompt_3_{bert_name.lower()}_bertscore_resultados.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados_p3, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"RESULTADOS con {bert_name}:\")\n",
    "    for model in MODEL_NAMES:\n",
    "        model_results = [r for r in resultados_p3 if r['modelo'] == model]\n",
    "        if model_results:\n",
    "            f1_scores = [r['f1_score'] for r in model_results]\n",
    "            precisions = [r['precision'] for r in model_results]\n",
    "            recalls = [r['recall'] for r in model_results]\n",
    "            \n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            p_mean = np.mean(precisions)\n",
    "            r_mean = np.mean(recalls)\n",
    "            \n",
    "            print(f\"   {model}:\")\n",
    "            print(f\"      • Precision: {p_mean:.4f}\")\n",
    "            print(f\"      • Recall:    {r_mean:.4f}\")\n",
    "            print(f\"      • F1 Score:  {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Sentence-BERT Similarity\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando Sentence-BERT Similarity\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_sbert = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_sbert.append({\n",
    "            'modismo': 'N/A',\n",
    "            'ejemplo': '',\n",
    "            'modelo': model,\n",
    "            'similarity': 0.0,\n",
    "            'significado_real': '',\n",
    "            'literal_generado': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['significado_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular similitud con Sentence-BERT\n",
    "    similarities = compute_sbert_similarity(candidatos, referencias)\n",
    "    \n",
    "    for idx, sim in enumerate(similarities):\n",
    "        resultados_sbert.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'ejemplo': model_data[idx]['ejemplo'],\n",
    "            'modelo': model,\n",
    "            'similarity': float(sim),\n",
    "            'significado_real': referencias[idx],\n",
    "            'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_3_sbert_similarity_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_sbert, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con Sentence-BERT:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_sbert if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        similarities = [r['similarity'] for r in model_results]\n",
    "        sim_mean = np.mean(similarities)\n",
    "        sim_std = np.std(similarities)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Similitud: {sim_mean:.4f} (±{sim_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. chrF Score\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando chrF Score\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_chrf = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model} - asignando métrica en 0\")\n",
    "        resultados_chrf.append({\n",
    "            'modismo': 'N/A',\n",
    "            'ejemplo': '',\n",
    "            'modelo': model,\n",
    "            'chrf_score': 0.0,\n",
    "            'significado_real': '',\n",
    "            'literal_generado': '',\n",
    "            'definicion_generada': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['significado_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular chrF\n",
    "    chrf_scores = compute_chrf_batch(candidatos, referencias)\n",
    "    \n",
    "    for idx, score in enumerate(chrf_scores):\n",
    "        resultados_chrf.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'ejemplo': model_data[idx]['ejemplo'],\n",
    "            'modelo': model,\n",
    "            'chrf_score': float(score),\n",
    "            'significado_real': referencias[idx],\n",
    "            'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_3_chrf_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_chrf, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con chrF:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_chrf if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        chrf_scores = [r['chrf_score'] for r in model_results]\n",
    "        chrf_mean = np.mean(chrf_scores)\n",
    "        chrf_std = np.std(chrf_scores)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • chrF: {chrf_mean:.4f} (±{chrf_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT 3 COMPLETADO\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
