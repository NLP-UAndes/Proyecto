{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d30cd0f2",
   "metadata": {},
   "source": [
    "# Evaluación de Modelos LLM en Modismos Colombianos\n",
    "\n",
    "Este notebook evalúa qué tan bien los modelos LLM entienden los modismos colombianos usando:\n",
    "- **BERTScore**: Para comparar similitud semántica entre textos (definiciones)\n",
    "- **Accuracy/Exact Match**: Para clasificación y coincidencias exactas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e03151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos en: Metricas LLM/Data_for_Metrics\n",
      "Resultados en: Metricas LLM/Metricas_Resultados\n",
      "Modelos: ['amazon/nova-micro-v1', 'cohere/command-r-08-2024']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Estilos para gráficos\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Modelos\n",
    "MODEL_NAMES = [\"amazon/nova-micro-v1\", \"cohere/command-r-08-2024\"]\n",
    "\n",
    "# Directorios\n",
    "DATA_DIR = 'Metricas LLM/Data_for_Metrics'\n",
    "OUTPUT_DIR = 'Metricas LLM/Metricas_Resultados'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Datos en: {DATA_DIR}\")\n",
    "print(f\"Resultados en: {OUTPUT_DIR}\")\n",
    "print(f\"Modelos: {MODEL_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28bca76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Métricas cargadas correctamente:\n",
      "  - BERTScore (BETO y SciBETO)\n",
      "  - Sentence-BERT (paraphrase-multilingual-mpnet-base-v2)\n",
      "  - chrF (Character n-gram F-score)\n"
     ]
    }
   ],
   "source": [
    "# Importar funciones de métricas\n",
    "import sys\n",
    "sys.path.append('Metricas LLM')\n",
    "\n",
    "from BertScore import compute_bertscore_beto, compute_bertscore_sci_beto\n",
    "from SentenceBert import compute_sbert_similarity\n",
    "from chrF import compute_chrf_batch\n",
    "\n",
    "print(\"✓ Métricas cargadas correctamente:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO)\")\n",
    "print(\"  - Sentence-BERT (paraphrase-multilingual-mpnet-base-v2)\")\n",
    "print(\"  - chrF (Character n-gram F-score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14c7be",
   "metadata": {},
   "source": [
    "## 1. PROMPT 1: Modismo → Definición\n",
    "\n",
    "**Métrica**: BERTScore (F1)  \n",
    "**Justificación**: Evalúa similitud semántica entre la definición generada y la real. Ideal para entender si el modelo captura el significado del modismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2dd6f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 1: Modismo → Definición\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo puede generar una definición correcta del modismo\n",
      "Métricas:\n",
      "  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\n",
      "  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\n",
      "  - chrF: Character n-gram F-score\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 20 registros válidos de 20 totales\n",
      "\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con BETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.4491\n",
      "      • Recall:    0.4305\n",
      "      • F1 Score:  0.4389 (±0.0617)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.4583\n",
      "      • Recall:    0.4338\n",
      "      • F1 Score:  0.4454 (±0.0906)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.4491\n",
      "      • Recall:    0.4305\n",
      "      • F1 Score:  0.4389 (±0.0617)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.4583\n",
      "      • Recall:    0.4338\n",
      "      • F1 Score:  0.4454 (±0.0906)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7636\n",
      "      • Recall:    0.7536\n",
      "      • F1 Score:  0.7584 (±0.0314)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.7612\n",
      "      • Recall:    0.7476\n",
      "      • F1 Score:  0.7541 (±0.0475)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.3897 (±0.2037)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.4064 (±0.2457)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2352 (±0.0570)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.2171 (±0.0933)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 1 COMPLETADO\n",
      "================================================================================\n",
      "\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7636\n",
      "      • Recall:    0.7536\n",
      "      • F1 Score:  0.7584 (±0.0314)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.7612\n",
      "      • Recall:    0.7476\n",
      "      • F1 Score:  0.7541 (±0.0475)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.3897 (±0.2037)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.4064 (±0.2457)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2352 (±0.0570)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.2171 (±0.0933)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_1_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 1 COMPLETADO\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 1: Modismo → Definición\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo puede generar una definición correcta del modismo\")\n",
    "print(\"Métricas:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\")\n",
    "print(\"  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\")\n",
    "print(\"  - chrF: Character n-gram F-score\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_1_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p1 = json.load(f)\n",
    "\n",
    "# Filtrar datos vacíos\n",
    "data_p1_valid = [d for d in data_p1 if d.get('definicion_real') and d.get('definicion_generada')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p1_valid)} registros válidos de {len(data_p1)} totales\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BERTScore con BETO y SciBETO\n",
    "# ============================================================================\n",
    "bert_models = [\n",
    "    ('BETO', compute_bertscore_beto),\n",
    "    ('SciBETO', compute_bertscore_sci_beto)\n",
    "]\n",
    "\n",
    "for bert_name, bert_func in bert_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Calculando BERTScore con {bert_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    resultados_p1 = []\n",
    "    \n",
    "    for model in MODEL_NAMES:\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        \n",
    "        # Filtrar datos de este modelo\n",
    "        model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "        \n",
    "        if not model_data:\n",
    "            print(f\"  ⚠ No hay datos para {model}\")\n",
    "            continue\n",
    "        \n",
    "        referencias = [d['definicion_real'] for d in model_data]\n",
    "        candidatos = [d['definicion_generada'] for d in model_data]\n",
    "        \n",
    "        # BERTScore: compara candidatos (generados) vs referencias (reales)\n",
    "        P, R, F1 = bert_func(candidatos, referencias)\n",
    "        \n",
    "        for idx, (p, r, f1) in enumerate(zip(P.tolist(), R.tolist(), F1.tolist())):\n",
    "            resultados_p1.append({\n",
    "                'modismo': model_data[idx]['modismo'],\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': p,\n",
    "                'recall': r,\n",
    "                'f1_score': f1,\n",
    "                'definicion_real': referencias[idx],\n",
    "                'definicion_generada': candidatos[idx]\n",
    "            })\n",
    "    \n",
    "    # Guardar resultados\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'prompt_1_{bert_name.lower()}_bertscore_resultados.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados_p1, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"RESULTADOS con {bert_name}:\")\n",
    "    for model in MODEL_NAMES:\n",
    "        model_results = [r for r in resultados_p1 if r['modelo'] == model]\n",
    "        if model_results:\n",
    "            f1_scores = [r['f1_score'] for r in model_results]\n",
    "            precisions = [r['precision'] for r in model_results]\n",
    "            recalls = [r['recall'] for r in model_results]\n",
    "            \n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            p_mean = np.mean(precisions)\n",
    "            r_mean = np.mean(recalls)\n",
    "            \n",
    "            print(f\"   {model}:\")\n",
    "            print(f\"      • Precision: {p_mean:.4f}\")\n",
    "            print(f\"      • Recall:    {r_mean:.4f}\")\n",
    "            print(f\"      • F1 Score:  {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Sentence-BERT Similarity\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando Sentence-BERT Similarity\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_sbert = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model}\")\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['definicion_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular similitud con Sentence-BERT\n",
    "    similarities = compute_sbert_similarity(candidatos, referencias)\n",
    "    \n",
    "    for idx, sim in enumerate(similarities):\n",
    "        resultados_sbert.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'modelo': model,\n",
    "            'similarity': float(sim),\n",
    "            'definicion_real': referencias[idx],\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_1_sbert_similarity_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_sbert, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con Sentence-BERT:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_sbert if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        similarities = [r['similarity'] for r in model_results]\n",
    "        sim_mean = np.mean(similarities)\n",
    "        sim_std = np.std(similarities)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Similitud: {sim_mean:.4f} (±{sim_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. chrF Score\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando chrF Score\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_chrf = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p1_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model}\")\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['definicion_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular chrF\n",
    "    chrf_scores = compute_chrf_batch(candidatos, referencias)\n",
    "    \n",
    "    for idx, score in enumerate(chrf_scores):\n",
    "        resultados_chrf.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'modelo': model,\n",
    "            'chrf_score': float(score),\n",
    "            'definicion_real': referencias[idx],\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_1_chrf_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_chrf, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con chrF:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_chrf if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        chrf_scores = [r['chrf_score'] for r in model_results]\n",
    "        chrf_mean = np.mean(chrf_scores)\n",
    "        chrf_std = np.std(chrf_scores)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • chrF: {chrf_mean:.4f} (±{chrf_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT 1 COMPLETADO\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545337c",
   "metadata": {},
   "source": [
    "## 2. PROMPT 2: Modismo → Es Modismo (Sí/No)\n",
    "\n",
    "**Métrica**: Accuracy  \n",
    "**Justificación**: Es una tarea de clasificación binaria. Mide si el modelo identifica correctamente que una expresión es un modismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a5292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 2: Modismo → ¿Es Modismo? (Sí/No)\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo identifica correctamente que una expresión es un modismo\n",
      "Métrica: Accuracy (Exactitud)\n",
      "Justificación: Problema de clasificación binaria (Sí/No).\n",
      "   Mide el porcentaje de respuestas correctas.\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 20 registros válidos de 20 totales\n",
      "\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "  • Accuracy: 0.1000 (1/10 correctos)\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "  • Accuracy: 0.5000 (5/10 correctos)\n",
      "\n",
      "RESULTADOS:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Accuracy:  0.1000 (1/10 correctos)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Accuracy:  0.5000 (5/10 correctos)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_2_accuracy_resultados.json\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 2: Modismo → ¿Es Modismo? (Sí/No)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo identifica correctamente que una expresión es un modismo\")\n",
    "print(\"Métrica: Accuracy (Exactitud)\")\n",
    "print(\"Justificación: Problema de clasificación binaria (Sí/No).\")\n",
    "print(\"   Mide el porcentaje de respuestas correctas.\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_2_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p2 = json.load(f)\n",
    "\n",
    "# Filtrar datos vacíos\n",
    "data_p2_valid = [d for d in data_p2 if d.get('es_modismo_real') and d.get('es_modismo_generado')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p2_valid)} registros válidos de {len(data_p2)} totales\")\n",
    "print()\n",
    "\n",
    "def normalizar_respuesta(respuesta):\n",
    "    \"\"\"Normaliza respuestas a 'Sí' o 'No'\"\"\"\n",
    "    if not respuesta:\n",
    "        return None\n",
    "    respuesta = str(respuesta).strip().lower()\n",
    "    if respuesta in ['sí', 'si', 'yes', 's', 'true', '1']:\n",
    "        return 'Sí'\n",
    "    elif respuesta in ['no', 'n', 'false', '0']:\n",
    "        return 'No'\n",
    "    return respuesta\n",
    "\n",
    "resultados_p2 = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    # Filtrar datos de este modelo\n",
    "    model_data = [d for d in data_p2_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model}\")\n",
    "        continue\n",
    "    \n",
    "    # Calcular accuracy\n",
    "    correctos = 0\n",
    "    for d in model_data:\n",
    "        real = normalizar_respuesta(d['es_modismo_real'])\n",
    "        generado = normalizar_respuesta(d['es_modismo_generado'])\n",
    "        correcto = (real == generado)\n",
    "        \n",
    "        if correcto:\n",
    "            correctos += 1\n",
    "        \n",
    "        resultados_p2.append({\n",
    "            'modismo': d['modismo'],\n",
    "            'modelo': model,\n",
    "            'respuesta_real': real,\n",
    "            'respuesta_generada': generado,\n",
    "            'correcto': correcto\n",
    "        })\n",
    "    \n",
    "    accuracy = correctos / len(model_data) if model_data else 0\n",
    "    print(f\"  • Accuracy: {accuracy:.4f} ({correctos}/{len(model_data)} correctos)\")\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_2_accuracy_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_p2, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_p2 if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        correctos = sum(1 for r in model_results if r['correcto'])\n",
    "        total = len(model_results)\n",
    "        accuracy = correctos / total if total > 0 else 0\n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Accuracy:  {accuracy:.4f} ({correctos}/{total} correctos)\")\n",
    "\n",
    "print()\n",
    "print(f\"✓ Guardado en: {output_file}\")\n",
    "print(\"=\"*80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03bb355",
   "metadata": {},
   "source": [
    "## 3. PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
    "\n",
    "**Métrica**: BERTScore (F1)  \n",
    "**Justificación**: Evalúa si el modelo puede generar interpretaciones literales del modismo en contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d89e4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
      "================================================================================\n",
      "Objetivo: Evaluar si el modelo genera interpretaciones literales correctas\n",
      "Métricas:\n",
      "  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\n",
      "  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\n",
      "  - chrF: Character n-gram F-score\n",
      "--------------------------------------------------------------------------------\n",
      "Datos cargados: 18 registros válidos de 20 totales\n",
      "\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con BETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.5324\n",
      "      • Recall:    0.5194\n",
      "      • F1 Score:  0.5256 (±0.0741)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.5483\n",
      "      • Recall:    0.5307\n",
      "      • F1 Score:  0.5387 (±0.1035)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "\n",
      "RESULTADOS con BETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.5324\n",
      "      • Recall:    0.5194\n",
      "      • F1 Score:  0.5256 (±0.0741)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.5483\n",
      "      • Recall:    0.5307\n",
      "      • F1 Score:  0.5387 (±0.1035)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_beto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando BERTScore con SciBETO\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7861\n",
      "      • Recall:    0.7712\n",
      "      • F1 Score:  0.7784 (±0.0272)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.8104\n",
      "      • Recall:    0.7922\n",
      "      • F1 Score:  0.8007 (±0.0453)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.4934 (±0.1952)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.6211 (±0.1605)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2694 (±0.0580)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.3215 (±0.1285)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 3 COMPLETADO\n",
      "================================================================================\n",
      "\n",
      "\n",
      "RESULTADOS con SciBETO:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Precision: 0.7861\n",
      "      • Recall:    0.7712\n",
      "      • F1 Score:  0.7784 (±0.0272)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Precision: 0.8104\n",
      "      • Recall:    0.7922\n",
      "      • F1 Score:  0.8007 (±0.0453)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_scibeto_bertscore_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando Sentence-BERT Similarity\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con Sentence-BERT:\n",
      "   amazon/nova-micro-v1:\n",
      "      • Similitud: 0.4934 (±0.1952)\n",
      "   cohere/command-r-08-2024:\n",
      "      • Similitud: 0.6211 (±0.1605)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_sbert_similarity_resultados.json\n",
      "\n",
      "============================================================\n",
      "Calculando chrF Score\n",
      "============================================================\n",
      "Evaluando modelo: amazon/nova-micro-v1\n",
      "Evaluando modelo: cohere/command-r-08-2024\n",
      "\n",
      "RESULTADOS con chrF:\n",
      "   amazon/nova-micro-v1:\n",
      "      • chrF: 0.2694 (±0.0580)\n",
      "   cohere/command-r-08-2024:\n",
      "      • chrF: 0.3215 (±0.1285)\n",
      "\n",
      "✓ Guardado en: Metricas LLM/Metricas_Resultados/prompt_3_chrf_resultados.json\n",
      "\n",
      "================================================================================\n",
      "PROMPT 3 COMPLETADO\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PROMPT 3: Modismo + Ejemplo → Literal + Definición\")\n",
    "print(\"=\"*80)\n",
    "print(\"Objetivo: Evaluar si el modelo genera interpretaciones literales correctas\")\n",
    "print(\"Métricas:\")\n",
    "print(\"  - BERTScore (BETO y SciBETO): Similitud semántica con embeddings de BERT\")\n",
    "print(\"  - Sentence-BERT: Similitud con modelo multilingüe paraphrase-mpnet\")\n",
    "print(\"  - chrF: Character n-gram F-score\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cargar datos desde JSON\n",
    "with open(os.path.join(DATA_DIR, 'prompt_3_metrics_data.json'), 'r', encoding='utf-8') as f:\n",
    "    data_p3 = json.load(f)\n",
    "\n",
    "# Filtrar datos con literal y definición generados\n",
    "data_p3_valid = [d for d in data_p3 if d.get('significado_real') and d.get('definicion_generada')]\n",
    "\n",
    "print(f\"Datos cargados: {len(data_p3_valid)} registros válidos de {len(data_p3)} totales\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BERTScore con BETO y SciBETO\n",
    "# ============================================================================\n",
    "bert_models = [\n",
    "    ('BETO', compute_bertscore_beto),\n",
    "    ('SciBETO', compute_bertscore_sci_beto)\n",
    "]\n",
    "\n",
    "for bert_name, bert_func in bert_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Calculando BERTScore con {bert_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    resultados_p3 = []\n",
    "    \n",
    "    for model in MODEL_NAMES:\n",
    "        print(f\"Evaluando modelo: {model}\")\n",
    "        \n",
    "        # Filtrar datos de este modelo\n",
    "        model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "        \n",
    "        if not model_data:\n",
    "            print(f\"  ⚠ No hay datos para {model}\")\n",
    "            continue\n",
    "        \n",
    "        referencias = [d['significado_real'] for d in model_data]\n",
    "        candidatos = [d['definicion_generada'] for d in model_data]\n",
    "        \n",
    "        # BERTScore: compara definición generada vs significado real\n",
    "        P, R, F1 = bert_func(candidatos, referencias)\n",
    "        \n",
    "        for idx, (p, r, f1) in enumerate(zip(P.tolist(), R.tolist(), F1.tolist())):\n",
    "            resultados_p3.append({\n",
    "                'modismo': model_data[idx]['modismo'],\n",
    "                'ejemplo': model_data[idx]['ejemplo'],\n",
    "                'modelo': model,\n",
    "                'bert_model': bert_name,\n",
    "                'precision': p,\n",
    "                'recall': r,\n",
    "                'f1_score': f1,\n",
    "                'significado_real': referencias[idx],\n",
    "                'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "                'definicion_generada': candidatos[idx]\n",
    "            })\n",
    "    \n",
    "    # Guardar resultados\n",
    "    output_file = os.path.join(OUTPUT_DIR, f'prompt_3_{bert_name.lower()}_bertscore_resultados.json')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(resultados_p3, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"RESULTADOS con {bert_name}:\")\n",
    "    for model in MODEL_NAMES:\n",
    "        model_results = [r for r in resultados_p3 if r['modelo'] == model]\n",
    "        if model_results:\n",
    "            f1_scores = [r['f1_score'] for r in model_results]\n",
    "            precisions = [r['precision'] for r in model_results]\n",
    "            recalls = [r['recall'] for r in model_results]\n",
    "            \n",
    "            f1_mean = np.mean(f1_scores)\n",
    "            f1_std = np.std(f1_scores)\n",
    "            p_mean = np.mean(precisions)\n",
    "            r_mean = np.mean(recalls)\n",
    "            \n",
    "            print(f\"   {model}:\")\n",
    "            print(f\"      • Precision: {p_mean:.4f}\")\n",
    "            print(f\"      • Recall:    {r_mean:.4f}\")\n",
    "            print(f\"      • F1 Score:  {f1_mean:.4f} (±{f1_std:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Sentence-BERT Similarity\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando Sentence-BERT Similarity\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_sbert = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model}\")\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['significado_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular similitud con Sentence-BERT\n",
    "    similarities = compute_sbert_similarity(candidatos, referencias)\n",
    "    \n",
    "    for idx, sim in enumerate(similarities):\n",
    "        resultados_sbert.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'ejemplo': model_data[idx]['ejemplo'],\n",
    "            'modelo': model,\n",
    "            'similarity': float(sim),\n",
    "            'significado_real': referencias[idx],\n",
    "            'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_3_sbert_similarity_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_sbert, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con Sentence-BERT:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_sbert if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        similarities = [r['similarity'] for r in model_results]\n",
    "        sim_mean = np.mean(similarities)\n",
    "        sim_std = np.std(similarities)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • Similitud: {sim_mean:.4f} (±{sim_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. chrF Score\n",
    "# ============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Calculando chrF Score\")\n",
    "print('='*60)\n",
    "\n",
    "resultados_chrf = []\n",
    "\n",
    "for model in MODEL_NAMES:\n",
    "    print(f\"Evaluando modelo: {model}\")\n",
    "    \n",
    "    model_data = [d for d in data_p3_valid if d.get('modelo') == model]\n",
    "    \n",
    "    if not model_data:\n",
    "        print(f\"  ⚠ No hay datos para {model}\")\n",
    "        continue\n",
    "    \n",
    "    referencias = [d['significado_real'] for d in model_data]\n",
    "    candidatos = [d['definicion_generada'] for d in model_data]\n",
    "    \n",
    "    # Calcular chrF\n",
    "    chrf_scores = compute_chrf_batch(candidatos, referencias)\n",
    "    \n",
    "    for idx, score in enumerate(chrf_scores):\n",
    "        resultados_chrf.append({\n",
    "            'modismo': model_data[idx]['modismo'],\n",
    "            'ejemplo': model_data[idx]['ejemplo'],\n",
    "            'modelo': model,\n",
    "            'chrf_score': float(score),\n",
    "            'significado_real': referencias[idx],\n",
    "            'literal_generado': model_data[idx].get('literal_generado', ''),\n",
    "            'definicion_generada': candidatos[idx]\n",
    "        })\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = os.path.join(OUTPUT_DIR, 'prompt_3_chrf_resultados.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(resultados_chrf, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print()\n",
    "print(\"RESULTADOS con chrF:\")\n",
    "for model in MODEL_NAMES:\n",
    "    model_results = [r for r in resultados_chrf if r['modelo'] == model]\n",
    "    if model_results:\n",
    "        chrf_scores = [r['chrf_score'] for r in model_results]\n",
    "        chrf_mean = np.mean(chrf_scores)\n",
    "        chrf_std = np.std(chrf_scores)\n",
    "        \n",
    "        print(f\"   {model}:\")\n",
    "        print(f\"      • chrF: {chrf_mean:.4f} (±{chrf_std:.4f})\")\n",
    "\n",
    "print(f\"\\n✓ Guardado en: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROMPT 3 COMPLETADO\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
