{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7bdf72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211428aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "path_source = \"Lexicc - CaroCuervo/Diccionario_breve_de_Colombiaismos_slown_text.txt\"\n",
    "\n",
    "#  limpieza \n",
    "def nfc(s: str) -> str:\n",
    "    return unicodedata.normalize(\"NFC\", s or \"\")\n",
    "\n",
    "def norm_spaces(s: str) -> str:\n",
    "    s = (s or \"\").replace(\"\\u00A0\", \" \").replace(\"\\u200b\", \"\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "# Expresión regular que detecta signos de puntuación solo al INICIO o al FINAL de una cadena.\n",
    "st_end_punt_re = re.compile(r\"^[¡!¿?«»\\\"'()\\[\\]{}·•.,;:—–-]+|[¡!¿?«»\\\"'()\\[\\]{}·•.,;:—–-]+$\")\n",
    "def strip_punct(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina signos de puntuación ubicados al inicio o al final de la cadena.\n",
    "    No afecta la puntuación que está en el medio.\n",
    "\n",
    "    Ejemplo:\n",
    "\n",
    "    Entrada: \"**¡ahijuelita!**\"\n",
    "    Salida:  \"ahijuelita\"\n",
    "    \"\"\"\n",
    "    return st_end_punt_re.sub(\"\", s or \"\").strip()\n",
    "\n",
    "\n",
    "def clean_curs(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Quita el marcado de negrita y cursiva en Markdown, dejando solo el texto limpio.\n",
    "\n",
    "    - Elimina `**` (negrita)\n",
    "    - Elimina `__` (subrayado)\n",
    "    - Reemplaza `*palabra*` por `palabra` (estaba en cursiva)\n",
    "\n",
    "    Ejemplo:\n",
    "\n",
    "    Entrada: \"**¡ahijuelita!**  *bonita*\"\n",
    "    Salida:  \"¡ahijuelita!  bonita\"\n",
    "    \"\"\"\n",
    "    s = (s or \"\").replace(\"**\", \"\").replace(\"__\", \"\")\n",
    "    s = re.sub(r\"\\*(.*?)\\*\", r\"\\1\", s)\n",
    "    return s\n",
    "\n",
    "# (por ejemplo: \"palabra||\", \"texto |\")\n",
    "remove_trailing_pipes_pattern = re.compile(r\"\\s*\\|+\\s*$\")\n",
    "\n",
    "def remove_trailing_pipes(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina las barras verticales '|' que aparecen al final de la cadena.\n",
    "    También remueve los espacios que puedan estar antes de las barras.\n",
    "\n",
    "    Ejemplo:\n",
    "\n",
    "    Entrada: \"ser vivo ||\"\n",
    "    Salida:  \"ser vivo\"\n",
    "\n",
    "    Entrada: \"abeja |\"\n",
    "    Salida:  \"abeja\"\n",
    "    \"\"\"\n",
    "    return remove_trailing_pipes_pattern.sub(\"\", s or \"\").strip()\n",
    "\n",
    "# al final de un lema, que se usan en entradas como \"bacano, a\" o \"cansado, da\".\n",
    "remove_gender_suffix_pattern = re.compile(r\"\\s*,\\s*(?:a|da)\\b\\.?\\s*$\", re.IGNORECASE)\n",
    "\n",
    "# Quitar sufijo de género \", a\" o \", da\"\n",
    "def remove_gender_suffix(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Quita el sufijo literal \", a\" o \", da\" al final de un lema para quedarse con la forma base.\n",
    "    Si no encuentra el sufijo, deja el texto sin cambios.\n",
    "\n",
    "    Ejemplo:\n",
    "\n",
    "    Entrada: \"bacano, a\"\n",
    "    Salida:  \"bacano\"\n",
    "\n",
    "    Entrada: \"cansado, da\"\n",
    "    Salida:  \"cansado\"\n",
    "\n",
    "    Entrada: \"árbol\"\n",
    "    Salida:  \"árbol\"\n",
    "    \"\"\"\n",
    "    before = token\n",
    "    after = remove_gender_suffix_pattern.sub(\"\", token)\n",
    "    return (after.strip() or before.strip())\n",
    "\n",
    "# Reemplazo de \"~\" por la palabra base (respetando sufijos y signos)\n",
    "def replace_tilde_with_base(text: str, base: str) -> str:\n",
    "    \"\"\"\n",
    "    Reemplaza la tilde \"~\" en una frase derivada por la palabra base del lema.\n",
    "    Mantiene correctamente sufijos, espacios y signos de puntuación.\n",
    "\n",
    "    Ejemplos:\n",
    "\n",
    "    - Entrada: texto=\"~ de encostalados\", palabra_base=\"carrera\"\n",
    "      Salida:  \"carrera de encostalados\"\n",
    "\n",
    "    - Entrada: texto=\"echar ~\", palabra_base=\"carreta\"\n",
    "      Salida:  \"echar carreta\"\n",
    "\n",
    "    - Entrada: texto=\"hablar ~.\", palabra_base=\"carreta\"\n",
    "      Salida:  \"hablar carreta.\"\n",
    "\n",
    "    - Entrada: texto=\"~osa\", palabra_base=\"carranch\"\n",
    "      Salida:  \"carranchosa\"\n",
    "    \"\"\"\n",
    "        \n",
    "    if not text:\n",
    "        return text\n",
    "    text = re.sub(r\"~([a-záéíóúüñ]+)\", lambda m: base + m.group(1), text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"~(\\s+)([a-záéíóúüñ]+)\", lambda m: base + m.group(1) + m.group(2), text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"~([.,;:?!])\", lambda m: base + m.group(1), text)\n",
    "    text = text.replace(\"~\", base)\n",
    "    return text\n",
    "\n",
    "# Encabezados con \"||\" (dentro del bloque en **...**)\n",
    "def resolve_pipe_tilde_in_head(head_bold_content: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Resuelve el caso de encabezados que usan \"||\" (pipes) y tilde \"~\".\n",
    "    Toma la parte DERECHA del \"||\" y reemplaza \"~\" por la palabra base que está a la izquierda.\n",
    "\n",
    "    Ejemplos:\n",
    "\n",
    "    - Entrada: lema_con_barras = \"carrera || ~ de encostalados\"\n",
    "      Salida:  \"carrera de encostalados\"\n",
    "\n",
    "    - Entrada: lema_con_barras = \"carraca || echar ~\"\n",
    "      Salida:  \"echar carraca\"\n",
    "\n",
    "    - Entrada: lema_con_barras = \"carreta || echar o hablar ~\"\n",
    "      Salida:  \"echar o hablar carreta\"\n",
    "    \"\"\"\n",
    "    parts = head_bold_content.split(\"||\", 1)\n",
    "    if len(parts) != 2:\n",
    "        return None\n",
    "    left, right = parts[0], parts[1]\n",
    "    base = strip_punct(clean_curs(norm_spaces(left)))\n",
    "    base = remove_gender_suffix(base)\n",
    "    base = strip_punct(base)\n",
    "    rhs = norm_spaces(right)\n",
    "    rhs = replace_tilde_with_base(rhs, base)\n",
    "    rhs = strip_punct(rhs)\n",
    "    rhs = remove_trailing_pipes(rhs)\n",
    "    rhs = norm_spaces(rhs)\n",
    "    return rhs\n",
    "\n",
    "# Extracción del lema en **negrita**\n",
    "\n",
    "re_bold_head = re.compile(r\"^-?\\s*\\*\\*(.*?)\\*\\*\")\n",
    "def extract_bold_head_general(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrae el lema (palabra principal) que aparece encerrado entre **negritas** \n",
    "    al inicio de una línea en el diccionario.\n",
    "\n",
    "    Ejemplos:\n",
    "\n",
    "    - Entrada: linea = \"- **carraca** f. coloq. Mandíbula del hombre o los animales.\"\n",
    "      Salida:  \"carraca\"\n",
    "\n",
    "    - Entrada: linea = \"- **carreta || echar o hablar ~** fr. coloq. Hablar cosas triviales...\"\n",
    "      Salida:  \"carreta || echar o hablar ~\"\n",
    "\n",
    "    - Entrada: linea = \"- **carranchil (carranchín)** m. Enfermedad cutánea caracterizada por un fuerte escozor.\"\n",
    "      Salida:  \"carranchil (carranchín)\"\n",
    "\n",
    "    - Entrada: linea = \"- **carrera || ~ de encostalados** Competencia deportiva en la cual los participantes...\"\n",
    "      Salida:  \"carrera || ~ de encostalados\"\n",
    "    \"\"\"\n",
    "    m = re_bold_head.search(line)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def remove_optional_plural_suffix(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina sufijos de plural opcional \"(s)\" o \"(es)\" de un lema.\n",
    "\n",
    "    Esto es común en el diccionario cuando un lema admite plural, \n",
    "    pero no se quiere que aparezca como parte de la palabra base.\n",
    "\n",
    "    Ejemplos :\n",
    "    \n",
    "    - Entrada: \"botarata(s)\"  \n",
    "      Salida:  \"botarata\"\n",
    "\n",
    "    - Entrada: \"carriel(es)\"  \n",
    "      Salida:  \"carriel\"\n",
    "\n",
    "    - Entrada: \"mataburro(s)\"  \n",
    "      Salida:  \"mataburro\"\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\((?:s|es)\\)$\", \"\", token, flags=re.IGNORECASE)\n",
    "\n",
    "def normalize_headword_content(head_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza el lema extraído de la cabecera en **negrita** de un artículo.\n",
    "\n",
    "    Acciones realizadas:\n",
    "    1. Normaliza caracteres Unicode a forma NFC.\n",
    "    2. Si el lema contiene '||' (derivados con \"~\"), los resuelve\n",
    "       reemplazando \"~\" por la palabra base.\n",
    "    3. Elimina sufijos de plural opcional \"(s)\" o \"(es)\".\n",
    "    4. Descarta variantes indicadas entre paréntesis después del lema.\n",
    "    5. Elimina colas literales como \", a\" o \", da\" (marcas de género).\n",
    "    6. Limpia signos de puntuación y barras verticales sobrantes.\n",
    "    7. Normaliza espacios en blanco.\n",
    "\n",
    "    Ejemplos :\n",
    "    \n",
    "    - Entrada: \"carranchil (carranchín)\"\n",
    "      Salida:  \"carranchil\"\n",
    "\n",
    "    - Entrada: \"carrera || ~ de encostalados\"\n",
    "      Salida:  \"carrera de encostalados\"\n",
    "\n",
    "    - Entrada: \"botarata(s)\"\n",
    "      Salida:  \"botarata\"\n",
    "    \"\"\"\n",
    "    head = nfc(head_content)\n",
    "    if \"||\" in head:\n",
    "        resolved = resolve_pipe_tilde_in_head(head)\n",
    "        if resolved:\n",
    "            return nfc(remove_optional_plural_suffix(resolved))\n",
    "    token = norm_spaces(head)\n",
    "    token = remove_optional_plural_suffix(token)\n",
    "    token = token.split(\" (\", 1)[0]\n",
    "    token = remove_gender_suffix(token)\n",
    "    token = strip_punct(token)\n",
    "    token = remove_trailing_pipes(token)\n",
    "    token = norm_spaces(token)\n",
    "    return nfc(token)\n",
    "\n",
    "# Ocurrencias externas \"|| **...~...**\"\n",
    "external_occurrence_pattern = re.compile(\n",
    "    r\"\\|\\|\\s*\\*{2}\\s*(.*?)\\s*\\*{2}\\s*([^|]*)(?=(?:\\|\\||$))\",\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "\n",
    "# Eliminar todas las abreviaturas gramaticales y bloques de regiones\n",
    "grammatical_tags  = [\n",
    "    \"m\", \"f\", \"pl\", \"sing\", \"sust\",\n",
    "    \"adj\", \"adv\", \"intr\", \"tr\", \"prnl\", \"fr\", \"interj\",\n",
    "    \"coloq\", \"pop\", \"rur\", \"vulg\", \"despect\", \"obsol\"\n",
    "]\n",
    "\n",
    "# Patrón para reconocer combinaciones de género \"m. y f.\"\n",
    "gender_pattern  = r\"(?:m(?:\\s*\\.)?\\s*(?:y\\s*f(?:\\s*\\.)?)?)\"\n",
    "\n",
    "# Patrón para identificar expresiones como \"U. t. c. adj./s./prnl.\"\n",
    "utc_pattern_PATTERN = r\"U\\s*\\.\\s*t\\s*\\.\\s*c\\s*\\.\\s*(?:s|adj|prnl)\\s*\\.?\"\n",
    "\n",
    "\n",
    "# Elimina cosas como: \"m.\", \"f.\", \"adj.\", \"intr.\", \"U. t. c. adj.\", etc.\n",
    "drop_tag = re.compile(\n",
    "    r\"(?:\\b{mf}\\b)|(?:\\b(?:{base})(?:\\s*\\.)?\\b)|(?:\\b{utc_pattern}\\b)\".format(\n",
    "        mf=gender_pattern, base=\"|\".join(grammatical_tags ), utc_pattern=utc_pattern_PATTERN\n",
    "    ),\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Bloques de REGIONES: SOLO si están EN CURSIVA (entre * ... *)\n",
    "region_names = [\n",
    "    \"Amaz\",\"Ant\",\"Atl\",\"Bog\",\"Bol\",\"Boy\",\"Cald\",\"Chocó\",\"Choco\",\"Córd\",\"Cord\",\n",
    "    \"Costa Atl\",\"Costa Pacíf\",\"Costa Pacif\",\"Cund\",\"Guaj\",\"Magd\",\"Nar\",\"NStder\",\"Stder\",\n",
    "    \"Quind\",\"Risar\",\"Tol\",\"Valle\",\"Huila\",\n",
    "    \"Amazonas\",\"Antioquia\",\"Atlántico\",\"Atlantico\",\"Bogotá\",\"Bogota\",\"Bolívar\",\"Bolivar\",\n",
    "    \"Boyacá\",\"Boyaca\",\"Caldas\",\"Córdoba\",\"Cordoba\",\n",
    "    \"Costa Atlántica\",\"Costa Atlantica\",\"Costa del Pacífico\",\"Costa del Pacifico\",\n",
    "    \"Cundinamarca\",\"La Guajira\",\"Magdalena\",\"Nariño\",\"Narino\",\n",
    "    \"Norte de Santander\",\"Santander\",\"Quindío\",\"Quindio\",\"Risaralda\",\"Tolima\",\n",
    "    \"Valle del Cauca\",\"Cauca\",\"Meta\",\"Arauca\",\"Casanare\",\n",
    "    \"Putumayo\",\"Caquetá\",\"Caqueta\",\"Llanos\"\n",
    "]\n",
    "\n",
    "# Unión alternada escapada de todas las regiones para incrustar en regex.\n",
    "region_pattern_union = \"|\".join(sorted(map(re.escape, region_names), key=len, reverse=True))\n",
    "\n",
    "# Ejemplos válidos: *Ant., Cald., Valle.*   *Costa Atl., Cund.*\n",
    "# Están en cursiva, pueden tener punto o no, y estar separadas por comas o conjunciones.\n",
    "# NO se detectan si NO están en cursiva (entre asteriscos).\n",
    "italic_region_block = re.compile(\n",
    "    r\"\\*\\s*(?:{REG})(?:\\.)?(?:\\s*(?:,|y|o|u)\\s*(?:{REG})(?:\\.)?)*\\s*\\*\".format(REG=region_pattern_union),\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# limpieza de citas con esa estructura\n",
    "citation_w_paren_re = re.compile(\n",
    "    r'(?:-\\s*)?\\(\\s*(?:Carrasquilla|Le[oó]n Rey)\\s*,\\s*[IVXLCDM]+\\s*(?:,\\s*(?:copla\\s*)?\\d+)?\\s*\"?\\s*\\)?',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# citas entre comillas\n",
    "citation_w_quote_re = re.compile(\n",
    "    r'(?:-\\s*)?[\\\"“”]\\s*Le[oó]n Rey\\s*,\\s*[IVXLCDM]+\\s*[\\\"“”]',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "def remove_citations(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina citas de obras/autor incrustadas en el texto con los formatos detectados.\n",
    "\n",
    "    Borra patrones como:\n",
    "    - (Carrasquilla, I, 150\")\n",
    "    - (Carrasquilla, II, 291\")\n",
    "    - (León Rey, II, copla 3932\")\n",
    "    - \"León Rey, I\"  /  “León Rey, I”\n",
    "\n",
    "    Además, corrige espacios dobles y comas sobrantes antes de signos.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    out = citation_w_paren_re.sub(\"\", text)\n",
    "    out = citation_w_quote_re.sub(\"\", out)\n",
    "    out = re.sub(r\"\\s*,\\s*(?=[.,;:])\", \"\", out)\n",
    "    out = re.sub(r\"\\s{2,}\", \" \", out)\n",
    "    return norm_spaces(out)\n",
    "\n",
    "def clean_regional_blocks_and_grammar_tags(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia del texto:\n",
    "    1) Bloques de regiones SOLO cuando están en cursiva *...* \n",
    "       (p. ej., *Cund., Boy., Nar.*; *Ant., Cald., Valle.*).\n",
    "    2) Abreviaturas/etiquetas gramaticales con límites de palabra\n",
    "       (m., f., adj., intr., tr., prnl., fr., interj., coloq., etc., y \"U. t. c. ...\").\n",
    "    3) Citas bibliográficas mediante `strip_bibliographic_citations`.\n",
    "    4) Puntuación/espacios sobrantes tras las limpiezas.\n",
    "\n",
    "    - No afecta texto normal: no elimina \"nar\" dentro de \"nariz\", p ej., \n",
    "      porque los bloques regionales se borran solo si están en *cursiva*.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    out = text\n",
    "\n",
    "    # 1) Limpiar citas\n",
    "    out = remove_citations(out)\n",
    "\n",
    "    # 2) Quitar bloques de regiones estrictamente en cursiva\n",
    "    while True:\n",
    "        new_out = italic_region_block.sub(\"\", out)\n",
    "        if new_out == out:\n",
    "            break\n",
    "        out = new_out\n",
    "\n",
    "    # 3) Quitar abreviaturas gramaticales\n",
    "    out = drop_tag.sub(\"\", out)\n",
    "\n",
    "    # 4) Limpieza de puntuación y espacios\n",
    "    out = re.sub(r\"\\s*,\\s*(?=[.,;:])\", \"\", out)\n",
    "    out = re.sub(r\"\\s*[.,;:—–-]\\s*(?=[.,;:—–-])\", \" \", out)\n",
    "    out = re.sub(r\"\\s{2,}\", \" \", out)\n",
    "    out = norm_spaces(strip_punct(out))\n",
    "    return out\n",
    "\n",
    "# Detecta numeraciones en negrita tipo **2.**, **3.**\n",
    "bold_number_pattern  = re.compile(r\"\\*\\*\\s*(\\d+)\\.\\s*\\*\\*\")\n",
    "\n",
    "# Detecta numeraciones simples tipo 2. , 3. (sin negrita)\n",
    "plain_number_pattern = re.compile(r\"(?<!\\d)(\\d{1,2})\\.\\s\")\n",
    "\n",
    "\n",
    "def count_enumerations(full_text_after_head: str) -> int:\n",
    "    \"\"\"\n",
    "    Cuenta cuántas definiciones enumeradas (2., 3., etc.) aparecen\n",
    "    en el texto de un artículo después del lema.\n",
    "\n",
    "    Se usa para duplicar las filas en el DataFrame por cada acepción enumerada.\n",
    "\n",
    "    Ejemplos:\n",
    "   - count_numbered_definitions(\"**2.** Segunda acepción. **3.** Tercera acepción.\") -> 2\n",
    "    - count_numbered_definitions(\"Definición simple sin numeración.\") -> 0\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for g in bold_number_pattern.findall(full_text_after_head):\n",
    "        try:\n",
    "            if int(g) >= 2: count += 1\n",
    "        except: pass\n",
    "    for g in plain_number_pattern.findall(full_text_after_head):\n",
    "        try:\n",
    "            if int(g) >= 2: count += 1\n",
    "        except: pass\n",
    "    return count\n",
    "\n",
    "# detección de 1., 2., 3.\n",
    "enumeration_marker_pattern = re.compile(r\"(?:\\*\\*\\s*(\\d+)\\.\\s*\\*\\*|(?<!\\d)(\\d{1,2})\\.\\s)\")\n",
    "\n",
    "\n",
    "def extract_enumerated_senses_from_first_line(first_line: str, head_end_pos: int):\n",
    "\n",
    "    \"\"\"\n",
    "    Extrae y separa las definiciones enumeradas (1., 2., 3., …) de la primera línea del artículo.\n",
    "\n",
    "    Separa el texto en segmentos para cada acepción numerada, y de cada segmento\n",
    "    extrae el significado y el ejemplo usando `extract_meaning_example_from_tail`.\n",
    "\n",
    "    Parámetros:\n",
    "\n",
    "    first_line : str\n",
    "        Primera línea completa del artículo (incluyendo el lema y las definiciones).\n",
    "    head_end_pos : int\n",
    "        Índice en el que termina el lema en la línea.\n",
    "\n",
    "    Retorna:\n",
    "\n",
    "    list of tuple(str, str)\n",
    "        Lista de tuplas (significado, ejemplo) para cada acepción.\n",
    "\n",
    "    Ejemplos:\n",
    "    \n",
    "     - first_line = \"- **carramán** m. coloq. Vehículo viejo. **2.** Persona vieja, acabada.\"\n",
    "     -> split_enumerated_definitions_from_line(first_line, 11)\n",
    "    [('Vehículo viejo.', 'Persona vieja, acabada.')]\n",
    "\n",
    "     - first_line = \"- **carreta** f. Carrete para hilos. **2.** Carretilla para materiales. **3.** Charla trivial.\"\n",
    "     -> split_enumerated_definitions_from_line(first_line, 10)\n",
    "    [('Carrete para hilos.', ''), ('Carretilla para materiales.', ''), ('Charla trivial.', '')]\n",
    "    \"\"\"\n",
    "        \n",
    "    tail_raw = (first_line or \"\")[head_end_pos:]\n",
    "    tail = tail_raw\n",
    "    cuts = []\n",
    "    for m in enumeration_marker_pattern.finditer(tail):\n",
    "        num = m.group(1) or m.group(2)\n",
    "        try:\n",
    "            if int(num) >= 2:\n",
    "                cuts.append((m.start(), m.end()))\n",
    "        except:\n",
    "            pass\n",
    "    segments = []\n",
    "    if not cuts:\n",
    "        segments = [tail]\n",
    "    else:\n",
    "        start = 0\n",
    "        for (s, e) in cuts:\n",
    "            segments.append(tail[start:s])\n",
    "            start = e\n",
    "        segments.append(tail[start:])\n",
    "    senses = []\n",
    "    for seg in segments:\n",
    "        sig, ej = extract_meaning_example_from_tail(seg)\n",
    "        senses.append((sig, ej))\n",
    "    return senses\n",
    "\n",
    "# Contenido significativo antes del primer \"||\", HEAD significa que está entre **...**\n",
    "def extract_base_from_head(head_bold_content: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Extrae la parte base del lema que aparece ANTES del primer '||'.\n",
    "\n",
    "    Se utiliza para normalizar la palabra principal,\n",
    "    removiendo puntuación, cursivas y sufijos de género.\n",
    "\n",
    "    Ejemplos:\n",
    "    \n",
    "    - extract_base_from_head(\"carrera || ~ de encostalados\")\n",
    "     ->'carrera'\n",
    "\n",
    "    - extract_base_from_head(\"carranchil (carranchín)\")\n",
    "     ->'carranchil'\n",
    "    \"\"\"\n",
    "    left = head_bold_content.split(\"||\", 1)[0] if \"||\" in head_bold_content else head_bold_content\n",
    "    base = strip_punct(clean_curs(norm_spaces(left)))\n",
    "    base = remove_gender_suffix(base)\n",
    "    base = strip_punct(base)\n",
    "    return norm_spaces(base)\n",
    "\n",
    "\n",
    "def check_meaning_before_pipes(first_line: str, head_end_pos: int) -> bool:\n",
    "    \"\"\"\n",
    "    Verifica si hay contenido significativo (significado) ANTES del primer '||'\n",
    "    en el texto que sigue al lema.\n",
    "\n",
    "    Esto permite detectar si el lema principal tiene una definición propia,\n",
    "    antes de listar ocurrencias derivadas con '||'.\n",
    "\n",
    "    Ejemplo:\n",
    "    \n",
    "    - check_meaning_before_pipes(\"- **carraca** f. Mandíbula. || echar ~.\", 11)\n",
    "     -> True  # Tiene definición antes de '||'\n",
    "\n",
    "    - check_meaning_before_pipes(\"- **carrera || ~ de encostalados** Competencia...\", 7)\n",
    "     -> False # No hay definición antes de '||'\n",
    "    \"\"\"\n",
    "    tail = first_line[head_end_pos:]\n",
    "    before = tail.split(\"||\", 1)[0]\n",
    "    clean = clean_regional_blocks_and_grammar_tags(before)\n",
    "    clean = strip_punct(norm_spaces(clean))\n",
    "    return bool(clean)\n",
    "\n",
    "# Variantes entre paréntesis dentro del **lema** \n",
    "parenthetical_variants_pattern  = re.compile(r\"\\(([^)]+)\\)\")\n",
    "\n",
    "def extract_parenthetical_variants(head_bold_content: str) -> list:\n",
    "    \"\"\"\n",
    "    Extrae las variantes indicadas entre paréntesis en el lema principal.\n",
    "\n",
    "    Ignora los casos en los que el paréntesis solo indica plural opcional (s/es).\n",
    "\n",
    "    Ejemplos:\n",
    "    \n",
    "    - extract_variants_from_parentheses(\"carranchil (carranchín)\")\n",
    "      -> ['carranchín']\n",
    "\n",
    "    - extract_variants_from_parentheses(\"carriel(es)\")\n",
    "      ->  # Ignora plural opcional\n",
    "    \"\"\"\n",
    "\n",
    "    left = head_bold_content.split(\"||\", 1)[0] if \"||\" in head_bold_content else head_bold_content\n",
    "    m = parenthetical_variants_pattern .search(left)\n",
    "    if not m:\n",
    "        return []\n",
    "    content = m.group(1)\n",
    "    if re.fullmatch(r\"(?:s|es)\", content.strip(), flags=re.IGNORECASE):\n",
    "        return []\n",
    "    parts = re.split(r\"\\s*(?:,|/|;|\\bo\\b|\\bu\\b|\\by\\b)\\s*\", content, flags=re.IGNORECASE)\n",
    "    variants = []\n",
    "    for raw in parts:\n",
    "        v = norm_spaces(raw)\n",
    "        if not v: continue\n",
    "        if re.fullmatch(r\"(?:s|es)\", v, flags=re.IGNORECASE): continue\n",
    "        v = remove_gender_suffix(v)\n",
    "        v = strip_punct(v)\n",
    "        v = remove_trailing_pipes(v)\n",
    "        v = norm_spaces(v)\n",
    "        if v: variants.append(nfc(v))\n",
    "    return variants\n",
    "\n",
    "# Extracción de significado y ejemplo ----\n",
    "\n",
    "# Patrón para localizar el primer texto en cursiva (normalmente el ejemplo)\n",
    "first_italic_pattern  = re.compile(r\"\\*(.*?)\\*\")\n",
    "\n",
    "def extract_meaning_example_after_head(first_line: str, head_end_pos: int):\n",
    "    \"\"\"\n",
    "    Extrae el significado y el ejemplo de la PRIMERA LÍNEA de un artículo,\n",
    "    considerando el texto que sigue al lema.\n",
    "\n",
    "    Busca la primera frase en cursiva como el ejemplo.\n",
    "    El resto antes de la cursiva se considera el significado.\n",
    "\n",
    "    Ejemplo:\n",
    "    \n",
    "    - extract_meaning_and_example_from_headline( \"- **carraca** f. Mandíbula del hombre. *Había perdido la carraca.*\", 11)\n",
    "      -> ('Mandíbula del hombre.', 'Había perdido la carraca.')\n",
    "    \"\"\"\n",
    "    tail_raw = (first_line or \"\")[head_end_pos:]\n",
    "    tail = clean_regional_blocks_and_grammar_tags(tail_raw)\n",
    "    m = first_italic_pattern .search(tail)\n",
    "    if m:\n",
    "        significado = norm_spaces(strip_punct(tail[:m.start()]))\n",
    "        ejemplo     = norm_spaces(strip_punct(m.group(1)))\n",
    "    else:\n",
    "        significado = norm_spaces(strip_punct(tail))\n",
    "        ejemplo     = \"\"\n",
    "    significado = clean_regional_blocks_and_grammar_tags(significado)\n",
    "    ejemplo     = clean_regional_blocks_and_grammar_tags(ejemplo)\n",
    "    return significado, ejemplo\n",
    "\n",
    "def extract_meaning_example_from_tail(tail_raw: str):\n",
    "    \"\"\"\n",
    "    Extrae el significado y el ejemplo desde el resto de texto de un artículo\n",
    "    (sin el lema).\n",
    "\n",
    "    Busca el primer texto en cursiva como ejemplo. Lo anterior es el significado.\n",
    "\n",
    "    Ejemplo:\n",
    "\n",
    "    - extract_meaning_and_example_from_tail( \"Mandíbula del hombre. *Había perdido la carraca.*\")\n",
    "      -> ('Mandíbula del hombre.', 'Había perdido la carraca.')\n",
    "    \"\"\"\n",
    "    tail = clean_regional_blocks_and_grammar_tags(tail_raw or \"\")\n",
    "    m = first_italic_pattern .search(tail)\n",
    "    if m:\n",
    "        significado = norm_spaces(strip_punct(tail[:m.start()]))\n",
    "        ejemplo     = norm_spaces(strip_punct(m.group(1)))\n",
    "    else:\n",
    "        significado = norm_spaces(strip_punct(tail))\n",
    "        ejemplo     = \"\"\n",
    "    significado = clean_regional_blocks_and_grammar_tags(significado)\n",
    "    ejemplo     = clean_regional_blocks_and_grammar_tags(ejemplo)\n",
    "    return significado, ejemplo\n",
    "\n",
    "# Segmentación en LINEAS\n",
    "# línea que comienza (opcionalmente con \"- \") y sigue un bloque **...**\n",
    "article_start_regex = re.compile(r\"^\\s*-?\\s*\\*\\*\")\n",
    "articles, current = [], []\n",
    "with open(path_source, \"r\", encoding=\"utf-8\") as f:\n",
    "    for raw in f:\n",
    "        line = nfc(raw.rstrip(\"\\n\"))\n",
    "        if article_start_regex.match(line):\n",
    "            if current:\n",
    "                articles.append(\"\\n\".join(current))\n",
    "                current = []\n",
    "        current.append(line) \n",
    "    if current:\n",
    "        articles.append(\"\\n\".join(current))\n",
    "\n",
    "#  Reescritura de “Véase **X**”\n",
    "see_also_pattern  = re.compile(r\"\\bVéase\\b\\s*\\*{2}\\s*([^*]+?)\\s*\\*{2}\", flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def _first_line_tail_after_head(first_line: str) -> str:\n",
    "    \"\"\"\n",
    "    Devuelve el texto que sigue inmediatamente después del bloque de cabecera en **negrita**\n",
    "    dentro de la PRIMERA LÍNEA de un artículo.\n",
    "\n",
    "    Se asume que la cabecera ya cumple el formato \"- **lema** ...\".\n",
    "    Si no hay cabecera detectada, devuelve cadena vacía.\n",
    "\n",
    "    Ejemplos:\n",
    "    \n",
    "    - get_tail_after_head_from_first_line(\"- **carraca** f. Mandíbula. *Ejemplo*\")\n",
    "      -> \" f. Mandíbula. *Ejemplo*\"\n",
    "\n",
    "    - get_tail_after_head_from_first_line(\"- **carreta || echar ~** fr. coloq. Hablar cosas triviales.\")\n",
    "       -> \" fr. coloq. Hablar cosas triviales.\"\n",
    "    \"\"\"\n",
    "    m = re_bold_head.search(first_line or \"\")\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    return (first_line or \"\")[m.end():]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754245f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# usar mapa: lema_normalizado -> cola de su primera línea (para poder “pegarla” al que dice “Véase **X**”)\n",
    "\n",
    "lemma_to_tail = {}\n",
    "for art in articles:\n",
    "    fl = art.split(\"\\n\", 1)[0]\n",
    "    head = extract_bold_head_general(fl)\n",
    "    if not head:\n",
    "        continue\n",
    "    lema_norm = normalize_headword_content(head)\n",
    "    if lema_norm and lema_norm not in lemma_to_tail:\n",
    "        lemma_to_tail[lema_norm] = _first_line_tail_after_head(fl)\n",
    "\n",
    "# Reescritura de artículos que contienen “Véase **X**”\n",
    "rewritten_articles = []\n",
    "for art in articles:\n",
    "    parts = art.split(\"\\n\", 1)\n",
    "    fl = parts[0]\n",
    "    rest = parts[1] if len(parts) > 1 else \"\"\n",
    "    mh = re_bold_head.search(fl)\n",
    "    if not mh:\n",
    "        rewritten_articles.append(art)\n",
    "        continue\n",
    "\n",
    "    head_end = mh.end()\n",
    "    tail = fl[head_end:]\n",
    "\n",
    "    msee = see_also_pattern .search(tail)\n",
    "    if not msee:\n",
    "        rewritten_articles.append(art)\n",
    "        continue\n",
    "\n",
    "    target_raw = msee.group(1)\n",
    "    target_norm = normalize_headword_content(target_raw)\n",
    "    target_tail = lemma_to_tail.get(target_norm, \"\")\n",
    "\n",
    "    if not target_tail:\n",
    "        rewritten_articles.append(art)\n",
    "        continue\n",
    "\n",
    "    new_first_line = fl[:head_end] + \" \" + target_tail.strip()\n",
    "    new_art = new_first_line if not rest else (new_first_line + \"\\n\" + rest)\n",
    "    rewritten_articles.append(new_art)\n",
    "\n",
    "articles = rewritten_articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330ffd7",
   "metadata": {},
   "source": [
    "En el pipeline, los “Requisitos 1 y 2” son dos condiciones que determinan cómo extraer el significado y el ejemplo del artículo según su estructura. Req. 1 se aplica cuando el artículo es simple: no tiene || (pipes) ni numeraciones (2., 3., …), de modo que el significado y el ejemplo se toman directamente de la primera línea después del lema y se asignan al lema (y a sus variantes). Req. 2 se aplica cuando no hay numeraciones pero sí existen ||, ya sea dentro del lema o como ocurrencias externas: en este caso, el significado y el ejemplo de la primera línea se asignan a cada frase derivada generada con || y ~; además, si el lema tiene significado antes del primer ||, también se crea una fila para él. Cuando hay numeraciones, se ignoran Req. 1 y Req. 2 y se procesa cada acepción enumerada por separado. En resumen, Req. 1 maneja artículos de una sola acepción sin derivados, mientras que Req. 2 maneja artículos sin numeraciones pero con derivados; ambos evitan la lógica más compleja de segmentar por numeraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31ceb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           palabra                                        significado  \\\n",
      "0            abagó  Selección de los mejores frutos de la cosecha ...   \n",
      "1          abalear                Disparar balas sobre alguien o algo   \n",
      "2          abalear                            Herir o matar a balazos   \n",
      "3           abaleo                                            Tiroteo   \n",
      "4          abanico                               Ventilador eléctrico   \n",
      "5          abanico  Utensilio rústico, hecho de fibras vegetales e...   \n",
      "6       abaniquear                          Darse aire con un abanico   \n",
      "7        abarrotes                                            Víveres   \n",
      "8  ser (una) abeja                   Ser muy vivo, listo, aprovechado   \n",
      "9         abombado                           Que es ligeramente tonto   \n",
      "\n",
      "                                             ejemplo  \n",
      "0  Si yo supiera lo cierto/Cuál jue el que te aco...  \n",
      "1                                                     \n",
      "2                                                     \n",
      "3      En el abaleo resultaron heridas tres personas  \n",
      "4             Prende el abanico que hace mucho calor  \n",
      "5                                                     \n",
      "6                                                     \n",
      "7                                                     \n",
      "8                                                     \n",
      "9                                                     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PIPELINE PRINCIPAL: de artículos en texto -> filas (palabra, significado, ejemplo)\n",
    "\n",
    "\n",
    "# Este bloque recorre cada artículo ya preprocesado (segmentado en artículos,\n",
    "# reescritas las remisiones “Véase **X**”, y con utilidades de limpieza listas),\n",
    "# y construye la lista de filas 'rows' que luego se convierte a DataFrame.\n",
    "#\n",
    "# IDEAS CLAVE DEL PROCESAMIENTO:\n",
    "# - Un “artículo” es un bloque de líneas que comienza con una línea cuyo inicio\n",
    "#   contiene un lema en **negrita** (p. ej., \"- **carraca** …\").\n",
    "# - La primera línea del artículo contiene el lema y, frecuentemente, una definición\n",
    "#   inicial (opcional) y/o ocurrencias derivadas separadas con \"||\".\n",
    "# - Las “ocurrencias externas” son frases adicionales en **negrita** dentro del artículo,\n",
    "#   introducidas con \"|| **...~...**\", donde \"~\" se reemplaza por la base del lema.\n",
    "# - Las “acepciones enumeradas” (2., 3., …) duplican filas: una por cada acepción.\n",
    "# - Si no hay enumeraciones (solo una acepción) y hay pipes/tilde, el significado/ejemplo\n",
    "#   extraído de la primera línea se reparte a todas las frases derivadas (Req. 2).\n",
    "# - Se eliminan etiquetas gramaticales (m., f., adj., intr., …), bloques de regiones\n",
    "#   (SOLO si están *en cursiva*, p.ej. *Cund., Nar.*), y citas bibliográficas.\n",
    "# - NO se eliminan trozos de región si no están en cursiva; así previene borrar “Nar”\n",
    "#   dentro de “nariz”, por ejemplo.\n",
    "# - Se limpian signos de puntuación al borde, pipes colgantes, sufijos de género (\", a\" / \", da\"),\n",
    "#   y se normalizan espacios y Unicode (NFC).\n",
    "\n",
    "rows = []  # ← acumulador final de registros {\"palabra\": ..., \"significado\": ..., \"ejemplo\": ...}\n",
    "\n",
    "for art in articles:\n",
    "    # 1) Toma SOLO la primera línea; ahí está el lema en **negrita**\n",
    "    first_line = art.split(\"\\n\", 1)[0]\n",
    "\n",
    "    # 2) Extraer el bloque **...** tal cual aparece en la primera línea (puede incluir \"|| ~\")\n",
    "    head_content = extract_bold_head_general(first_line)\n",
    "    if not head_content:\n",
    "        # Si no hay lema en negrita, no es un artículo válido\n",
    "        continue\n",
    "\n",
    "    # 3) Normalizar el lema base:\n",
    "    #    - Se resuelven pipes/tilde si están dentro del lema (p.ej. \"**carreta || echar ~**\")\n",
    "    #    - Quitar plural opcional \"(s)/(es)\", sufijos \", a\"/\", da\", puntas de puntuación, pipes colgantes\n",
    "    #    - NFC + espacios normalizados\n",
    "    palabra_head = normalize_headword_content(head_content)\n",
    "    if not palabra_head:\n",
    "        # Lema vacío tras limpieza -> no hay palabra; saltar\n",
    "        continue\n",
    "\n",
    "    # 4) Localizar fin del bloque **lema** en la línea para poder cortar\n",
    "    m_head = re_bold_head.search(first_line)\n",
    "    head_end = m_head.end() if m_head else 0\n",
    "\n",
    "    # 5) Extraer todo lo que sigue al lema en el artículo (todas las líneas) para análisis:\n",
    "    #    -  para detectar ocurrencias externas con \"|| **...~...**\"\n",
    "    #    -  para detectar numeraciones (2., 3., ...)\n",
    "    after_head_all = art[art.find(first_line) + head_end:]\n",
    "\n",
    "    # 6) Flags para estructura del artículo\n",
    "    has_external_pipes = (\"||\" in after_head_all)                      # ¿hay ocurrencias externas?\n",
    "    head_has_meaning   = check_meaning_before_pipes(first_line, head_end) if m_head else False  # ¿hay sig. propio del lema?\n",
    "\n",
    "    # 7) ¿Cuántas acepciones enumeradas hay (2., 3., …)? -> n_defs = 1 + n_extra_defs\n",
    "    n_extra_defs = count_enumerations(after_head_all)\n",
    "    n_defs = 1 + n_extra_defs\n",
    "\n",
    "    # 8) Variantes en el lema (p.ej., \"**carranchil (carranchín)**\") -> crear filas también para cada variante\n",
    "    variants = extract_parenthetical_variants(head_content)\n",
    "\n",
    "    # 9) Clasificación de casos (Req. 1 y Req. 2):\n",
    "    #    - is_req1_simple: SOLO una acepción, SIN pipes externos, y SIN pipes en el lema\n",
    "    #      -> extraer significado/ejemplo de la primera línea y asignar SOLO al lema base\n",
    "    #    - is_req2_simple: SOLO una acepción y HAY pipes (en lema o externos)\n",
    "    #      -> extraer significado/ejemplo de la primera línea y asignarlo a TODAS las frases derivadas\n",
    "    head_has_pipes = (\"||\" in head_content)\n",
    "    is_req1_simple = (not has_external_pipes) and (n_extra_defs == 0) and (not head_has_pipes)\n",
    "    is_req2_simple = (n_extra_defs == 0) and (head_has_pipes or has_external_pipes)\n",
    "\n",
    "    # 10) Si aplica, se extrae significado/ejemplo de la PRIMERA línea (tras el lema), ya con limpieza de etiquetas/regiones/citas.\n",
    "    sig_req1 = ej_req1 = \"\"\n",
    "    sig_req2 = ej_req2 = \"\"\n",
    "    if (m_head and is_req1_simple) or (m_head and is_req2_simple):\n",
    "        sig, ej = extract_meaning_example_after_head(first_line, head_end)\n",
    "        if is_req1_simple:\n",
    "            sig_req1, ej_req1 = sig, ej\n",
    "        if is_req2_simple:\n",
    "            sig_req2, ej_req2 = sig, ej\n",
    "\n",
    "    # 11) Si hay enumeraciones (2., 3., …), se separan las acepciones de la primera línea,  y se extrae (significado, ejemplo) por segmento.\n",
    "    enumerated_senses = []\n",
    "    if n_extra_defs > 0:\n",
    "        enumerated_senses = extract_enumerated_senses_from_first_line(first_line, head_end)\n",
    "        # “Colchón” por si se detectó menos segmentos de los esperados\n",
    "        if len(enumerated_senses) < n_defs:\n",
    "            enumerated_senses += [(\"\", \"\")] * (n_defs - len(enumerated_senses))\n",
    "\n",
    "    # 12) CREACIÓN DE FILAS PARA LA PALABRA BASE (y variantes):\n",
    "    #     - Si NO es el caso “solo pipes externos sin sig. propio”,\n",
    "    #       crea filas para el lema base (y para cada variante), una por acepción.\n",
    "    #       (En caso de enumeración, se usa enumerated_senses[i];\n",
    "    #        si no, se usa los sig/ej tomados en Req.1/Req.2 o vacío.)\n",
    "    #     - Caso especial excluido: cuando hay pipes externos y NO hay significado antes del primer \"||\":\n",
    "    #       en ese caso, el lema base no “merece” fila por sí solo; solo se listan derivados.\n",
    "    if not (has_external_pipes and not head_has_meaning):\n",
    "        # a) Filas para el lema base\n",
    "        for i in range(n_defs):\n",
    "            if n_extra_defs > 0:\n",
    "                sig_i, ej_i = enumerated_senses[i]\n",
    "            else:\n",
    "                sig_i = (sig_req1 if is_req1_simple else (sig_req2 if is_req2_simple else \"\"))\n",
    "                ej_i  = (ej_req1  if is_req1_simple else (ej_req2  if is_req2_simple else \"\"))\n",
    "            rows.append({\n",
    "                \"palabra\": palabra_head,\n",
    "                \"significado\": sig_i,\n",
    "                \"ejemplo\": ej_i\n",
    "            })\n",
    "        # b) Filas para cada variante entre paréntesis (si existen)\n",
    "        for var in variants:\n",
    "            for i in range(n_defs):\n",
    "                if n_extra_defs > 0:\n",
    "                    sig_i, ej_i = enumerated_senses[i]\n",
    "                else:\n",
    "                    sig_i = (sig_req1 if is_req1_simple else (sig_req2 if is_req2_simple else \"\"))\n",
    "                    ej_i  = (ej_req1  if is_req1_simple else (ej_req2  if is_req2_simple else \"\"))\n",
    "                rows.append({\n",
    "                    \"palabra\": var,\n",
    "                    \"significado\": sig_i,\n",
    "                    \"ejemplo\": ej_i\n",
    "                })\n",
    "\n",
    "    # 13) CREACIÓN DE FILAS PARA OCURRENCIAS EXTERNAS (derivados con \"|| **...~...**\"):\n",
    "    #     - Por cada \"**...~...**\" tras \"||\", se reemplaza \"~\" por la base del lema.\n",
    "    #     - Si NO hay enumeraciones, se puede heredar sig/ej de la 1ª línea (Req. 2).\n",
    "    if has_external_pipes:\n",
    "        # Base del lema para expandir \"~\"\n",
    "        base_raw = extract_base_from_head(head_content)\n",
    "\n",
    "        # Recorre todas las ocurrencias: \"**frase_con_tilde** cola_local\"\n",
    "        for bold_phrase, local_tail in external_occurrence_pattern.findall(after_head_all):\n",
    "            # Reemplazar tilde y limpiar marca de estilo/puntuación/pipes al final\n",
    "            phrase = replace_tilde_with_base(bold_phrase, base_raw)\n",
    "            phrase = clean_curs(phrase)\n",
    "            phrase = strip_punct(remove_trailing_pipes(norm_spaces(phrase))).rstrip(\".\")\n",
    "            if not phrase:\n",
    "                # Evitar crear filas vacías si la frase quedó sin contenido\n",
    "                continue\n",
    "\n",
    "            # Si NO hay enumeraciones, extraer sig/ej de la cola local (después de esa frase)\n",
    "            sig_loc = ej_loc = \"\"\n",
    "            if n_extra_defs == 0:\n",
    "                sig_loc, ej_loc = extract_meaning_example_from_tail(local_tail)\n",
    "\n",
    "            rows.append({\n",
    "                \"palabra\": phrase,\n",
    "                \"significado\": sig_loc,\n",
    "                \"ejemplo\": ej_loc\n",
    "            })\n",
    "\n",
    "\n",
    "# CONSTRUCCIÓN DEL df\n",
    "df = pd.DataFrame(rows).reset_index(drop=True)\n",
    "\n",
    "# (páginas iniciales/ruido de la transcripción antes del primer artículo real).\n",
    "if len(df) >= 3:\n",
    "    df = df.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b61a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_csv = \"Lexicc_CaroCuervoFinal.csv\"\n",
    "\n",
    "# 1. Limpieza final de columnas\n",
    "for col in ['palabra', 'significado', 'ejemplo']:\n",
    "    # Convertir a string y limpiar espacios en extremos\n",
    "    df[col] = df[col].astype(str).str.strip()\n",
    "    \n",
    "\n",
    "    df[col] = df[col].str.replace(r'^[\\'\"]|[\\'\"]$', '', regex=True)\n",
    "    \n",
    "    # Convertir todo a minúsculas\n",
    "    df[col] = df[col].str.lower()\n",
    "\n",
    "# 2. Ordena el DataFrame alfabéticamente por la columna 'palabra'\n",
    "df = df.sort_values(by='palabra', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# 3. a CSV por comas\n",
    "df.to_csv(out_file_csv, index=False, quoting=1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
