{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd5c3f1",
   "metadata": {},
   "source": [
    "## STRAICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee6a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from typing import List, Optional, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "# API endpoint\n",
    "API_URL = \"https://api.straico.com/v1/prompt/completion\"\n",
    "\n",
    "# Hardcoded API key and default model(s)\n",
    "API_KEY = \"v8-Ps43kVOZynO8LHli5ay9oXNDR7VNMmDkxONui5xcp8Q0tjAo\"\n",
    "\n",
    "PROMPT_ID_MAP = {\n",
    "    \"prompt_1\": \"P1\", # Dada la palabra --> Definición del modismo.\n",
    "    \"prompt_2\": \"P2\", # Dada la palabra --> Decir si es modismo o no.\n",
    "    \"prompt_3\": \"P3\", # Dado el ejemplo --> Reemplazo literal y definición del remplazo.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3601cb",
   "metadata": {},
   "source": [
    "#### Usar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc200094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_prompt(message: str, models: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    \n",
    "    \"\"\" Send a single text prompt to Straico and return the parsed response.\n",
    "        This function always performs a live HTTP request.\n",
    "    \"\"\"\n",
    "\n",
    "    payload: Dict[str, Any] = {\"models\": models, \"message\": message}\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(API_URL, headers=headers, json=payload, timeout=30)\n",
    "    except requests.exceptions.RequestException as exc:\n",
    "        return {\"error\": str(exc)}\n",
    "\n",
    "    # Parse JSON if available\n",
    "    try:\n",
    "        data = resp.json()\n",
    "    except Exception:\n",
    "        data = None\n",
    "\n",
    "    if 200 <= resp.status_code < 300:\n",
    "\n",
    "        # Try to extract the assistant 'content' text from the common response\n",
    "        # shape: data -> completions -> <model> -> completion -> choices[0] -> message -> content\n",
    "\n",
    "        if isinstance(data, dict):\n",
    "            completions = data.get('data', {}).get('completions', {})\n",
    "\n",
    "            # If we passed a single model, try to use that key; otherwise take first\n",
    "            model_key = None\n",
    "\n",
    "            if models and len(models) == 1:\n",
    "                model_key = models[0]\n",
    "            if not model_key and isinstance(completions, dict) and len(completions) > 0:\n",
    "                model_key = next(iter(completions.keys()))\n",
    "\n",
    "            if model_key and model_key in completions:\n",
    "                try:\n",
    "                    content = completions[model_key]['completion']['choices'][0]['message']['content']\n",
    "                    return content\n",
    "                \n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Fallback: return raw text or the full JSON string\n",
    "        if data is None:\n",
    "            return resp.text\n",
    "        \n",
    "        try:\n",
    "            return json.dumps(data, ensure_ascii=False)\n",
    "        except Exception:\n",
    "            return str(data)\n",
    "\n",
    "    return {\"error\": f\"status={resp.status_code}\", \"response\": data if data is not None else resp.text}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2029b9",
   "metadata": {},
   "source": [
    "### Configuración General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455601bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _import_prompts() -> Dict[str, str]:\n",
    "    \"\"\"Load prompt_1..prompt_3 from prompts.py and return as a dict.\n",
    "    If prompts.py isn't present, returns an empty dict.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from Straico import prompts as p  # type: ignore\n",
    "    except Exception:\n",
    "        try:\n",
    "            import prompts as p  # type: ignore\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "    out: Dict[str, str] = {}\n",
    "    for name in (\"prompt_1\", \"prompt_2\", \"prompt_3\"):\n",
    "        if hasattr(p, name):\n",
    "            out[name] = getattr(p, name)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf83c9f",
   "metadata": {},
   "source": [
    "#### Modelos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelos desde el archivo text_model_ids.txt\n",
    "MODELS_FILE = 'Straico/text_model_usefull.txt'\n",
    "\n",
    "def load_models_from_file(filepath):\n",
    "    \"\"\"Carga los nombres de modelos desde un archivo de texto.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontró el archivo {filepath}\")\n",
    "        return []\n",
    "\n",
    "DEFAULT_MODELS = load_models_from_file(MODELS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab22b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95894612",
   "metadata": {},
   "source": [
    "#### Ejecusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fa34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'modismos_Dataset_Cleaned.csv'\n",
    "RESPONSES_DIR = 'Straico'\n",
    "\n",
    "# Calcular N_ROWS automáticamente\n",
    "with open(DATASET_PATH, encoding='utf-8') as f:\n",
    "    N_ROWS = sum(1 for _ in csv.DictReader(f, delimiter=';'))\n",
    "\n",
    "print(f\"Total de filas en el dataset: {N_ROWS}\")\n",
    "\n",
    "# Cargar prompts\n",
    "PROMPTS = _import_prompts()\n",
    "print(f\"Prompts cargados: {list(PROMPTS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f38ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_dataset(n_rows=None):\n",
    "    \"\"\"Carga el dataset y retorna una lista de diccionarios con modismo, significado y ejemplo.\"\"\"\n",
    "    rows = []\n",
    "    seen_modismos = set()\n",
    "    \n",
    "    with open(DATASET_PATH, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter=';')\n",
    "        for r in reader:\n",
    "            modismo = r.get('modismo', '').strip()\n",
    "            if not modismo or modismo.casefold() in seen_modismos:\n",
    "                continue\n",
    "            seen_modismos.add(modismo.casefold())\n",
    "            \n",
    "            rows.append({\n",
    "                'modismo': modismo,\n",
    "                'significado': r.get('significado', '').strip(),\n",
    "                'ejemplo': r.get('ejemplo', '').strip()\n",
    "            })\n",
    "            \n",
    "            if n_rows and len(rows) >= n_rows:\n",
    "                break\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "def sanitize_model_name(model_name):\n",
    "    \"\"\"Convierte nombres de modelos en nombres válidos para nombres de archivos.\"\"\"\n",
    "    return model_name.replace('/', '_').replace(':', '_').replace('-', '_').replace('.', '_')\n",
    "\n",
    "\n",
    "def save_json(filepath, data):\n",
    "    \"\"\"Guarda datos en un archivo JSON.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✓ Guardado: {filepath}\")\n",
    "\n",
    "\n",
    "def save_model_response(base_dir, prompt_name, model_name, data):\n",
    "    \"\"\"Guarda la respuesta de un modelo específico en su propia carpeta.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Directorio base (ej: 'Straico')\n",
    "        prompt_name: Nombre del prompt (ej: 'Prompt 1')\n",
    "        model_name: Nombre del modelo\n",
    "        data: Lista de respuestas del modelo\n",
    "    \"\"\"\n",
    "    model_safe_name = sanitize_model_name(model_name)\n",
    "    model_dir = os.path.join(base_dir, prompt_name, model_safe_name)\n",
    "    filepath = os.path.join(model_dir, f\"{model_safe_name}.json\")\n",
    "    save_json(filepath, data)\n",
    "\n",
    "\n",
    "def save_consolidated_response(base_dir, prompt_name, all_models_data):\n",
    "    \"\"\"Guarda todas las respuestas consolidadas en un solo archivo.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Directorio base (ej: 'Straico')\n",
    "        prompt_name: Nombre del prompt (ej: 'Prompt 1')\n",
    "        all_models_data: Diccionario con {model_name: [respuestas]}\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(base_dir, prompt_name, \"all_models.json\")\n",
    "    save_json(filepath, all_models_data)\n",
    "\n",
    "\n",
    "def save_partial_progress(base_dir, prompt_name, progress_data):\n",
    "    \"\"\"Guarda el progreso parcial durante la ejecución.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Directorio base (ej: 'Straico')\n",
    "        prompt_name: Nombre del prompt (ej: 'Prompt 1')\n",
    "        progress_data: Datos parciales acumulados\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(base_dir, prompt_name, \"progress.json\")\n",
    "    save_json(filepath, progress_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006497db",
   "metadata": {},
   "source": [
    "---\n",
    "## PROMPT 1: Modismo → Definición\n",
    "\n",
    "**Input**: `modismo`  \n",
    "**Output**: `definicion`  \n",
    "**Guardado**: \n",
    "- `Straico/Prompt 1/{modelo}/{modelo}.json` (por cada modelo)\n",
    "- `Straico/Prompt 1/all_models.json` (consolidado)\n",
    "- `Straico/Prompt 1/progress.json` (progreso parcial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_1(models=DEFAULT_MODELS, n_rows=N_ROWS):\n",
    "    \"\"\"\n",
    "    PROMPT 1: Dada la palabra/modismo → Generar definición\n",
    "    INPUT: modismo\n",
    "    OUTPUT: definicion\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EJECUTANDO PROMPT 1: Modismo -> Definicion\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Cargar dataset\n",
    "    dataset = cargar_dataset(n_rows)\n",
    "    print(f\"Dataset cargado: {len(dataset)} filas\")\n",
    "    print(f\"Modelos a consultar: {len(models)}\")\n",
    "\n",
    "    # Obtener template del prompt\n",
    "    template = PROMPTS.get('prompt_1')\n",
    "    if not template:\n",
    "        print(\"ERROR: prompt_1 no encontrado\")\n",
    "        return\n",
    "\n",
    "    # Verificar progreso previo\n",
    "    progress_path = os.path.join(RESPONSES_DIR, \"Prompt 1\", \"progress.json\")\n",
    "    if os.path.exists(progress_path):\n",
    "        with open(progress_path, 'r', encoding='utf-8') as f:\n",
    "            progress_data = json.load(f)\n",
    "        print(\"Progreso encontrado. Continuando desde el último punto...\")\n",
    "        all_models_data = progress_data.get(\"data\", {})\n",
    "        start_model_idx = progress_data.get(\"current_model_idx\", 0)\n",
    "        start_row_idx = progress_data.get(\"current_row_idx\", 0)\n",
    "    else:\n",
    "        all_models_data = {model: [] for model in models}\n",
    "        start_model_idx = 0\n",
    "        start_row_idx = 0\n",
    "\n",
    "    progress_data = {\n",
    "        \"current_model_idx\": start_model_idx,\n",
    "        \"current_row_idx\": start_row_idx,\n",
    "        \"total_rows\": len(dataset),\n",
    "        \"models\": models,\n",
    "        \"data\": all_models_data\n",
    "    }\n",
    "\n",
    "    # Procesar modelo por modelo\n",
    "    for model_idx, model in enumerate(models[start_model_idx:], start=start_model_idx):\n",
    "        print(f\"\\nProcesando modelo {model_idx + 1}/{len(models)}: {model}\")\n",
    "\n",
    "        for row_idx, row in enumerate(tqdm(dataset[start_row_idx:], desc=f\"Modismos para {model}\", position=0), start=start_row_idx + 1):\n",
    "            modismo = row['modismo']\n",
    "\n",
    "            # Armar el prompt reemplazando placeholders\n",
    "            prompt_text = template.replace('{{modismo}}', modismo)\n",
    "\n",
    "            # Obtener respuesta del modelo\n",
    "            resp = send_prompt(prompt_text, models=[model])\n",
    "\n",
    "            # Procesar respuesta\n",
    "            if isinstance(resp, str):\n",
    "                try:\n",
    "                    parsed = json.loads(resp)\n",
    "                    response_data = parsed\n",
    "                except:\n",
    "                    response_data = {\"raw_response\": resp}\n",
    "            elif isinstance(resp, dict):\n",
    "                response_data = resp\n",
    "            else:\n",
    "                response_data = {\"raw_response\": str(resp)}\n",
    "\n",
    "            # Agregar metadatos\n",
    "            entry = {\n",
    "                \"modismo\": modismo,\n",
    "                \"model\": model,\n",
    "                \"response\": response_data\n",
    "            }\n",
    "\n",
    "            all_models_data[model].append(entry)\n",
    "\n",
    "            # Actualizar progreso\n",
    "            progress_data[\"current_model_idx\"] = model_idx\n",
    "            progress_data[\"current_row_idx\"] = row_idx\n",
    "            progress_data[\"data\"] = all_models_data\n",
    "\n",
    "            # Guardar progreso parcial cada 100 filas\n",
    "            if row_idx % 100 == 0:\n",
    "                save_partial_progress(RESPONSES_DIR, \"Prompt 1\", progress_data)\n",
    "                save_model_response(RESPONSES_DIR, \"Prompt 1\", model, all_models_data[model])\n",
    "\n",
    "        # Guardar respuestas del modelo al finalizar\n",
    "        save_model_response(RESPONSES_DIR, \"Prompt 1\", model, all_models_data[model])\n",
    "\n",
    "        # Reiniciar el índice de filas al cambiar de modelo\n",
    "        start_row_idx = 0\n",
    "\n",
    "    # Guardar consolidado final\n",
    "    print(\"\\nGuardando archivo consolidado...\")\n",
    "    save_consolidated_response(RESPONSES_DIR, \"Prompt 1\", all_models_data)\n",
    "\n",
    "    # Eliminar progreso al completar\n",
    "    if os.path.exists(progress_path):\n",
    "        os.remove(progress_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PROMPT 1 COMPLETADO\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e42aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar Prompt 1\n",
    "run_prompt_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcddcfb",
   "metadata": {},
   "source": [
    "---\n",
    "## PROMPT 2: Modismo → Es Modismo (Sí/No)\n",
    "\n",
    "**Input**: `modismo`  \n",
    "**Output**: `es_modismo` (Sí/No)  \n",
    "**Guardado**: \n",
    "- `Straico/Prompt 2/{modelo}/{modelo}.json` (por cada modelo)\n",
    "- `Straico/Prompt 2/all_models.json` (consolidado)\n",
    "- `Straico/Prompt 2/progress.json` (progreso parcial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f414854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_2(models=DEFAULT_MODELS, n_rows=N_ROWS):\n",
    "    \"\"\"\n",
    "    PROMPT 2: Dada la palabra/modismo → Determinar si es modismo (Sí/No)\n",
    "    INPUT: modismo\n",
    "    OUTPUT: es_modismo\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EJECUTANDO PROMPT 2: Modismo -> Es Modismo (Si/No)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Cargar dataset\n",
    "    dataset = cargar_dataset(n_rows)\n",
    "    print(f\"Dataset cargado: {len(dataset)} filas\")\n",
    "    print(f\"Modelos a consultar: {len(models)}\")\n",
    "\n",
    "    # Obtener template del prompt\n",
    "    template = PROMPTS.get('prompt_2')\n",
    "    if not template:\n",
    "        print(\"ERROR: prompt_2 no encontrado\")\n",
    "        return\n",
    "\n",
    "    # Verificar progreso previo\n",
    "    progress_path = os.path.join(RESPONSES_DIR, \"Prompt 2\", \"progress.json\")\n",
    "    if os.path.exists(progress_path):\n",
    "        with open(progress_path, 'r', encoding='utf-8') as f:\n",
    "            progress_data = json.load(f)\n",
    "        print(\"Progreso encontrado. Continuando desde el último punto...\")\n",
    "        all_models_data = progress_data.get(\"data\", {})\n",
    "        start_model_idx = progress_data.get(\"current_model_idx\", 0)\n",
    "        start_row_idx = progress_data.get(\"current_row_idx\", 0)\n",
    "    else:\n",
    "        all_models_data = {model: [] for model in models}\n",
    "        start_model_idx = 0\n",
    "        start_row_idx = 0\n",
    "\n",
    "    progress_data = {\n",
    "        \"current_model_idx\": start_model_idx,\n",
    "        \"current_row_idx\": start_row_idx,\n",
    "        \"total_rows\": len(dataset),\n",
    "        \"models\": models,\n",
    "        \"data\": all_models_data\n",
    "    }\n",
    "\n",
    "    # Procesar modelo por modelo\n",
    "    for model_idx, model in enumerate(models[start_model_idx:], start=start_model_idx):\n",
    "        print(f\"\\nProcesando modelo {model_idx + 1}/{len(models)}: {model}\")\n",
    "\n",
    "        for row_idx, row in enumerate(tqdm(dataset[start_row_idx:], desc=f\"Modismos para {model}\", position=0), start=start_row_idx + 1):\n",
    "            modismo = row['modismo']\n",
    "\n",
    "            # Armar el prompt reemplazando placeholders\n",
    "            prompt_text = template.replace('{{modismo}}', modismo)\n",
    "\n",
    "            # Obtener respuesta del modelo\n",
    "            resp = send_prompt(prompt_text, models=[model])\n",
    "\n",
    "            # Procesar respuesta\n",
    "            if isinstance(resp, str):\n",
    "                try:\n",
    "                    parsed = json.loads(resp)\n",
    "                    response_data = parsed\n",
    "                except:\n",
    "                    response_data = {\"raw_response\": resp}\n",
    "            elif isinstance(resp, dict):\n",
    "                response_data = resp\n",
    "            else:\n",
    "                response_data = {\"raw_response\": str(resp)}\n",
    "\n",
    "            # Agregar metadatos\n",
    "            entry = {\n",
    "                \"modismo\": modismo,\n",
    "                \"model\": model,\n",
    "                \"response\": response_data\n",
    "            }\n",
    "\n",
    "            all_models_data[model].append(entry)\n",
    "\n",
    "            # Actualizar progreso\n",
    "            progress_data[\"current_model_idx\"] = model_idx\n",
    "            progress_data[\"current_row_idx\"] = row_idx\n",
    "            progress_data[\"data\"] = all_models_data\n",
    "\n",
    "            # Guardar progreso parcial cada 100 filas\n",
    "            if row_idx % 100 == 0:\n",
    "                save_partial_progress(RESPONSES_DIR, \"Prompt 2\", progress_data)\n",
    "                save_model_response(RESPONSES_DIR, \"Prompt 2\", model, all_models_data[model])\n",
    "\n",
    "        # Guardar respuestas del modelo al finalizar\n",
    "        save_model_response(RESPONSES_DIR, \"Prompt 2\", model, all_models_data[model])\n",
    "\n",
    "        # Reiniciar el índice de filas al cambiar de modelo\n",
    "        start_row_idx = 0\n",
    "\n",
    "    # Guardar consolidado final\n",
    "    print(\"\\nGuardando archivo consolidado...\")\n",
    "    save_consolidated_response(RESPONSES_DIR, \"Prompt 2\", all_models_data)\n",
    "\n",
    "    # Eliminar progreso al completar\n",
    "    if os.path.exists(progress_path):\n",
    "        os.remove(progress_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PROMPT 2 COMPLETADO\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar Prompt 2\n",
    "run_prompt_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaae65",
   "metadata": {},
   "source": [
    "---\n",
    "## PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
    "\n",
    "**Input**: `modismo` + `ejemplo`  \n",
    "**Output**: `literal` + `definicion`  \n",
    "**Guardado**: \n",
    "- `Straico/Prompt 3/{modelo}/{modelo}.json` (por cada modelo)\n",
    "- `Straico/Prompt 3/all_models.json` (consolidado)\n",
    "- `Straico/Prompt 3/progress.json` (progreso parcial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_3(models=DEFAULT_MODELS, n_rows=N_ROWS):\n",
    "    \"\"\"\n",
    "    PROMPT 3: Dado modismo + ejemplo → Generar literal + definición\n",
    "    INPUT: modismo + ejemplo\n",
    "    OUTPUT: literal + definicion\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EJECUTANDO PROMPT 3: Modismo + Ejemplo -> Literal + Definicion\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Cargar dataset\n",
    "    dataset = cargar_dataset(n_rows)\n",
    "    dataset_with_examples = [row for row in dataset if row['ejemplo']]\n",
    "    print(f\"Dataset cargado: {len(dataset_with_examples)} filas con ejemplo\")\n",
    "    print(f\"Modelos a consultar: {len(models)}\")\n",
    "\n",
    "    # Obtener template del prompt\n",
    "    template = PROMPTS.get('prompt_3')\n",
    "    if not template:\n",
    "        print(\"ERROR: prompt_3 no encontrado\")\n",
    "        return\n",
    "\n",
    "    # Verificar progreso previo\n",
    "    progress_path = os.path.join(RESPONSES_DIR, \"Prompt 3\", \"progress.json\")\n",
    "    if os.path.exists(progress_path):\n",
    "        with open(progress_path, 'r', encoding='utf-8') as f:\n",
    "            progress_data = json.load(f)\n",
    "        print(\"Progreso encontrado. Continuando desde el último punto...\")\n",
    "        all_models_data = progress_data.get(\"data\", {})\n",
    "        start_model_idx = progress_data.get(\"current_model_idx\", 0)\n",
    "        start_row_idx = progress_data.get(\"current_row_idx\", 0)\n",
    "    else:\n",
    "        all_models_data = {model: [] for model in models}\n",
    "        start_model_idx = 0\n",
    "        start_row_idx = 0\n",
    "\n",
    "    progress_data = {\n",
    "        \"current_model_idx\": start_model_idx,\n",
    "        \"current_row_idx\": start_row_idx,\n",
    "        \"total_rows\": len(dataset_with_examples),\n",
    "        \"models\": models,\n",
    "        \"data\": all_models_data\n",
    "    }\n",
    "\n",
    "    # Procesar modelo por modelo\n",
    "    for model_idx, model in enumerate(models[start_model_idx:], start=start_model_idx):\n",
    "        print(f\"\\nProcesando modelo {model_idx + 1}/{len(models)}: {model}\")\n",
    "\n",
    "        for row_idx, row in enumerate(tqdm(dataset_with_examples[start_row_idx:], desc=f\"Modismos para {model}\", position=0), start=start_row_idx + 1):\n",
    "            modismo = row['modismo']\n",
    "            ejemplo = row['ejemplo']\n",
    "\n",
    "            # Armar el prompt reemplazando placeholders\n",
    "            prompt_text = template.replace('{{modismo}}', modismo).replace('{{ejemplo}}', ejemplo)\n",
    "\n",
    "            # Obtener respuesta del modelo\n",
    "            resp = send_prompt(prompt_text, models=[model])\n",
    "\n",
    "            # Procesar respuesta\n",
    "            if isinstance(resp, str):\n",
    "                try:\n",
    "                    parsed = json.loads(resp)\n",
    "                    response_data = parsed\n",
    "                except:\n",
    "                    response_data = {\"raw_response\": resp}\n",
    "            elif isinstance(resp, dict):\n",
    "                response_data = resp\n",
    "            else:\n",
    "                response_data = {\"raw_response\": str(resp)}\n",
    "\n",
    "            # Agregar metadatos\n",
    "            entry = {\n",
    "                \"modismo\": modismo,\n",
    "                \"ejemplo\": ejemplo,\n",
    "                \"model\": model,\n",
    "                \"response\": response_data\n",
    "            }\n",
    "\n",
    "            all_models_data[model].append(entry)\n",
    "\n",
    "            # Actualizar progreso\n",
    "            progress_data[\"current_model_idx\"] = model_idx\n",
    "            progress_data[\"current_row_idx\"] = row_idx\n",
    "            progress_data[\"data\"] = all_models_data\n",
    "\n",
    "            # Guardar progreso parcial cada 100 filas\n",
    "            if row_idx % 100 == 0:\n",
    "                save_partial_progress(RESPONSES_DIR, \"Prompt 3\", progress_data)\n",
    "                save_model_response(RESPONSES_DIR, \"Prompt 3\", model, all_models_data[model])\n",
    "\n",
    "        # Guardar respuestas del modelo al finalizar\n",
    "        save_model_response(RESPONSES_DIR, \"Prompt 3\", model, all_models_data[model])\n",
    "\n",
    "        # Reiniciar el índice de filas al cambiar de modelo\n",
    "        start_row_idx = 0\n",
    "\n",
    "    # Guardar consolidado final\n",
    "    print(\"\\nGuardando archivo consolidado...\")\n",
    "    save_consolidated_response(RESPONSES_DIR, \"Prompt 3\", all_models_data)\n",
    "\n",
    "    # Eliminar progreso al completar\n",
    "    if os.path.exists(progress_path):\n",
    "        os.remove(progress_path)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PROMPT 3 COMPLETADO\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar Prompt 3\n",
    "run_prompt_3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
