{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd5c3f1",
   "metadata": {},
   "source": [
    "## STRAICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee6a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from typing import List, Optional, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# API endpoint\n",
    "API_URL = \"https://api.straico.com/v1/prompt/completion\"\n",
    "\n",
    "# Hardcoded API key and default model(s)\n",
    "API_KEY = \"v8-Ps43kVOZynO8LHli5ay9oXNDR7VNMmDkxONui5xcp8Q0tjAo\"\n",
    "\n",
    "PROMPT_ID_MAP = {\n",
    "    \"prompt_1\": \"P1\", # Dada la palabra --> Definición del modismo.\n",
    "    \"prompt_2\": \"P2\", # Dada la palabra --> Decir si es modismo o no.\n",
    "    \"prompt_3\": \"P3\", # Dado el ejemplo --> Reemplazo literal y definición del remplazo.\n",
    "}\n",
    "\n",
    "# Configuración de paralelización\n",
    "MAX_WORKERS = 8  # Puedes aumentar a 10-12 si el API lo permite\n",
    "\n",
    "# Lock global para operaciones de escritura thread-safe\n",
    "_write_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc200094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_prompt(message: str, models: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    \n",
    "    \"\"\" Send a single text prompt to Straico and return the parsed response.\n",
    "        This function always performs a live HTTP request.\n",
    "    \"\"\"\n",
    "\n",
    "    payload: Dict[str, Any] = {\"models\": models, \"message\": message}\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(API_URL, headers=headers, json=payload, timeout=30)\n",
    "    except requests.exceptions.RequestException as exc:\n",
    "        return {\"error\": str(exc)}\n",
    "\n",
    "    # Parse JSON if available\n",
    "    try:\n",
    "        data = resp.json()\n",
    "    except Exception:\n",
    "        data = None\n",
    "\n",
    "    if 200 <= resp.status_code < 300:\n",
    "\n",
    "        # Try to extract the assistant 'content' text from the common response\n",
    "        # shape: data -> completions -> <model> -> completion -> choices[0] -> message -> content\n",
    "\n",
    "        if isinstance(data, dict):\n",
    "            completions = data.get('data', {}).get('completions', {})\n",
    "\n",
    "            # If we passed a single model, try to use that key; otherwise take first\n",
    "            model_key = None\n",
    "\n",
    "            if models and len(models) == 1:\n",
    "                model_key = models[0]\n",
    "            if not model_key and isinstance(completions, dict) and len(completions) > 0:\n",
    "                model_key = next(iter(completions.keys()))\n",
    "\n",
    "            if model_key and model_key in completions:\n",
    "                try:\n",
    "                    content = completions[model_key]['completion']['choices'][0]['message']['content']\n",
    "                    return content\n",
    "                \n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        # Fallback: return raw text or the full JSON string\n",
    "        if data is None:\n",
    "            return resp.text\n",
    "        \n",
    "        try:\n",
    "            return json.dumps(data, ensure_ascii=False)\n",
    "        except Exception:\n",
    "            return str(data)\n",
    "\n",
    "    return {\"error\": f\"status={resp.status_code}\", \"response\": data if data is not None else resp.text}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2029b9",
   "metadata": {},
   "source": [
    "### Configuración General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455601bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _import_prompts() -> Dict[str, str]:\n",
    "    \"\"\"Load prompt_1..prompt_3 from prompts.py and return as a dict.\n",
    "    If prompts.py isn't present, returns an empty dict.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from Straico import prompts as p  # type: ignore\n",
    "    except Exception:\n",
    "        try:\n",
    "            import prompts as p  # type: ignore\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "    out: Dict[str, str] = {}\n",
    "    for name in (\"prompt_1\", \"prompt_2\", \"prompt_3\"):\n",
    "        if hasattr(p, name):\n",
    "            out[name] = getattr(p, name)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf83c9f",
   "metadata": {},
   "source": [
    "#### Modelos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelos desde el archivo text_model_ids.txt\n",
    "MODELS_FILE = 'Straico/text_model_usefull.txt'\n",
    "\n",
    "def load_models_from_file(filepath):\n",
    "    \"\"\"Carga los nombres de modelos desde un archivo de texto.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: No se encontró el archivo {filepath}\")\n",
    "        return []\n",
    "\n",
    "DEFAULT_MODELS = load_models_from_file(MODELS_FILE)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab22b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95894612",
   "metadata": {},
   "source": [
    "#### Ejecusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fa34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "DATASET_PATH = 'modismos_Dataset_Final.csv'\n",
    "N_ROWS = 20\n",
    "RESPONSES_DIR = 'Straico'\n",
    "\n",
    "# Cargar prompts\n",
    "PROMPTS = _import_prompts()\n",
    "print(f\"Prompts cargados: {list(PROMPTS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f38ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_dataset(n_rows=None):\n",
    "    \"\"\"Carga el dataset y retorna una lista de diccionarios con modismo, significado y ejemplo.\"\"\"\n",
    "    rows = []\n",
    "    seen_modismos = set()\n",
    "    \n",
    "    with open(DATASET_PATH, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter=';')\n",
    "        for r in reader:\n",
    "            modismo = r.get('modismo', '').strip()\n",
    "            if not modismo or modismo.casefold() in seen_modismos:\n",
    "                continue\n",
    "            seen_modismos.add(modismo.casefold())\n",
    "            \n",
    "            rows.append({\n",
    "                'modismo': modismo,\n",
    "                'significado': r.get('significado', '').strip(),\n",
    "                'ejemplo': r.get('ejemplo', '').strip()\n",
    "            })\n",
    "            \n",
    "            if n_rows and len(rows) >= n_rows:\n",
    "                break\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "def sanitize_model_name(model_name):\n",
    "    \"\"\"Convierte nombres de modelos en nombres válidos para nombres de archivos.\"\"\"\n",
    "    return model_name.replace('/', '_').replace(':', '_').replace('-', '_').replace('.', '_')\n",
    "\n",
    "\n",
    "def save_json(filepath, data):\n",
    "    \"\"\"Guarda datos en un archivo JSON de forma thread-safe.\"\"\"\n",
    "    with _write_lock:\n",
    "        try:\n",
    "            dir_path = os.path.dirname(filepath)\n",
    "            if dir_path:\n",
    "                os.makedirs(dir_path, exist_ok=True)\n",
    "            \n",
    "            # Escritura atómica usando archivo temporal\n",
    "            temp_filepath = f\"{filepath}.tmp\"\n",
    "            with open(temp_filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # Renombrar es operación atómica en sistemas POSIX\n",
    "            os.replace(temp_filepath, filepath)\n",
    "            print(f\"[OK] Guardado: {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] No se pudo guardar {filepath}: {e}\")\n",
    "            # Limpiar archivo temporal si existe\n",
    "            if os.path.exists(temp_filepath):\n",
    "                try:\n",
    "                    os.remove(temp_filepath)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "def save_model_response(base_dir, prompt_name, model_name, data):\n",
    "    \"\"\"Guarda la respuesta de un modelo específico en su propia carpeta.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Directorio base (ej: 'Straico')\n",
    "        prompt_name: Nombre del prompt (ej: 'Prompt 1')\n",
    "        model_name: Nombre del modelo\n",
    "        data: Lista de respuestas del modelo\n",
    "    \"\"\"\n",
    "    model_safe_name = sanitize_model_name(model_name)\n",
    "    model_dir = os.path.join(base_dir, prompt_name, model_safe_name)\n",
    "    filepath = os.path.join(model_dir, f\"{model_safe_name}.json\")\n",
    "    save_json(filepath, data)\n",
    "\n",
    "\n",
    "def save_consolidated_response(base_dir, prompt_name, all_models_data):\n",
    "    \"\"\"Guarda todas las respuestas consolidadas en un solo archivo.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Directorio base (ej: 'Straico')\n",
    "        prompt_name: Nombre del prompt (ej: 'Prompt 1')\n",
    "        all_models_data: Diccionario con {model_name: [respuestas]}\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(base_dir, prompt_name, \"all_models.json\")\n",
    "    save_json(filepath, all_models_data)\n",
    "\n",
    "\n",
    "def save_partial_progress(base_dir, prompt_name, progress_data):\n",
    "    \"\"\"Guarda el progreso parcial durante la ejecución.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Directorio base (ej: 'Straico')\n",
    "        prompt_name: Nombre del prompt (ej: 'Prompt 1')\n",
    "        progress_data: Datos parciales acumulados\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(base_dir, prompt_name, \"progress.json\")\n",
    "    save_json(filepath, progress_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006497db",
   "metadata": {},
   "source": [
    "---\n",
    "## PROMPT 1: Modismo → Definición\n",
    "\n",
    "**Input**: `modismo`  \n",
    "**Output**: `definicion`  \n",
    "**Guardado**: \n",
    "- `Straico/Prompt 1/{modelo}/{modelo}.json` (por cada modelo)\n",
    "- `Straico/Prompt 1/all_models.json` (consolidado)\n",
    "- `Straico/Prompt 1/progress.json` (progreso parcial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_model_prompt_1(model, dataset, template, responses_dir=\"Straico\"):\n",
    "    \"\"\"\n",
    "    Procesa un solo modelo para el Prompt 1.\n",
    "    Esta función se ejecutará en paralelo para múltiples modelos.\n",
    "    \"\"\"\n",
    "    model_responses = []\n",
    "    errors_count = 0\n",
    "    \n",
    "    for row in tqdm(dataset, desc=f\"[{model}]\", position=None, leave=True):\n",
    "        try:\n",
    "            modismo = row.get('modismo', '').strip()\n",
    "            if not modismo:\n",
    "                continue\n",
    "            \n",
    "            # Armar el prompt\n",
    "            prompt_text = template.replace('{{modismo}}', modismo)\n",
    "            \n",
    "            # Obtener respuesta del modelo con retry básico\n",
    "            max_retries = 3\n",
    "            resp = None\n",
    "            for attempt in range(max_retries):\n",
    "                resp = send_prompt(prompt_text, models=[model])\n",
    "                if not isinstance(resp, dict) or 'error' not in resp:\n",
    "                    break\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(1 * (attempt + 1))  # Backoff exponencial\n",
    "            \n",
    "            # Procesar respuesta\n",
    "            if isinstance(resp, str):\n",
    "                try:\n",
    "                    parsed = json.loads(resp)\n",
    "                    response_data = parsed\n",
    "                except:\n",
    "                    response_data = {\"raw_response\": resp}\n",
    "            elif isinstance(resp, dict):\n",
    "                if 'error' in resp:\n",
    "                    errors_count += 1\n",
    "                response_data = resp\n",
    "            else:\n",
    "                response_data = {\"raw_response\": str(resp)}\n",
    "            \n",
    "            # Agregar metadatos\n",
    "            entry = {\n",
    "                \"modismo\": modismo,\n",
    "                \"model\": model,\n",
    "                \"response\": response_data\n",
    "            }\n",
    "            \n",
    "            model_responses.append(entry)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Modelo {model}, modismo '{modismo}': {e}\")\n",
    "            errors_count += 1\n",
    "            continue\n",
    "    \n",
    "    # Guardar respuestas del modelo\n",
    "    try:\n",
    "        save_model_response(responses_dir, \"Prompt 1\", model, model_responses)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo guardar respuestas del modelo {model}: {e}\")\n",
    "    \n",
    "    if errors_count > 0:\n",
    "        print(f\"[WARNING] Modelo {model}: {errors_count} errores encontrados\")\n",
    "    \n",
    "    return model, model_responses\n",
    "\n",
    "\n",
    "def run_prompt_1(models=DEFAULT_MODELS, n_rows=N_ROWS, max_workers=MAX_WORKERS):\n",
    "    \"\"\"\n",
    "    PROMPT 1: Dada la palabra/modismo -> Generar definicion (PARALELIZADO)\n",
    "    INPUT: modismo\n",
    "    OUTPUT: definicion\n",
    "    \n",
    "    Optimizado para Mac M4 Pro con procesamiento paralelo de modelos.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EJECUTANDO PROMPT 1: Modismo -> Definicion (PARALELO)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Cargar dataset\n",
    "    dataset = cargar_dataset(n_rows)\n",
    "    if not dataset:\n",
    "        print(\"[ERROR] No se pudo cargar el dataset\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset cargado: {len(dataset)} filas\")\n",
    "    print(f\"Modelos a consultar: {len(models)}\")\n",
    "    print(f\"Workers paralelos: {max_workers}\")\n",
    "\n",
    "    # Obtener template del prompt\n",
    "    template = PROMPTS.get('prompt_1')\n",
    "    if not template:\n",
    "        print(\"[ERROR] prompt_1 no encontrado\")\n",
    "        return\n",
    "\n",
    "    all_models_data = {}\n",
    "    completed_models = 0\n",
    "    failed_models = 0\n",
    "    \n",
    "    # Procesar modelos en paralelo\n",
    "    print(\"\\n[INFO] Iniciando procesamiento paralelo...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Crear función parcial con parámetros fijos\n",
    "        process_func = partial(process_single_model_prompt_1, \n",
    "                              dataset=dataset, \n",
    "                              template=template, \n",
    "                              responses_dir=RESPONSES_DIR)\n",
    "        \n",
    "        # Enviar todos los modelos a procesar\n",
    "        future_to_model = {executor.submit(process_func, model): model for model in models}\n",
    "        \n",
    "        # Recolectar resultados conforme van completando\n",
    "        for future in as_completed(future_to_model):\n",
    "            model = future_to_model[future]\n",
    "            try:\n",
    "                model_name, model_responses = future.result(timeout=600)  # Timeout 10 min por modelo\n",
    "                all_models_data[model_name] = model_responses\n",
    "                completed_models += 1\n",
    "                print(f\"[OK] Completado {completed_models}/{len(models)}: {model_name} ({len(model_responses)} respuestas)\")\n",
    "            except Exception as exc:\n",
    "                print(f\"[ERROR] Fallo en {model}: {exc}\")\n",
    "                all_models_data[model] = []\n",
    "                failed_models += 1\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Guardar consolidado final\n",
    "    print(f\"\\n[INFO] Guardando archivo consolidado...\")\n",
    "    try:\n",
    "        save_consolidated_response(RESPONSES_DIR, \"Prompt 1\", all_models_data)\n",
    "        print(f\"[OK] Archivo consolidado guardado correctamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo guardar archivo consolidado: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"PROMPT 1 COMPLETADO\")\n",
    "    print(f\"Modelos exitosos: {completed_models}/{len(models)}\")\n",
    "    print(f\"Modelos fallidos: {failed_models}/{len(models)}\")\n",
    "    print(f\"Tiempo total: {elapsed_time:.2f} segundos\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e42aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar Prompt 1\n",
    "run_prompt_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcddcfb",
   "metadata": {},
   "source": [
    "---\n",
    "## PROMPT 2: Modismo → Es Modismo (Sí/No)\n",
    "\n",
    "**Input**: `modismo`  \n",
    "**Output**: `es_modismo` (Sí/No)  \n",
    "**Guardado**: \n",
    "- `Straico/Prompt 2/{modelo}/{modelo}.json` (por cada modelo)\n",
    "- `Straico/Prompt 2/all_models.json` (consolidado)\n",
    "- `Straico/Prompt 2/progress.json` (progreso parcial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f414854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_model_prompt_2(model, dataset, template, responses_dir=\"Straico\"):\n",
    "    \"\"\"\n",
    "    Procesa un solo modelo para el Prompt 2.\n",
    "    Esta función se ejecutará en paralelo para múltiples modelos.\n",
    "    \"\"\"\n",
    "    model_responses = []\n",
    "    errors_count = 0\n",
    "    \n",
    "    for row in tqdm(dataset, desc=f\"[{model}]\", position=None, leave=True):\n",
    "        try:\n",
    "            modismo = row.get('modismo', '').strip()\n",
    "            if not modismo:\n",
    "                continue\n",
    "            \n",
    "            # Armar el prompt\n",
    "            prompt_text = template.replace('{{modismo}}', modismo)\n",
    "            \n",
    "            # Obtener respuesta del modelo con retry básico\n",
    "            max_retries = 3\n",
    "            resp = None\n",
    "            for attempt in range(max_retries):\n",
    "                resp = send_prompt(prompt_text, models=[model])\n",
    "                if not isinstance(resp, dict) or 'error' not in resp:\n",
    "                    break\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(1 * (attempt + 1))\n",
    "            \n",
    "            # Procesar respuesta\n",
    "            if isinstance(resp, str):\n",
    "                try:\n",
    "                    parsed = json.loads(resp)\n",
    "                    response_data = parsed\n",
    "                except:\n",
    "                    response_data = {\"raw_response\": resp}\n",
    "            elif isinstance(resp, dict):\n",
    "                if 'error' in resp:\n",
    "                    errors_count += 1\n",
    "                response_data = resp\n",
    "            else:\n",
    "                response_data = {\"raw_response\": str(resp)}\n",
    "            \n",
    "            # Agregar metadatos\n",
    "            entry = {\n",
    "                \"modismo\": modismo,\n",
    "                \"model\": model,\n",
    "                \"response\": response_data\n",
    "            }\n",
    "            \n",
    "            model_responses.append(entry)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Modelo {model}, modismo '{modismo}': {e}\")\n",
    "            errors_count += 1\n",
    "            continue\n",
    "    \n",
    "    # Guardar respuestas del modelo\n",
    "    try:\n",
    "        save_model_response(responses_dir, \"Prompt 2\", model, model_responses)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo guardar respuestas del modelo {model}: {e}\")\n",
    "    \n",
    "    if errors_count > 0:\n",
    "        print(f\"[WARNING] Modelo {model}: {errors_count} errores encontrados\")\n",
    "    \n",
    "    return model, model_responses\n",
    "\n",
    "\n",
    "def run_prompt_2(models=DEFAULT_MODELS, n_rows=N_ROWS, max_workers=MAX_WORKERS):\n",
    "    \"\"\"\n",
    "    PROMPT 2: Dada la palabra/modismo -> Determinar si es modismo (Si/No) (PARALELIZADO)\n",
    "    INPUT: modismo\n",
    "    OUTPUT: es_modismo\n",
    "    \n",
    "    Optimizado para Mac M4 Pro con procesamiento paralelo de modelos.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EJECUTANDO PROMPT 2: Modismo -> Es Modismo (Si/No) (PARALELO)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Cargar dataset\n",
    "    dataset = cargar_dataset(n_rows)\n",
    "    if not dataset:\n",
    "        print(\"[ERROR] No se pudo cargar el dataset\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Dataset cargado: {len(dataset)} filas\")\n",
    "    print(f\"Modelos a consultar: {len(models)}\")\n",
    "    print(f\"Workers paralelos: {max_workers}\")\n",
    "\n",
    "    # Obtener template del prompt\n",
    "    template = PROMPTS.get('prompt_2')\n",
    "    if not template:\n",
    "        print(\"[ERROR] prompt_2 no encontrado\")\n",
    "        return\n",
    "\n",
    "    all_models_data = {}\n",
    "    completed_models = 0\n",
    "    failed_models = 0\n",
    "    \n",
    "    # Procesar modelos en paralelo\n",
    "    print(\"\\n[INFO] Iniciando procesamiento paralelo...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Crear función parcial con parámetros fijos\n",
    "        process_func = partial(process_single_model_prompt_2, \n",
    "                              dataset=dataset, \n",
    "                              template=template, \n",
    "                              responses_dir=RESPONSES_DIR)\n",
    "        \n",
    "        # Enviar todos los modelos a procesar\n",
    "        future_to_model = {executor.submit(process_func, model): model for model in models}\n",
    "        \n",
    "        # Recolectar resultados conforme van completando\n",
    "        for future in as_completed(future_to_model):\n",
    "            model = future_to_model[future]\n",
    "            try:\n",
    "                model_name, model_responses = future.result(timeout=600)\n",
    "                all_models_data[model_name] = model_responses\n",
    "                completed_models += 1\n",
    "                print(f\"[OK] Completado {completed_models}/{len(models)}: {model_name} ({len(model_responses)} respuestas)\")\n",
    "            except Exception as exc:\n",
    "                print(f\"[ERROR] Fallo en {model}: {exc}\")\n",
    "                all_models_data[model] = []\n",
    "                failed_models += 1\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Guardar consolidado final\n",
    "    print(f\"\\n[INFO] Guardando archivo consolidado...\")\n",
    "    try:\n",
    "        save_consolidated_response(RESPONSES_DIR, \"Prompt 2\", all_models_data)\n",
    "        print(f\"[OK] Archivo consolidado guardado correctamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo guardar archivo consolidado: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"PROMPT 2 COMPLETADO\")\n",
    "    print(f\"Modelos exitosos: {completed_models}/{len(models)}\")\n",
    "    print(f\"Modelos fallidos: {failed_models}/{len(models)}\")\n",
    "    print(f\"Tiempo total: {elapsed_time:.2f} segundos\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar Prompt 2\n",
    "run_prompt_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaae65",
   "metadata": {},
   "source": [
    "---\n",
    "## PROMPT 3: Modismo + Ejemplo → Literal + Definición\n",
    "\n",
    "**Input**: `modismo` + `ejemplo`  \n",
    "**Output**: `literal` + `definicion`  \n",
    "**Guardado**: \n",
    "- `Straico/Prompt 3/{modelo}/{modelo}.json` (por cada modelo)\n",
    "- `Straico/Prompt 3/all_models.json` (consolidado)\n",
    "- `Straico/Prompt 3/progress.json` (progreso parcial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df7fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_model_prompt_3(model, dataset, template, responses_dir=\"Straico\"):\n",
    "    \"\"\"\n",
    "    Procesa un solo modelo para el Prompt 3.\n",
    "    Esta función se ejecutará en paralelo para múltiples modelos.\n",
    "    \"\"\"\n",
    "    model_responses = []\n",
    "    errors_count = 0\n",
    "    \n",
    "    for row in tqdm(dataset, desc=f\"[{model}]\", position=None, leave=True):\n",
    "        try:\n",
    "            modismo = row.get('modismo', '').strip()\n",
    "            ejemplo = row.get('ejemplo', '').strip()\n",
    "            \n",
    "            if not modismo or not ejemplo:\n",
    "                continue\n",
    "            \n",
    "            # Armar el prompt\n",
    "            prompt_text = template.replace('{{modismo}}', modismo).replace('{{ejemplo}}', ejemplo)\n",
    "            \n",
    "            # Obtener respuesta del modelo con retry básico\n",
    "            max_retries = 3\n",
    "            resp = None\n",
    "            for attempt in range(max_retries):\n",
    "                resp = send_prompt(prompt_text, models=[model])\n",
    "                if not isinstance(resp, dict) or 'error' not in resp:\n",
    "                    break\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(1 * (attempt + 1))\n",
    "            \n",
    "            # Procesar respuesta\n",
    "            if isinstance(resp, str):\n",
    "                try:\n",
    "                    parsed = json.loads(resp)\n",
    "                    response_data = parsed\n",
    "                except:\n",
    "                    response_data = {\"raw_response\": resp}\n",
    "            elif isinstance(resp, dict):\n",
    "                if 'error' in resp:\n",
    "                    errors_count += 1\n",
    "                response_data = resp\n",
    "            else:\n",
    "                response_data = {\"raw_response\": str(resp)}\n",
    "            \n",
    "            # Agregar metadatos\n",
    "            entry = {\n",
    "                \"modismo\": modismo,\n",
    "                \"ejemplo\": ejemplo,\n",
    "                \"model\": model,\n",
    "                \"response\": response_data\n",
    "            }\n",
    "            \n",
    "            model_responses.append(entry)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Modelo {model}, modismo '{modismo}': {e}\")\n",
    "            errors_count += 1\n",
    "            continue\n",
    "    \n",
    "    # Guardar respuestas del modelo\n",
    "    try:\n",
    "        save_model_response(responses_dir, \"Prompt 3\", model, model_responses)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo guardar respuestas del modelo {model}: {e}\")\n",
    "    \n",
    "    if errors_count > 0:\n",
    "        print(f\"[WARNING] Modelo {model}: {errors_count} errores encontrados\")\n",
    "    \n",
    "    return model, model_responses\n",
    "\n",
    "\n",
    "def run_prompt_3(models=DEFAULT_MODELS, n_rows=N_ROWS, max_workers=MAX_WORKERS):\n",
    "    \"\"\"\n",
    "    PROMPT 3: Dado modismo + ejemplo -> Generar literal + definicion (PARALELIZADO)\n",
    "    INPUT: modismo + ejemplo\n",
    "    OUTPUT: literal + definicion\n",
    "    \n",
    "    Optimizado para Mac M4 Pro con procesamiento paralelo de modelos.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EJECUTANDO PROMPT 3: Modismo + Ejemplo -> Literal + Definicion (PARALELO)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Cargar dataset\n",
    "    dataset = cargar_dataset(n_rows)\n",
    "    if not dataset:\n",
    "        print(\"[ERROR] No se pudo cargar el dataset\")\n",
    "        return\n",
    "    \n",
    "    dataset_with_examples = [row for row in dataset if row.get('ejemplo', '').strip()]\n",
    "    print(f\"Dataset cargado: {len(dataset_with_examples)} filas con ejemplo\")\n",
    "    print(f\"Modelos a consultar: {len(models)}\")\n",
    "    print(f\"Workers paralelos: {max_workers}\")\n",
    "\n",
    "    # Obtener template del prompt\n",
    "    template = PROMPTS.get('prompt_3')\n",
    "    if not template:\n",
    "        print(\"[ERROR] prompt_3 no encontrado\")\n",
    "        return\n",
    "\n",
    "    all_models_data = {}\n",
    "    completed_models = 0\n",
    "    failed_models = 0\n",
    "    \n",
    "    # Procesar modelos en paralelo\n",
    "    print(\"\\n[INFO] Iniciando procesamiento paralelo...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Crear función parcial con parámetros fijos\n",
    "        process_func = partial(process_single_model_prompt_3, \n",
    "                              dataset=dataset_with_examples, \n",
    "                              template=template, \n",
    "                              responses_dir=RESPONSES_DIR)\n",
    "        \n",
    "        # Enviar todos los modelos a procesar\n",
    "        future_to_model = {executor.submit(process_func, model): model for model in models}\n",
    "        \n",
    "        # Recolectar resultados conforme van completando\n",
    "        for future in as_completed(future_to_model):\n",
    "            model = future_to_model[future]\n",
    "            try:\n",
    "                model_name, model_responses = future.result(timeout=600)\n",
    "                all_models_data[model_name] = model_responses\n",
    "                completed_models += 1\n",
    "                print(f\"[OK] Completado {completed_models}/{len(models)}: {model_name} ({len(model_responses)} respuestas)\")\n",
    "            except Exception as exc:\n",
    "                print(f\"[ERROR] Fallo en {model}: {exc}\")\n",
    "                all_models_data[model] = []\n",
    "                failed_models += 1\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Guardar consolidado final\n",
    "    print(f\"\\n[INFO] Guardando archivo consolidado...\")\n",
    "    try:\n",
    "        save_consolidated_response(RESPONSES_DIR, \"Prompt 3\", all_models_data)\n",
    "        print(f\"[OK] Archivo consolidado guardado correctamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] No se pudo guardar archivo consolidado: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"PROMPT 3 COMPLETADO\")\n",
    "    print(f\"Modelos exitosos: {completed_models}/{len(models)}\")\n",
    "    print(f\"Modelos fallidos: {failed_models}/{len(models)}\")\n",
    "    print(f\"Tiempo total: {elapsed_time:.2f} segundos\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar Prompt 3\n",
    "run_prompt_3()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
